<html>

<head>
    <link href="book.css" rel="stylesheet" />
</head>
<div id="bookContainer">
    <div id="sbo-rt-content">
        <section data-type="chapter" epub:type="chapter" class="pagenumrestart"
            data-pdf-bookmark="Chapter 1. GPT-4 and ChatGPT Essentials">
            <div class="chapter" id="gpt_4_and_chatgpt_essentials">
                <h1><span class="label">Chapter 1. </span>GPT-4 and ChatGPT Essentials</h1>
                <p>Imagine a world where you can<a contenteditable="false" data-type="indexterm" data-primary="OpenAI"
                        data-secondary="about GPT models" id="id196"></a><a contenteditable="false"
                        data-type="indexterm" data-primary="GPT models" data-secondary="about" id="id197"></a><a
                        contenteditable="false" data-type="indexterm" data-primary="large language models (LLMs)"
                        data-secondary="about GPT models" id="id198"></a><a contenteditable="false"
                        data-type="indexterm" data-primary="natural language processing (NLP)"
                        data-secondary="about GPT models" id="id199"></a><a contenteditable="false"
                        data-type="indexterm" data-primary="GPT-4 (OpenAI)" data-secondary="about GPT models"
                        id="id200"></a><a contenteditable="false" data-type="indexterm" data-primary="ChatGPT (OpenAI)"
                        data-secondary="about" id="id201"></a><a contenteditable="false" data-type="indexterm"
                        data-primary="LLMs" data-see="large language models" id="id202"></a> communicate with computers
                    as quickly as you can with your friends. What would that look like? What applications could you
                    create? This is the world that OpenAI is helping to build with its GPT models, bringing human-like
                    conversational capabilities to our devices. As the latest advancements in AI, GPT-4 and other GPT
                    models are large language models (LLMs) trained on massive amounts of data, enabling them to
                    recognize and generate human-like text with very high accuracy. </p>
                <p>The implications of these AI models go far beyond simple voice assistants. Thanks to OpenAI’s models,
                    developers can now exploit the power of natural language processing (NLP) to create applications
                    that understand our needs in ways that were once science fiction. From innovative customer support
                    systems that learn and adapt to personalized educational tools that understand each student’s unique
                    learning style, GPT-4 and ChatGPT open up a whole new world of possibilities. </p>
                <p>But what <em>are</em> GPT-4 and ChatGPT? The goal of this chapter is to take a deep dive into the
                    foundations, origins, and key features of these AI models. By understanding the basics of these
                    models, you will be well on your way to building the next generation of LLM-powered applications.
                </p>
                <section data-type="sect1" data-pdf-bookmark="Introducing Large Language Models">
                    <div class="sect1" id="introducing_large_language_models">
                        <h2>Introducing Large Language Models</h2>
                        <p>This section lays down the fundamental building blocks that have shaped the development of
                            GPT-4 and ChatGPT. We aim to provide a comprehensive understanding of language models and
                            NLP, the role of transformer architectures, and the tokenization and prediction processes
                            within GPT models.</p>
                        <section data-type="sect2"
                            data-pdf-bookmark="Exploring the Foundations of Language Models and NLP">
                            <div class="sect2" id="exploring_the_foundations_of_language_models_and_n">
                                <h3>Exploring the Foundations of Language Models and NLP</h3>
                                <p>As LLMs, GPT-4 and ChatGPT are<a contenteditable="false" data-type="indexterm"
                                        data-primary="large language models (LLMs)"
                                        data-secondary="foundations of language models" id="ch01-foud"></a><a
                                        contenteditable="false" data-type="indexterm"
                                        data-primary="natural language processing (NLP)"
                                        data-secondary="large language models"
                                        data-tertiary="foundations of language models" id="ch01-foud2"></a><a
                                        contenteditable="false" data-type="indexterm" data-primary="ChatGPT (OpenAI)"
                                        data-secondary="as large language model" data-see="large language models"
                                        data-secondary-sortas="large language model" id="id203"></a><a
                                        contenteditable="false" data-type="indexterm" data-primary="GPT-4 (OpenAI)"
                                        data-secondary="as large language model" data-see="large language models"
                                        data-secondary-sortas="large language model" id="id204"></a><a
                                        contenteditable="false" data-type="indexterm" data-primary="GPT models"
                                        data-secondary="large language models" data-see="large language models"
                                        id="id205"></a> the latest type of model obtained in the field of NLP, which is
                                    itself a subfield of machine learning (ML) and AI. Before delving into GPT-4 and
                                    ChatGPT, it is essential to take a look at NLP and its related fields.</p>
                                <p>There are different definitions<a contenteditable="false" data-type="indexterm"
                                        data-primary="AI (artificial intelligence)" data-secondary="definition"
                                        id="id206"></a><a contenteditable="false" data-type="indexterm"
                                        data-primary="ML (machine learning)" data-secondary="definition"
                                        data-tertiary="AI definition" id="id207"></a> of AI, but one of them, more or
                                    less the consensus, says that AI is the development of computer systems that can
                                    perform tasks that typically require human intelligence. With this definition, many
                                    algorithms fall under the AI umbrella. Consider, for example, the traffic prediction
                                    task in GPS applications or the rule-based systems used in strategic video games. In
                                    these examples, seen from the outside, the machine seems to require intelligence to
                                    accomplish these tasks.</p>
                                <p>ML is a subset of AI. In ML,<a contenteditable="false" data-type="indexterm"
                                        data-primary="AI (artificial intelligence)" data-secondary="definition"
                                        data-tertiary="ML definition" id="id208"></a><a contenteditable="false"
                                        data-type="indexterm" data-primary="ML (machine learning)"
                                        data-secondary="definition" id="id209"></a> we do not try to directly implement
                                    the decision rules used by the AI system. Instead, we try to develop algorithms that
                                    allow the system to learn by itself from examples. Since the 1950s, when ML research
                                    began, many ML algorithms have been proposed in the scientific literature. </p>
                                <p class="fix_tracking">Among them, deep learning algorithms<a contenteditable="false"
                                        data-type="indexterm" data-primary="ML (machine learning)"
                                        data-secondary="definition" data-tertiary="deep learning definition"
                                        id="id210"></a><a contenteditable="false" data-type="indexterm"
                                        data-primary="AI (artificial intelligence)" data-secondary="definition"
                                        data-tertiary="deep learning definition" id="id211"></a><a
                                        contenteditable="false" data-type="indexterm" data-primary="deep learning"
                                        id="id212"></a><a contenteditable="false" data-type="indexterm"
                                        data-primary="artificial neural network definition" id="id213"></a> have come to
                                    the fore. <em>Deep learning</em> is a branch of ML that focuses on algorithms
                                    inspired by the structure of the brain. These algorithms are called <em>artificial
                                        neural networks</em>. They can handle very large amounts of data and perform
                                    very well on tasks such as image and speech recognition and NLP.</p>
                                <p class="fix_tracking">GPT-4 and ChatGPT are based<a contenteditable="false"
                                        data-type="indexterm" data-primary="deep learning" data-secondary="transformers"
                                        id="id214"></a><a contenteditable="false" data-type="indexterm"
                                        data-primary="ML (machine learning)" data-secondary="transformers"
                                        id="id215"></a><a contenteditable="false" data-type="indexterm"
                                        data-primary="AI (artificial intelligence)" data-secondary="about transformers"
                                        id="id216"></a><a contenteditable="false" data-type="indexterm"
                                        data-primary="large language models (LLMs)"
                                        data-secondary="foundations of language models"
                                        data-tertiary="about transformers" id="id217"></a><a contenteditable="false"
                                        data-type="indexterm" data-primary="large language models (LLMs)"
                                        data-secondary="Transformer architecture" data-tertiary="about transformers"
                                        id="id218"></a><a contenteditable="false" data-type="indexterm"
                                        data-primary="transformers" data-secondary="about" id="id219"></a><a
                                        contenteditable="false" data-type="indexterm"
                                        data-primary="natural language processing (NLP)"
                                        data-secondary="large language models" data-tertiary="about transformers"
                                        id="id220"></a> on a particular type of deep learning algorithm called
                                    <em>transformers</em>. Transformers are like reading machines. They pay attention to
                                    different parts of a sentence or block of text to understand its context and produce
                                    a coherent response. They can also understand the order of words in a sentence and
                                    their context. This makes them highly effective at tasks such as language
                                    translation, question answering, and text generation. <a data-type="xref"
                                        href="#fig_1_a_nested_set_of_technologies_from_ai_to_transforme">Figure&nbsp;1-1</a>
                                    illustrates the relationships among these terms.</p>
                                <figure>
                                    <div id="fig_1_a_nested_set_of_technologies_from_ai_to_transforme" class="figure">
                                        <img src="/api/v2/epubs/urn:orm:book:9781098152475/files/assets/dagc_0101.png"
                                            alt="" width="600" height="566">
                                        <h6><span class="label">Figure 1-1. </span>A nested set of technologies from AI
                                            to transformers</h6>
                                    </div>
                                </figure>
                                <p>NLP is a subfield of AI focused<a contenteditable="false" data-type="indexterm"
                                        data-primary="AI (artificial intelligence)"
                                        data-secondary="about natural language processing" id="ch01-nlp"></a><a
                                        contenteditable="false" data-type="indexterm"
                                        data-primary="natural language processing (NLP)" data-secondary="about"
                                        id="ch01-nlp2"></a><a contenteditable="false" data-type="indexterm"
                                        data-primary="NLP" data-see="natural language processing" id="id221"></a> on
                                    enabling computers to process, interpret, and generate natural human language.
                                    Modern NLP solutions are based on ML algorithms. The goal of NLP is to allow
                                    computers to process natural language text. This goal covers a wide range of
                                    tasks:<a contenteditable="false" data-type="indexterm"
                                        data-primary="text classification" id="id222"></a><a contenteditable="false"
                                        data-type="indexterm" data-primary="machine translation"
                                        data-secondary="translating automatically as NLP task" id="id223"></a><a
                                        contenteditable="false" data-type="indexterm" data-primary="translation"
                                        data-see="machine translation" id="id224"></a><a contenteditable="false"
                                        data-type="indexterm" data-primary="automatic translation" id="id225"></a><a
                                        contenteditable="false" data-type="indexterm" data-primary="question answering"
                                        id="id226"></a><a contenteditable="false" data-type="indexterm"
                                        data-primary="answering questions" id="id227"></a><a contenteditable="false"
                                        data-type="indexterm" data-primary="text generation" data-secondary="about NLP"
                                        data-seealso="natural language processing" id="id228"></a><a
                                        contenteditable="false" data-type="indexterm" data-primary="prompts"
                                        data-secondary="text generation definition" id="id229"></a><a
                                        contenteditable="false" data-type="indexterm" data-primary="text generation"
                                        data-secondary="prompts" id="id230"></a></p>
                                <dl>
                                    <dt>Text classification </dt>
                                    <dd>
                                        <p>Categorizing input text into predefined groups. This includes, for example,
                                            sentiment analysis and topic categorization. Companies can use sentiment
                                            analysis to understand customers’ opinions about their services. Email
                                            filtering is an example of topic categorization in which email can be put
                                            into categories such as “Personal,” “Social,” “Promotions,” and “Spam.”</p>
                                    </dd>
                                    <dt>Automatic translation </dt>
                                    <dd>
                                        <p>Automatic translation of text from one language to another. Note that this
                                            can include areas like translating code from one programming language to
                                            another, such as from Python to C++.</p>
                                    </dd>
                                    <dt>Question answering </dt>
                                    <dd>
                                        <p>Answering questions based on a given text. For example, an online customer
                                            service portal could use an NLP model to answer FAQs about a product, or
                                            educational software could use NLP to provide answers to students’ questions
                                            about the topic being studied.</p>
                                    </dd>
                                    <dt>Text generation </dt>
                                    <dd>
                                        <p>Generating a coherent and relevant output text based on a given input text,
                                            called a prompt.</p>
                                    </dd>
                                </dl>
                                <p>As mentioned earlier, LLMs are ML models trying to solve text generation tasks, among
                                    others. LLMs enable computers to process, interpret, and generate human language,
                                    allowing for more effective human–machine communication. <a contenteditable="false"
                                        data-type="indexterm" data-primary="large language models (LLMs)"
                                        data-secondary="foundations of language models" data-tertiary="training"
                                        id="id231"></a><a contenteditable="false" data-type="indexterm"
                                        data-primary="training" data-secondary="about large language models"
                                        id="id232"></a><a contenteditable="false" data-type="indexterm"
                                        data-primary="natural language processing (NLP)"
                                        data-secondary="large language models" data-tertiary="about training"
                                        id="id233"></a>To be able to do this, LLMs analyze or <em>train</em> on vast
                                    amounts of text data and thereby learn patterns and relationships between words in
                                    sentences. A variety of data sources can be used to perform this learning process.
                                    This data can include text from Wikipedia, Reddit, the archive of thousands of
                                    books, or even the archive of the internet itself. Given an input text, this
                                    learning process allows the LLMs to make predictions about the likeliest following
                                    words and, in this way, can generate meaningful responses to the input text. The
                                    modern language models, published in the past few months, are so large and have been
                                    trained on so many texts that they can now directly perform most NLP tasks, such as
                                    text classification, machine translation, question answering, and many others. The
                                    GPT-4 and ChatGPT models are modern LLMs that excel at text generation tasks.</p>
                                <p>The development of LLMs <a contenteditable="false" data-type="indexterm"
                                        data-primary="n-grams" id="id234"></a>goes back several years. It started with
                                    simple language models such as <em>n-grams</em>, which tried to predict the next
                                    word in a sentence based on the previous words. N-gram models use <em>frequency</em>
                                    to do this. The predicted next word is the most frequent word that follows the
                                    previous words in the text the n-gram model was trained on. While this approach was
                                    a good start, n-gram models’ need for improvement in understanding context and
                                    grammar resulted in inconsistent text generation.</p>
                                <p>To improve the performance of n-gram models,<a contenteditable="false"
                                        data-type="indexterm" data-primary="recurrent neural networks (RNNs)"
                                        id="id235"></a><a contenteditable="false" data-type="indexterm"
                                        data-primary="RNNs (recurrent neural networks)" id="id236"></a><a
                                        contenteditable="false" data-type="indexterm"
                                        data-primary="long short-term memory (LSTM)" id="id237"></a> more advanced
                                    learning algorithms were introduced, including recurrent neural networks (RNNs) and
                                    long short-term memory (LSTM) networks. These models could learn longer sequences
                                    and analyze the context better than n-grams, but they still needed help processing
                                    large amounts of data efficiently. These types of recurrent models were the most
                                    efficient ones for a long time and therefore were the most used in tools such as
                                    automatic machine translation. <a contenteditable="false" data-type="indexterm"
                                        data-primary="" data-startref="ch01-foud" id="id238"></a><a
                                        contenteditable="false" data-type="indexterm" data-primary=""
                                        data-startref="ch01-foud2" id="id239"></a><a contenteditable="false"
                                        data-type="indexterm" data-primary="" data-startref="ch01-nlp" id="id240"></a><a
                                        contenteditable="false" data-type="indexterm" data-primary=""
                                        data-startref="ch01-nlp2" id="id241"></a></p>
                            </div>
                        </section>
                        <section data-type="sect2"
                            data-pdf-bookmark="Understanding the Transformer Architecture and Its Role in LLMs">
                            <div class="sect2" id="understanding_the_transformer_architecture_and_its">
                                <h3>Understanding the Transformer Architecture and Its Role in LLMs</h3>
                                <p>The Transformer architecture revolutionized NLP,<a contenteditable="false"
                                        data-type="indexterm" data-primary="ML (machine learning)"
                                        data-secondary="transformers" data-tertiary="Transformer architecture"
                                        id="ch01-tra"></a><a contenteditable="false" data-type="indexterm"
                                        data-primary="deep learning" data-secondary="transformers"
                                        data-tertiary="Transformer architecture" id="ch01-tra2"></a><a
                                        contenteditable="false" data-type="indexterm"
                                        data-primary="large language models (LLMs)"
                                        data-secondary="Transformer architecture" id="ch01-tra3"></a><a
                                        contenteditable="false" data-type="indexterm" data-primary="transformers"
                                        data-secondary="Transformer architecture" id="ch01-tra4"></a><a
                                        contenteditable="false" data-type="indexterm"
                                        data-primary="natural language processing (NLP)"
                                        data-secondary="large language models" data-tertiary="Transformer architecture"
                                        id="ch01-tra5"></a><a contenteditable="false" data-type="indexterm"
                                        data-primary="RNNs (recurrent neural networks)"
                                        data-secondary="transformers addressing catastrophic forgetting"
                                        id="id242"></a><a contenteditable="false" data-type="indexterm"
                                        data-primary="recurrent neural networks (RNNs)"
                                        data-secondary="transformers addressing catastrophic forgetting"
                                        id="id243"></a><a contenteditable="false" data-type="indexterm"
                                        data-primary="catastrophic forgetting" data-secondary="transformers addressing"
                                        id="id244"></a> primarily because transformers effectively address one of the
                                    critical limitations of previous NLP models such as RNNs: their struggle with long
                                    text sequences and maintaining context over these lengths. In other words, while
                                    RNNs tended to forget the context in longer sequences (the infamous “catastrophic
                                    forgetting”), transformers came with the ability to handle and encode this context
                                    effectively.</p>
                                <p>The central pillar of this revolution<a contenteditable="false" data-type="indexterm"
                                        data-primary="attention mechanism" data-secondary="about" id="id245"></a><a
                                        contenteditable="false" data-type="indexterm" data-primary="transformers"
                                        data-secondary="Transformer architecture" data-tertiary="attention mechanism"
                                        id="id246"></a><a contenteditable="false" data-type="indexterm"
                                        data-primary="cross-attention" data-secondary="attention mechanism as basis"
                                        id="id247"></a><a contenteditable="false" data-type="indexterm"
                                        data-primary="self-attention" data-secondary="attention mechanism as basis"
                                        id="id248"></a><a contenteditable="false" data-type="indexterm"
                                        data-primary="large language models (LLMs)"
                                        data-secondary="Transformer architecture" data-tertiary="attention mechanism"
                                        id="id249"></a> is the <em>attention mechanism</em>, a simple yet powerful idea.
                                    Instead of treating all words in a text sequence as equally important, the model
                                    “pays attention” to the most relevant terms for each step of its task.
                                    Cross-attention and self-attention are two architectural blocks based on this
                                    attention mechanism, and they are often found in LLMs. The Transformer architecture
                                    makes extensive use of these cross-attention and self-attention blocks.</p>
                                <p><em>Cross-attention</em> helps the model<a contenteditable="false"
                                        data-type="indexterm" data-primary="cross-attention" id="id250"></a><a
                                        contenteditable="false" data-type="indexterm" data-primary="attention mechanism"
                                        data-secondary="cross-attention" id="id251"></a> determine the relevance of the
                                    different parts of the input text for accurately predicting the next word in the
                                    output text. It’s like a spotlight that shines on words or phrases in the input
                                    text, highlighting the relevant information needed to make the next word prediction
                                    while ignoring less important details. </p>
                                <p>To illustrate this, let’s take an example<a contenteditable="false"
                                        data-type="indexterm" data-primary="machine translation"
                                        data-secondary="cross-attention process" id="id252"></a> of a simple sentence
                                    translation task. Imagine we have an input English sentence, “Alice enjoyed the
                                    sunny weather in Brussels,” which should be translated into French as “Alice a
                                    profité du temps ensoleillé à Bruxelles.” In this example, let us focus on
                                    generating the French word <em>ensoleillé</em>, which means <em>sunny</em>. For this
                                    prediction, cross-attention would give more weight to the English words
                                    <em>sunny</em> and <em>weather</em> since they are both relevant to the meaning of
                                    <em>ensoleillé</em>. By focusing on these two words, cross-attention helps the model
                                    generate an accurate translation for this part of the sentence. <a data-type="xref"
                                        href="#fig_2_cross_attention_uses_the_attention_mechanism_to_fo">Figure&nbsp;1-2</a>
                                    illustrates this <span class="keep-together">example.</span></p>
                                <figure>
                                    <div id="fig_2_cross_attention_uses_the_attention_mechanism_to_fo" class="figure">
                                        <img src="/api/v2/epubs/urn:orm:book:9781098152475/files/assets/dagc_0102.png"
                                            alt="" width="600" height="209">
                                        <h6><span class="label">Figure 1-2. </span>Cross-attention uses the attention
                                            mechanism to focus on essential parts of the input text (English sentence)
                                            to predict the next word in the output text (French sentence)</h6>
                                    </div>
                                </figure>
                                <p><em>Self-attention</em> refers to the<a contenteditable="false" data-type="indexterm"
                                        data-primary="self-attention" id="id253"></a><a contenteditable="false"
                                        data-type="indexterm" data-primary="attention mechanism"
                                        data-secondary="self-attention" id="id254"></a> ability of a model to focus on
                                    different parts of its input text. In the context of NLP, the model can evaluate the
                                    importance of each word in a sentence with the other words. <a
                                        contenteditable="false" data-type="indexterm"
                                        data-primary="concepts from self-attention" id="id255"></a>This allows it to
                                    better understand the relationships between the words and helps the model build new
                                    <em>concepts</em> from multiple words in the input text.</p>
                                <p class="fix_tracking2">As a more specific example, consider the following: “Alice
                                    received praise from her colleagues.” Assume that the model is trying to understand
                                    the meaning of the word <em>her</em> in the sentence. The self-attention mechanism
                                    assigns different weights to the words in the sentence, highlighting the words
                                    relevant to <em>her</em> in this context. In this example, self-attention would
                                    place more weight on the words <em>Alice</em> and <em>colleagues</em>.
                                    Self-attention helps the model build new concepts from these words. In this example,
                                    one of the concepts that could emerge would be “Alice’s colleagues,” as shown in <a
                                        data-type="xref"
                                        href="#fig_3_self_attention_allows_the_emergence_of_the_alice">Figure&nbsp;1-3</a>.
                                </p>
                                <figure>
                                    <div id="fig_3_self_attention_allows_the_emergence_of_the_alice" class="figure">
                                        <img src="/api/v2/epubs/urn:orm:book:9781098152475/files/assets/dagc_0103.png"
                                            alt="" width="600" height="345">
                                        <h6><span class="label">Figure 1-3. </span>Self-attention allows the emergence
                                            of the “Alice’s colleagues” concept</h6>
                                    </div>
                                </figure>
                                <p>Unlike the recurrent architecture, <a contenteditable="false" data-type="indexterm"
                                        data-primary="transformers" data-secondary="parallel processing"
                                        id="id256"></a><a contenteditable="false" data-type="indexterm"
                                        data-primary="parallel processing by transformers" id="id257"></a>transformers
                                    also have the advantage of being easily <em>parallelized</em>. This means the
                                    Transformer architecture can process multiple parts of the input text simultaneously
                                    rather than sequentially. This allows faster computation and training because
                                    different parts of the model can work in parallel without waiting for previous steps
                                    to complete, <a contenteditable="false" data-type="indexterm"
                                        data-primary="recurrent neural networks (RNNs)"
                                        data-secondary="sequential processing" id="id258"></a><a contenteditable="false"
                                        data-type="indexterm" data-primary="RNNs (recurrent neural networks)"
                                        data-secondary="sequential processing" id="id259"></a><a contenteditable="false"
                                        data-type="indexterm"
                                        data-primary="graphics processing unit parallel processing architecture"
                                        id="id260"></a><a contenteditable="false" data-type="indexterm"
                                        data-primary="parallel processing architecture of GPUs" id="id261"></a><a
                                        contenteditable="false" data-type="indexterm"
                                        data-primary="GPU parallel processing architecture" id="id262"></a>unlike
                                    recurrent architectures, which require sequential processing. The parallel
                                    processing capability of transformer models fits perfectly with the architecture of
                                    graphics processing units (GPUs), which are designed to handle multiple computations
                                    simultaneously. Therefore, GPUs are ideal for training and running these transformer
                                    models because of their high parallelism and computational power. This advance
                                    allowed data scientists to train models on much larger datasets, paving the way for
                                    developing LLMs. </p>
                                <p>The Transformer architecture, introduced<a contenteditable="false"
                                        data-type="indexterm"
                                        data-primary="“Attention Is All You Need” (Vaswani et al.)"
                                        data-primary-sortas="Attention Is All" id="id263"></a><a contenteditable="false"
                                        data-type="indexterm" data-primary="Vaswani, Ashish" id="id264"></a> in 2017 by
                                    Vaswani et al. from Google in the paper “<a href="https://oreil.ly/jVZW1">Attention
                                        Is All You Need”</a>, was originally developed for sequence-to-sequence tasks
                                    such as machine translation. <a contenteditable="false" data-type="indexterm"
                                        data-primary="transformers" data-secondary="Transformer architecture"
                                        data-tertiary="encoder and decoder" id="id265"></a><a contenteditable="false"
                                        data-type="indexterm" data-primary="large language models (LLMs)"
                                        data-secondary="Transformer architecture" data-tertiary="encoder and decoder"
                                        id="id266"></a><a contenteditable="false" data-type="indexterm"
                                        data-primary="attention mechanism"
                                        data-secondary="encoder and decoder of transformers" id="id267"></a><a
                                        contenteditable="false" data-type="indexterm"
                                        data-primary="encoder of transformer" id="id268"></a><a contenteditable="false"
                                        data-type="indexterm" data-primary="decoder of transformer" id="id269"></a><a
                                        contenteditable="false" data-type="indexterm" data-primary="embeddings"
                                        data-secondary="encoder into decoder" id="id270"></a>A standard transformer
                                    consists of two primary components: an encoder and a decoder, both of which rely
                                    heavily on attention mechanisms. The task of the encoder is to process the input
                                    text, identify valuable features, and generate a meaningful representation of that
                                    text, known as <em>embedding</em>. The decoder then uses this embedding to produce
                                    an output, such as a translation or summary. This output effectively interprets the
                                    encoded information. </p>
                                <p><em>Generative pre-trained transformers</em>, commonly<a contenteditable="false"
                                        data-type="indexterm" data-primary="generative pre-trained transformers (GPT)"
                                        id="id271"></a><a contenteditable="false" data-type="indexterm"
                                        data-primary="GPT models" data-secondary="about"
                                        data-tertiary="as generative pre-trained transformers"
                                        data-tertiary-sortas="generative pre-trained transformers" id="id272"></a><a
                                        contenteditable="false" data-type="indexterm" data-primary="transformers"
                                        data-secondary="Transformer architecture" data-tertiary="GPT models"
                                        id="id273"></a><a contenteditable="false" data-type="indexterm"
                                        data-primary="encoder of transformer" data-secondary="not present in GPT"
                                        id="id274"></a><a contenteditable="false" data-type="indexterm"
                                        data-primary="decoder of transformer" data-secondary="GPT utilizing"
                                        id="id275"></a><a contenteditable="false" data-type="indexterm"
                                        data-primary="cross-attention" data-secondary="GPT not utilizing"
                                        id="id276"></a><a contenteditable="false" data-type="indexterm"
                                        data-primary="self-attention" data-secondary="GPT utilizing" id="id277"></a><a
                                        contenteditable="false" data-type="indexterm"
                                        data-primary="large language models (LLMs)"
                                        data-secondary="Transformer architecture" data-tertiary="GPT models"
                                        id="id278"></a><a contenteditable="false" data-type="indexterm"
                                        data-primary="natural language processing (NLP)"
                                        data-secondary="about GPT models" id="id279"></a> known as <em>GPT</em>, are a
                                    family of models that are based on the Transformer architecture and that
                                    specifically utilize the decoder part of the original architecture. In GPT, the
                                    encoder is not present, so there is no need for cross-attention to integrate the
                                    embeddings produced by an encoder. As a result, GPT relies solely on the
                                    self-attention mechanism within the decoder to generate context-aware
                                    representations and predictions. <a contenteditable="false" data-type="indexterm"
                                        data-primary="BERT (Bidirectional Encoder Representations from Transformers)"
                                        id="id280"></a>Note that other well-known models, such as BERT (Bidirectional
                                    Encoder Representations from Transformers), are based on the encoder part. We don’t
                                    cover this type of model in this book. <a data-type="xref"
                                        href="#fig_4_the_evolution_of_nlp_techniques_from_n_grams_to_th">Figure&nbsp;1-4</a>
                                    illustrates the evolution of these different models.<a contenteditable="false"
                                        data-type="indexterm" data-primary="" data-startref="ch01-tra" id="id281"></a><a
                                        contenteditable="false" data-type="indexterm" data-primary=""
                                        data-startref="ch01-tra2" id="id282"></a><a contenteditable="false"
                                        data-type="indexterm" data-primary="" data-startref="ch01-tra3"
                                        id="id283"></a><a contenteditable="false" data-type="indexterm" data-primary=""
                                        data-startref="ch01-tra4" id="id284"></a><a contenteditable="false"
                                        data-type="indexterm" data-primary="" data-startref="ch01-tra5" id="id285"></a>
                                </p>
                                <figure>
                                    <div id="fig_4_the_evolution_of_nlp_techniques_from_n_grams_to_th" class="figure">
                                        <img src="/api/v2/epubs/urn:orm:book:9781098152475/files/assets/dagc_0104.png"
                                            alt="" width="600" height="101">
                                        <h6><span class="label">Figure 1-4. </span>The evolution of NLP techniques from
                                            n-grams to the emergence of LLMs</h6>
                                    </div>
                                </figure>
                            </div>
                        </section>
                        <section data-type="sect2" class="pagebreak-before"
                            data-pdf-bookmark="Demystifying the Tokenization and Prediction Steps in GPT Models">
                            <div class="sect2" id="demystifying_the_tokenization_and_prediction_steps">
                                <h3 class="less_space">Demystifying the Tokenization and Prediction Steps in GPT Models
                                </h3>
                                <p>LLMs in the GPT family receive<a contenteditable="false" data-type="indexterm"
                                        data-primary="large language models (LLMs)"
                                        data-secondary="tokenization and prediction steps in GPT" id="id286"></a><a
                                        contenteditable="false" data-type="indexterm" data-primary="GPT models"
                                        data-secondary="tokenization and prediction steps" id="id287"></a><a
                                        contenteditable="false" data-type="indexterm" data-primary="text completion"
                                        data-secondary="tokenization and prediction steps in GPT" id="id288"></a><a
                                        contenteditable="false" data-type="indexterm"
                                        data-primary="tokenization in GPT models" id="id289"></a><a
                                        contenteditable="false" data-type="indexterm" data-primary="prompts"
                                        data-secondary="tokenization" id="id290"></a> a prompt as input, and in response
                                    they generate a text. This process is known as <em>text completion</em>. For
                                    example, the prompt could be “<em>The weather is nice today, so I decided to</em>”
                                    and the model output might be “<em>go for a walk</em>”. You may be wondering how the
                                    LLM model builds this output text from the input prompt. As you will see, it’s
                                    mostly just a question of probabilities. </p>
                                <p>When a prompt is sent to an LLM, it first breaks<a contenteditable="false"
                                        data-type="indexterm" data-primary="tokenization in GPT models"
                                        data-secondary="tokens" id="id291"></a> the input into smaller pieces called
                                    <em>tokens</em>. These tokens represent single words, parts of words, or spaces and
                                    punctuation. For example, the preceding prompt could be broken like this:
                                    [“<em>The”, “wea”, “ther”, “is”, “nice”, “today”, “,”, “so”, “I”, “de”, “ci”, “ded”,
                                        “to</em>”]. Each language model comes with its own tokenizer. The GPT-4
                                    tokenizer is not available at the time of this writing, <a contenteditable="false"
                                        data-type="indexterm" data-primary="tokenization in GPT models"
                                        data-secondary="GPT-3 tokenizer link" id="id292"></a><a contenteditable="false"
                                        data-type="indexterm" data-primary="GPT-3 (OpenAI)"
                                        data-secondary="tokenizer link" id="id293"></a><a contenteditable="false"
                                        data-type="indexterm" data-primary="resources online" data-secondary="OpenAI"
                                        data-tertiary="GPT-3 tokenizer" id="id294"></a>but you can test the <a
                                        href="https://platform.openai.com/tokenizer">GPT-3 tokenizer</a>.<a
                                        contenteditable="false" data-type="indexterm"
                                        data-primary="tokenization in GPT models" data-secondary="tokens"
                                        data-tertiary="100 tokens as 75 English words" id="id295"></a> </p>
                                <div data-type="tip">
                                    <h6>Tip</h6>
                                    <p>A rule of thumb for understanding tokens in terms of word length is that 100
                                        tokens equal approximately 75 words for an English text.</p>
                                </div>
                                <p>Thanks to the attention principle and the Transformer architecture introduced
                                    earlier, the LLM processes these tokens and can interpret the relationships between
                                    them and the overall meaning of the prompt. The Transformer architecture allows a
                                    model to efficiently identify the critical information and the context within the
                                    text. </p>
                                <p>To create a new sentence, the LLM predicts the tokens most likely to follow, based on
                                    the context of the prompt. <a contenteditable="false" data-type="indexterm"
                                        data-primary="GPT-4 (OpenAI)" data-secondary="token context window versions"
                                        id="id296"></a>OpenAI produced two versions of GPT-4, with context windows of
                                    8,192 tokens and 32,768 tokens. Unlike the previous recurrent models, which had
                                    difficulty handling long input sequences, the Transformer architecture with the
                                    attention mechanism allows the modern LLM to consider the context as a whole. Based
                                    on this context, the model assigns a probability score for each potential subsequent
                                    token. The token with the highest probability is then selected as the next token in
                                    the sequence. In our example, after “The weather is nice today, so I decided to”,
                                    the next best token could be “go”.</p>
                                <p>This process is then repeated, but now the context becomes “The weather is nice
                                    today, so I decided to go”, where the previously predicted token “go” is added to
                                    the original prompt. The second token that the model might predict could be “for”.
                                    This process is repeated until a complete sentence is formed: “go for a walk”. This
                                    process relies on the LLM’s ability to learn the next most probable word from
                                    massive text data. <a data-type="xref"
                                        href="#fig_5_the_completion_process_is_iterative_token_by_toke">Figure&nbsp;1-5</a>
                                    illustrates this process.</p>
                                <figure>
                                    <div id="fig_5_the_completion_process_is_iterative_token_by_toke" class="figure">
                                        <img src="/api/v2/epubs/urn:orm:book:9781098152475/files/assets/dagc_0105.png"
                                            alt="" width="600" height="401">
                                        <h6><span class="label">Figure 1-5. </span>The completion process is iterative,
                                            token by token</h6>
                                    </div>
                                </figure>
                            </div>
                        </section>
                    </div>
                </section>
                <section data-type="sect1" data-pdf-bookmark="A Brief History: From GPT-1 to GPT-4">
                    <div class="sect1" id="a_brief_history_from_gpt_1_to_gpt_4">
                        <h2>A Brief History: From GPT-1 to GPT-4</h2>
                        <p>In this section, we will review the evolution of the OpenAI GPT models from GPT-1 to GPT-4.<a
                                contenteditable="false" data-type="indexterm" data-primary="ChatGPT (OpenAI)"
                                data-secondary="history of" data-tertiary="starting with GPT-1" id="ch01-hst"></a><a
                                contenteditable="false" data-type="indexterm" data-primary="GPT models"
                                data-secondary="history of" id="ch01-hst2"></a> </p>
                        <section data-type="sect2" data-pdf-bookmark="GPT-1">
                            <div class="sect2" id="gpt_1_idPSUVz9">
                                <h3>GPT-1</h3>
                                <p>In mid-2018, just one year<a contenteditable="false" data-type="indexterm"
                                        data-primary="GPT-1 (OpenAI)" id="id297"></a><a contenteditable="false"
                                        data-type="indexterm"
                                        data-primary="“Improving Language Understanding by Generative Pre-Training” (Radford et al.)"
                                        data-primary-sortas="Improving Language Understanding" id="id298"></a><a
                                        contenteditable="false" data-type="indexterm" data-primary="Radford, Alec"
                                        id="id299"></a><a contenteditable="false" data-type="indexterm"
                                        data-primary="generative pre-trained transformers (GPT)" data-secondary="GPT-1"
                                        id="id300"></a><a contenteditable="false" data-type="indexterm"
                                        data-primary="history of GPT models" data-secondary="GPT-1" id="id301"></a>
                                    after the invention of the Transformer architecture, OpenAI published a paper titled
                                    <a href="https://oreil.ly/Yakwa">“Improving Language Understanding by Generative
                                        Pre-Training”</a>, by Radford et al., in which the company introduced the
                                    Generative Pre-trained Transformer, also known as GPT-1. </p>
                                <p>Before GPT-1, the common approach<a contenteditable="false" data-type="indexterm"
                                        data-primary="supervised learning before GPT" id="id302"></a> to building
                                    high-performance NLP neural models relied on supervised learning. These learning
                                    techniques use large amounts of manually labeled data. For example, in a sentiment
                                    analysis task where the goal is to classify whether a given text has positive or
                                    negative sentiment, a common strategy would require collecting thousands of manually
                                    labeled text examples to build an effective classification model. However, the need
                                    for large amounts of well-annotated, supervised data has limited the performance of
                                    these techniques because such datasets are both difficult and expensive to generate.
                                </p>
                                <p class="pagebreak-before less_space">In their paper, the authors of GPT-1 proposed a
                                    new learning process in which an unsupervised pre-training step is introduced. In
                                    this pre-training step, no labeled data is needed. Instead, the model is trained to
                                    predict what the next token is. Thanks to the use of the Transformer architecture,
                                    which allows parallelization, this pre-training was performed on a large amount of
                                    data. <a contenteditable="false" data-type="indexterm"
                                        data-primary="BookCorpus dataset" id="id303"></a><a contenteditable="false"
                                        data-type="indexterm" data-primary="datasets" data-secondary="BookCorpus"
                                        id="id304"></a><a contenteditable="false" data-type="indexterm"
                                        data-primary="training" data-secondary="pretraining"
                                        data-tertiary="BookCorpus dataset" id="id305"></a><a contenteditable="false"
                                        data-type="indexterm" data-primary="pretraining"
                                        data-secondary="BookCorpus dataset" id="id306"></a><a contenteditable="false"
                                        data-type="indexterm" data-primary="“Aligning Books and Movies” (Zhu et al.)"
                                        data-primary-sortas="Aligning Books and Movies" id="id307"></a><a
                                        contenteditable="false" data-type="indexterm" data-primary="Zhu, Yukun"
                                        id="id308"></a>For the pre-training, the GPT-1 model used the <em>BookCorpus
                                        dataset</em>, which contains the text of approximately 11,000 unpublished books.
                                    This dataset was initially presented in 2015 in the scientific paper <a
                                        href="https://oreil.ly/3hWl1">“Aligning Books and Movies: Towards Story-Like
                                        Visual Explanations by Watching Movies and Reading Books”</a> by Zhu et al., and
                                    was initially made available on a University of Toronto web page. However, today the
                                    official version of the original dataset is no longer publicly accessible.</p>
                                <p>The GPT-1 model was found to be effective in a variety of basic completion tasks. In
                                    the unsupervised learning phase, the model learned to predict the next item in the
                                    texts of the BookCorpus dataset. However, since GPT-1 is a small model, it was
                                    unable to perform complex tasks without fine-tuning. Therefore, fine-tuning was
                                    performed as a second supervised learning step on a small set of manually labeled
                                    data to adapt the model to a specific target task. For example, in a classification
                                    task such as sentiment analysis, it may be necessary to retrain the model on a small
                                    set of manually labeled text examples to achieve reasonable accuracy. This process
                                    allowed the parameters learned in the initial pre-training phase to be modified to
                                    better fit the task at hand. </p>
                                <p>Despite its relatively small size, GPT-1 showed remarkable performance on several NLP
                                    tasks using only a small amount of manually labeled data for fine-tuning. The GPT-1
                                    architecture consisted of a decoder similar to the original transformer, which was
                                    introduced in 2017 and had 117 million parameters. This first GPT model paved the
                                    way for more powerful models with larger datasets and more parameters to take better
                                    advantage of the potential of the Transformer architecture.</p>
                            </div>
                        </section>
                        <section data-type="sect2" data-pdf-bookmark="GPT-2">
                            <div class="sect2" id="gpt_2_idpSpGu9">
                                <h3>GPT-2</h3>
                                <p>In early 2019, OpenAI proposed GPT-2,<a contenteditable="false" data-type="indexterm"
                                        data-primary="history of GPT models" data-secondary="GPT-2" id="id309"></a><a
                                        contenteditable="false" data-type="indexterm" data-primary="GPT-2 (OpenAI)"
                                        id="id310"></a> a scaled-up version of the GPT-1 model that increased the number
                                    of parameters and the size of the training dataset tenfold. The number of parameters
                                    of this new version was 1.5 billion, trained on 40 GB of text. In November 2019,
                                    OpenAI released the full version of the GPT-2 language model.<a
                                        contenteditable="false" data-type="indexterm" data-primary="GPT-2 (OpenAI)"
                                        data-secondary="download link" id="id311"></a><a contenteditable="false"
                                        data-type="indexterm" data-primary="resources online" data-secondary="OpenAI"
                                        data-tertiary="GPT-2 download" id="id312"></a> </p>
                                <div data-type="note" epub:type="note" class="pagebreak-before less_space">
                                    <h6>Note</h6>
                                    <p>GPT-2 is publicly available and can be downloaded from <a
                                            href="https://huggingface.co/gpt2">Hugging Face</a> or <a
                                            href="https://github.com/openai/gpt-2">GitHub</a>. </p>
                                </div>
                                <p>GPT-2 showed that training a larger language model on a larger dataset improves the
                                    ability of a language model to process tasks and outperforms the state of the art on
                                    many jobs. It also showed that even larger language models can process natural
                                    language better.</p>
                            </div>
                        </section>
                        <section data-type="sect2" data-pdf-bookmark="GPT-3">
                            <div class="sect2" id="gpt_3_idoFnMWH">
                                <h3>GPT-3</h3>
                                <p>OpenAI released version 3 of GPT<a contenteditable="false" data-type="indexterm"
                                        data-primary="history of GPT models" data-secondary="GPT-3" id="ch01-hgp"></a><a
                                        contenteditable="false" data-type="indexterm" data-primary="GPT-3 (OpenAI)"
                                        data-secondary="history of" id="ch01-hgp2"></a> in June 2020. The main
                                    differences between GPT-2 and GPT-3 are the size of the model and the quantity of
                                    data used for the training. GPT-3 is a much larger model than GPT-2, with 175
                                    billion parameters, allowing it to capture more complex patterns. <a
                                        contenteditable="false" data-type="indexterm"
                                        data-primary="Common Crawl dataset" id="id313"></a><a contenteditable="false"
                                        data-type="indexterm" data-primary="datasets" data-secondary="Common Crawl"
                                        id="id314"></a><a contenteditable="false" data-type="indexterm"
                                        data-primary="training" data-secondary="Common Crawl dataset" id="id315"></a>In
                                    addition, GPT-3 was trained on a more extensive dataset. This includes <a
                                        href="https://commoncrawl.org">Common Crawl</a>, a large web archive containing
                                    text from billions of web pages and other sources, such as Wikipedia. This training
                                    dataset, which includes content from websites, books, and articles, allows GPT-3 to
                                    develop a deeper understanding of the language and context. As a result, GPT-3
                                    demonstrates improved performance on a variety of linguistics tasks. It also
                                    demonstrates superior coherence and creativity in its generated texts. It is even
                                    capable of writing code snippets, such as SQL queries, and performing other
                                    intelligent tasks. Furthermore, GPT-3 eliminates the need for a fine-tuning step,
                                    which was mandatory for its predecessors. </p>
                                <p>However, with GPT-3 there is a problem of misalignment between the tasks given by end
                                    users and what the model has seen during its training. As we have seen, language
                                    models are trained to predict the next token based on the input context. This
                                    training process is not necessarily directly aligned with the tasks end users want
                                    the model to perform. In addition, increasing the size of language models does not
                                    inherently make them better at following user intent or instructions. Moreover,
                                    models like GPT-3 were trained on data from different sources on the internet.
                                    Although a cleanup has been made in the selection of sources, the learning data may
                                    contain false or problematic text, including racist text, sexist text,
                                    misinformation, or disinformation. As a result, the model may sometimes say the
                                    wrong things or even toxic things. <a contenteditable="false" data-type="indexterm"
                                        data-primary="GPT-3 (OpenAI)" data-secondary="InstructGPT" id="id316"></a><a
                                        contenteditable="false" data-type="indexterm" data-primary="InstructGPT"
                                        id="id317"></a><a contenteditable="false" data-type="indexterm"
                                        data-primary="training" data-secondary="GPT-3 Instruct series"
                                        data-tertiary="reinforcement learning with human feedback" id="ch01-rei"></a><a
                                        contenteditable="false" data-type="indexterm"
                                        data-primary="reinforcement learning from human feedback (RLHF)"
                                        id="ch01-rei2"></a><a contenteditable="false" data-type="indexterm"
                                        data-primary="RLHF" data-see="reinforcement learning from human feedback"
                                        id="id318"></a>In 2021 a new release of the GPT-3 model was published, called
                                    the <em>Instruct series</em>. Unlike the original GPT-3 base model, the instruct
                                    models are optimized by <em>reinforcement learning</em> with human feedback, meaning
                                    that they use the feedback to learn and to improve over time. This allows the models
                                    to learn from human instructions while making them more truthful and less toxic.
                                </p>
                                <p class="pagebreak-before less_space">To illustrate the difference, we input the prompt
                                    “Explain what is meant by time complexity”, and we receive the following:</p>
                                <ul>
                                    <li>
                                        <p>With the standard GPT-3, we obtain the output: “<em>Explain what is meant by
                                                space complexity. Explain what is meant by the big-O notation</em>.”</p>
                                    </li>
                                    <li>
                                        <p>With the instructGPT-3 model, we obtain: “<em>Time complexity is a way of
                                                measuring the amount of time it takes for an algorithm to run and
                                                complete its task. It is usually expressed using Big O notation, which
                                                measures the complexity of an algorithm in terms of the number of
                                                operations it performs. The time complexity of an algorithm is important
                                                because it determines how efficient the algorithm is and how well it
                                                scales with larger inputs.</em>” </p>
                                    </li>
                                </ul>
                                <p>We can see that for the same input, the first model cannot answer the question (the
                                    answer is even weird), whereas the second model does answer the question. <a
                                        contenteditable="false" data-type="indexterm" data-primary="prompts"
                                        data-secondary="GPT-3 prompt engineering" id="id319"></a><a
                                        contenteditable="false" data-type="indexterm" data-primary="GPT-3 (OpenAI)"
                                        data-secondary="prompt engineering" id="id320"></a><a contenteditable="false"
                                        data-type="indexterm" data-primary="prompt engineering"
                                        data-secondary="GPT-3 model" id="id321"></a>It is, of course, possible to obtain
                                    the desired response with a standard GPT-3 model. However, contrary to instruction
                                    models, it is necessary to apply specific prompt design and optimization techniques
                                    to obtain the desired output from the GPT-3 model. This technique is called
                                    <em>prompt engineering</em> and will be detailed in the coming chapters.</p>
                            </div>
                        </section>
                        <section data-type="sect2" data-pdf-bookmark="From GPT-3 to InstructGPT">
                            <div class="sect2" id="from_gpt_3_to_instructgpt">
                                <h3>From GPT-3 to InstructGPT</h3>
                                <p>OpenAI explains how the Instruct series<a contenteditable="false"
                                        data-type="indexterm" data-primary="InstructGPT"
                                        data-secondary="from GPT-3 to InstructGPT" id="ch01-ins2"></a><a
                                        contenteditable="false" data-type="indexterm" data-primary="training"
                                        data-secondary="GPT-3 Instruct series" data-tertiary="from GPT-3 to InstructGPT"
                                        id="ch01-ins3"></a><a contenteditable="false" data-type="indexterm"
                                        data-primary="GPT-3 (OpenAI)" data-secondary="InstructGPT"
                                        data-tertiary="from GPT-3 to InstructGPT" id="ch01-ins4"></a> was constructed in
                                    the scientific paper <a href="https://oreil.ly/sz90A">“Training Language Models to
                                        Follow Instructions with Human Feedback”</a> by Ouyang et al.<a
                                        contenteditable="false" data-type="indexterm" data-primary="Ouyang, Long"
                                        id="id322"></a><a contenteditable="false" data-type="indexterm"
                                        data-primary="“Training Language Models to Follow Instructions with Human Feedback” (Ouyang et al.)"
                                        data-primary-sortas="Training language models to follow" id="id323"></a> </p>
                                <p>The training recipe has two main stages<a contenteditable="false"
                                        data-type="indexterm" data-primary="supervised fine-tuning (SFT) model"
                                        id="ch01-sft"></a><a contenteditable="false" data-type="indexterm"
                                        data-primary="SFT (supervised fine-tuning) model" id="ch01-sft2"></a> to go from
                                    a GPT-3 model to an instructed GPT-3 model: <em>supervised fine-tuning</em>
                                    (<em>SFT</em>) and <em>reinforcement learning from human feedback</em>
                                    (<em>RLHF</em>). In each stage, the results of the prior stage are fine-tuned. That
                                    is, the SFT stage receives the GPT-3 model and returns a new model, which is sent to
                                    the RLHF stage to obtain the instructed version.</p>
                                <p><a data-type="xref"
                                        href="#fig_6_the_steps_to_obtain_the_instructed_models_from_an">Figure&nbsp;1-6</a>,
                                    from the scientific paper from OpenAI, details the entire process.</p>
                                <figure>
                                    <div id="fig_6_the_steps_to_obtain_the_instructed_models_from_an" class="figure">
                                        <img src="/api/v2/epubs/urn:orm:book:9781098152475/files/assets/dagc_0106.png"
                                            alt="" width="564" height="800">
                                        <h6><span class="label">Figure 1-6. </span>The steps to obtain the instructed
                                            models (redrawn from an image by Ouyang et al.)</h6>
                                    </div>
                                </figure>
                                <p>We will step through these stages one by one. </p>
                                <p>In the SFT stage, the original GPT-3 model is fine-tuned with straightforward
                                    supervised learning (step 1 in <a data-type="xref"
                                        href="#fig_6_the_steps_to_obtain_the_instructed_models_from_an">Figure&nbsp;1-6</a>).
                                    OpenAI has a collection of prompts made by end users. The process starts with the
                                    random selection of a prompt from the set of available prompts. <a
                                        contenteditable="false" data-type="indexterm" data-primary="training"
                                        data-secondary="human labelers" id="id324"></a><a contenteditable="false"
                                        data-type="indexterm" data-primary="human labelers for training"
                                        id="id325"></a><a contenteditable="false" data-type="indexterm"
                                        data-primary="labelers for training" id="id326"></a>A human (called a
                                    <em>labeler</em>) is then asked to write an example of an ideal answer to this
                                    prompt. This process is repeated thousands of times to obtain a supervised training
                                    set composed of prompts and the corresponding ideal responses. This dataset is then
                                    used to fine-tune the GPT-3 model to give more consistent answers to user requests.
                                    The resulting model is called the SFT model.<a contenteditable="false"
                                        data-type="indexterm" data-primary="" data-startref="ch01-sft" id="id327"></a><a
                                        contenteditable="false" data-type="indexterm" data-primary=""
                                        data-startref="ch01-sft2" id="id328"></a></p>
                                <p>The RLHF stage is divided into two substeps. <a contenteditable="false"
                                        data-type="indexterm"
                                        data-primary="reinforcement learning from human feedback (RLHF)"
                                        data-secondary="reward model" id="id329"></a><a contenteditable="false"
                                        data-type="indexterm" data-primary="reward model (RM)" id="id330"></a>First a
                                    reward model (RM) is built (step 2 in <a data-type="xref"
                                        href="#fig_6_the_steps_to_obtain_the_instructed_models_from_an">Figure&nbsp;1-6</a>),
                                    and then the RM is used for reinforcement learning (step 3 in <a data-type="xref"
                                        href="#fig_6_the_steps_to_obtain_the_instructed_models_from_an">Figure&nbsp;1-6</a>).
                                </p>
                                <p>The goal of the RM is to automatically give a score to a response to a prompt. When
                                    the response matches what is indicated in the prompt, the RM score should be high;
                                    when it doesn’t match, it should be low. To construct the RM, OpenAI begins by
                                    randomly selecting a question and using the SFT model to produce several possible
                                    answers. <a contenteditable="false" data-type="indexterm"
                                        data-primary="temperature input parameter for chat completion" id="id331"></a>As
                                    we will see later, it is possible to produce many responses with the same input
                                    prompt via a parameter called <em>temperature</em>. A human labeler is then asked to
                                    rank the responses based on criteria such as fit with the prompt and toxicity of the
                                    response. After running this procedure many times, a dataset is used to fine-tune
                                    the SFT model for scoring. This RM will be used to build the final InstructGPT
                                    model.</p>
                                <p>The final step in training InstructGPT models involves reinforcement learning, which
                                    is an iterative process. It starts with an initial generative model, such as the SFT
                                    model. Then a random prompt is selected, and the model predicts an output, which the
                                    RM evaluates. Based on the reward received, the generative model is updated
                                    accordingly. This process can be repeated countless times without human
                                    intervention, providing a more efficient and automated approach to adapting the
                                    model for better performance.</p>
                                <p>InstructGPT models are better at producing accurate completions for what people give
                                    as input in the prompt. <a contenteditable="false" data-type="indexterm"
                                        data-primary="InstructGPT" data-secondary="use instead of GPT-3"
                                        id="id332"></a><a contenteditable="false" data-type="indexterm"
                                        data-primary="GPT-3 (OpenAI)" data-secondary="InstructGPT"
                                        data-tertiary="use instead of GPT-3" id="id333"></a>OpenAI recommends using the
                                    InstructGPT series rather than the original series.<a contenteditable="false"
                                        data-type="indexterm" data-primary="" data-startref="ch01-ins2"
                                        id="id334"></a><a contenteditable="false" data-type="indexterm" data-primary=""
                                        data-startref="ch01-ins3" id="id335"></a><a contenteditable="false"
                                        data-type="indexterm" data-primary="" data-startref="ch01-ins4"
                                        id="id336"></a><a contenteditable="false" data-type="indexterm" data-primary=""
                                        data-startref="ch01-rei" id="id337"></a><a contenteditable="false"
                                        data-type="indexterm" data-primary="" data-startref="ch01-rei2" id="id338"></a>
                                </p>
                            </div>
                        </section>
                        <section data-type="sect2" data-pdf-bookmark="GPT-3.5, Codex, and ChatGPT">
                            <div class="sect2" id="gpt_3_5_codex_and_chatgpt">
                                <h3>GPT-3.5, Codex, and ChatGPT</h3>
                                <p>In March 2022, OpenAI made available<a contenteditable="false" data-type="indexterm"
                                        data-primary="GPT-3.5 (OpenAI)" data-secondary="introduction of GPT-3.5 series"
                                        id="id339"></a> new versions of GPT-3. These new models can edit text or insert
                                    content into text. They have been trained on data through June 2021 and are
                                    described as more powerful than previous versions. At the end of November 2022,
                                    OpenAI began referring to these models as belonging to the GPT-3.5 series.</p>
                                <p>OpenAI also proposed the Codex model,<a contenteditable="false" data-type="indexterm"
                                        data-primary="GPT-3 (OpenAI)" data-secondary="Codex model" id="id340"></a><a
                                        contenteditable="false" data-type="indexterm"
                                        data-primary="Codex model (OpenAI)" id="id341"></a><a contenteditable="false"
                                        data-type="indexterm" data-primary="Codex model (OpenAI)"
                                        data-secondary="GitHub Copilot powered by" id="id342"></a><a
                                        contenteditable="false" data-type="indexterm" data-primary="GitHub"
                                        data-secondary="Copilot powered by Codex model" id="id343"></a><a
                                        contenteditable="false" data-type="indexterm"
                                        data-primary="Copilot (GitHub) powered by Codex model" id="id344"></a> a GPT-3
                                    model that is fine-tuned on billions of lines of code and that powers the <a
                                        href="https://github.com/features/copilot">GitHub Copilot</a> autocompletion
                                    programming tool to assist developers of many text editors including Visual Studio
                                    Code, JetBrains, and even Neovim. <a contenteditable="false" data-type="indexterm"
                                        data-primary="Codex model (OpenAI)" data-secondary="deprecated"
                                        id="id345"></a><a contenteditable="false" data-type="indexterm"
                                        data-primary="GPT-3 (OpenAI)" data-secondary="Codex model"
                                        data-tertiary="deprecated" id="id346"></a><a contenteditable="false"
                                        data-type="indexterm" data-primary="GitHub"
                                        data-secondary="Copilot powered by Codex model"
                                        data-tertiary="Copilot X based on GPT-4" id="id347"></a><a
                                        contenteditable="false" data-type="indexterm"
                                        data-primary="Copilot (GitHub) powered by Codex model"
                                        data-secondary="Copilot X based on GPT-4" id="id348"></a>However, the Codex
                                    model was deprecated by OpenAI in March 2023. Instead, OpenAI recommends that users
                                    switch from Codex to GPT-3.5 Turbo or GPT-4. At the same time, GitHub released
                                    Copilot X, which is based on GPT-4 and provides much more functionality than the
                                    previous version.<a contenteditable="false" data-type="indexterm"
                                        data-primary="APIs (application programming interfaces)"
                                        data-secondary="deprecation lesson" id="id349"></a></p>
                                <div data-type="warning" epub:type="warning">
                                    <h6>Warning</h6>
                                    <p>OpenAI’s deprecation of the Codex model serves as a stark reminder of the
                                        inherent risk of working with APIs: they can be subject to changes or
                                        discontinuation over time as newer, more efficient models are developed and
                                        rolled out.</p>
                                </div>
                                <p>In November 2022, OpenAI introduced<a contenteditable="false" data-type="indexterm"
                                        data-primary="ChatGPT (OpenAI)" data-secondary="history of" id="id350"></a><a
                                        contenteditable="false" data-type="indexterm"
                                        data-primary="history of GPT models" data-secondary="ChatGPT" id="id351"></a> <a
                                        href="https://chat.openai.com">ChatGPT</a> as an experimental conversational
                                    model. This model has been fine-tuned to excel at interactive dialogue, using a
                                    technique similar to that shown in <a data-type="xref"
                                        href="#fig_6_the_steps_to_obtain_the_instructed_models_from_an">Figure&nbsp;1-6</a>.
                                    ChatGPT has its roots in the GPT-3.5 series, which served as the basis for its
                                    development.<a contenteditable="false" data-type="indexterm" data-primary=""
                                        data-startref="ch01-hgp" id="id352"></a><a contenteditable="false"
                                        data-type="indexterm" data-primary="" data-startref="ch01-hgp2" id="id353"></a>
                                </p>
                                <div data-type="note" epub:type="note">
                                    <h6>Note</h6>
                                    <p>It can be argued that ChatGPT<a contenteditable="false" data-type="indexterm"
                                            data-primary="GPT-3.5 (OpenAI)" data-secondary="ChatGPT" id="id354"></a><a
                                            contenteditable="false" data-type="indexterm"
                                            data-primary="ChatGPT (OpenAI)" data-secondary="about"
                                            data-tertiary="GPT-3.5 Turbo" id="id355"></a> is an application powered by
                                        an LLM, not an actual LLM. The LLM behind ChatGPT is GPT-3.5 Turbo. However,
                                        OpenAI itself refers to ChatGPT as a model in its <a
                                            href="https://openai.com/blog/chatgpt">release note</a>. In this book, we
                                        use <em>ChatGPT</em> as a generic term for both the application and the model,
                                        unless we are manipulating code, in which case we use
                                        <code translate="no" translate="no">gpt-3.5-turbo</code>.</p>
                                </div>
                                <section data-type="sect3" data-pdf-bookmark="GPT-4">
                                    <div class="sect3" id="gpt_4_idY53dtv">
                                        <h4>GPT-4</h4>
                                        <p>In March 2023, OpenAI made GPT-4 available.<a contenteditable="false"
                                                data-type="indexterm" data-primary="history of GPT models"
                                                data-secondary="GPT-4" id="id356"></a><a contenteditable="false"
                                                data-type="indexterm" data-primary="GPT-4 (OpenAI)"
                                                data-secondary="history of" id="id357"></a> We know very little about
                                            the architecture of this new model, as OpenAI has provided little
                                            information. It is OpenAI’s most advanced system to date and should produce
                                            more secure and useful answers. The company claims that GPT-4 surpasses
                                            ChatGPT in its advanced reasoning capabilities.</p>
                                        <p>Unlike the other models in the OpenAI GPT family,<a contenteditable="false"
                                                data-type="indexterm" data-primary="prompts"
                                                data-secondary="GPT-4 multimodal capability" id="id358"></a> GPT-4 is
                                            the first multimodal model capable of receiving not only text but also
                                            images. This means that GPT-4 considers both the images and the text in the
                                            context that the model uses to generate an output sentence, which makes it
                                            possible to add an image to a prompt and ask questions about it. Note that
                                            OpenAI has not yet made this feature publicly available as of the writing of
                                            this book.</p>
                                        <p>The models have also been evaluated on various tests, and GPT-4 has
                                            outperformed ChatGPT by scoring in higher percentiles among the test takers.
                                            <a contenteditable="false" data-type="indexterm" data-primary="GPT models"
                                                data-secondary="test and exam results" id="id359"></a><a
                                                contenteditable="false" data-type="indexterm"
                                                data-primary="ChatGPT (OpenAI)" data-secondary="about"
                                                data-tertiary="test and exam results" id="id360"></a><a
                                                contenteditable="false" data-type="indexterm"
                                                data-primary="GPT-4 (OpenAI)" data-secondary="about GPT models"
                                                data-tertiary="test and exam results" id="id361"></a><a
                                                contenteditable="false" data-type="indexterm"
                                                data-primary="legal document analysis"
                                                data-secondary="GPT-4 90th percentile score on Uniform Bar Exam"
                                                id="id362"></a><a contenteditable="false" data-type="indexterm"
                                                data-primary="Uniform Bar Exam results by GPT models" id="id363"></a><a
                                                contenteditable="false" data-type="indexterm"
                                                data-primary="International Biology Olympiad results by GTP models"
                                                id="id364"></a>For example, on the <a
                                                href="https://oreil.ly/opXec">Uniform Bar Exam</a>, ChatGPT scored in
                                            the 10th percentile, while GPT-4 scored in the 90th percentile. The same
                                            goes for the <a href="https://oreil.ly/a8CP6">International Biology
                                                Olympiad</a>, in which ChatGPT scored in the 31st percentile and GPT-4
                                            in the 99th percentile. This progress is very impressive, especially
                                            considering that it was achieved in less than one year.</p>
                                        <p><a data-type="xref" href="#table-1-1">Table&nbsp;1-1</a> summarizes the
                                            evolution of the GPT models.<a contenteditable="false" data-type="indexterm"
                                                data-primary="history of GPT models" data-secondary="summary"
                                                id="id365"></a></p>
                                        <table class="striped" id="table-1-1">
                                            <caption><span class="label">Table 1-1. </span>Evolution of the GPT models
                                            </caption>
                                            <tbody>
                                                <tr>
                                                    <td>2017</td>
                                                    <td>The paper “Attention Is All You Need” by Vaswani et al. is
                                                        published.</td>
                                                </tr>
                                                <tr>
                                                    <td>2018</td>
                                                    <td>The first GPT model is introduced with 117 million parameters.
                                                    </td>
                                                </tr>
                                                <tr>
                                                    <td>2019</td>
                                                    <td>The GPT-2 model is introduced with 1.5 billion parameters.</td>
                                                </tr>
                                                <tr>
                                                    <td>2020</td>
                                                    <td>The GPT-3 model is introduced with 175 billion parameters.</td>
                                                </tr>
                                                <tr>
                                                    <td>2022</td>
                                                    <td>The GPT-3.5 (ChatGPT) model is introduced with 175 billion
                                                        parameters.</td>
                                                </tr>
                                                <tr>
                                                    <td>2023</td>
                                                    <td>The GPT-4 model is introduced, but the number of parameters is
                                                        not disclosed.</td>
                                                </tr>
                                            </tbody>
                                        </table>
                                        <div data-type="note" epub:type="note">
                                            <h6>Note</h6>
                                            <p>You may have heard the term <em>foundation model</em>. While LLMs like
                                                GPT are trained to process human language, <a contenteditable="false"
                                                    data-type="indexterm" data-primary="foundation models"
                                                    id="id366"></a><a contenteditable="false" data-type="indexterm"
                                                    data-primary="large language models (LLMs)"
                                                    data-secondary="as foundation models"
                                                    data-secondary-sortas="foundation models" id="id367"></a>a
                                                foundation model is a broader concept. These models are trained on many
                                                types of data, not just text, and they can be fine-tuned for various
                                                tasks, including but not limited to NLP. Thus, all LLMs are foundation
                                                models, but not all foundation models are LLMs.<a
                                                    contenteditable="false" data-type="indexterm" data-primary=""
                                                    data-startref="ch01-hst" id="id368"></a><a contenteditable="false"
                                                    data-type="indexterm" data-primary="" data-startref="ch01-hst2"
                                                    id="id369"></a></p>
                                        </div>
                                    </div>
                                </section>
                            </div>
                        </section>
                    </div>
                </section>
                <section data-type="sect1" data-pdf-bookmark="LLM Use Cases and Example Products">
                    <div class="sect1" id="llm_use_cases_and_example_products">
                        <h2>LLM Use Cases and Example Products</h2>
                        <p>OpenAI includes many inspiring customer stories on its website. This section explores some of
                            these applications, use cases, and product examples. We will discover how these models may
                            transform our society and open new opportunities for business and creativity. As you will
                            see, many businesses already use these new technologies, but there is room for more ideas.
                            It is now up to you.</p>
                        <section data-type="sect2" data-pdf-bookmark="Be My Eyes">
                            <div class="sect2" id="be_my_eyes">
                                <h3>Be My Eyes</h3>
                                <p>Since 2012, <a href="https://www.bemyeyes.com">Be My Eyes</a> has created
                                    technologies<a contenteditable="false" data-type="indexterm"
                                        data-primary="large language models (LLMs)"
                                        data-secondary="use cases and example products" data-tertiary="Be My Eyes"
                                        id="id370"></a><a contenteditable="false" data-type="indexterm"
                                        data-primary="Be My Eyes example product" id="id371"></a> for a community of
                                    several million people who are blind or have limited vision. For example, it has an
                                    app that connects volunteers with blind or visually impaired persons who need help
                                    with everyday tasks, such as identifying a product or navigating in an airport. With
                                    only one click in the app, the person who needs help is contacted by a volunteer
                                    who, through video and microphone sharing, can help the person.</p>
                                <p>The new multimodal capacity<a contenteditable="false" data-type="indexterm"
                                        data-primary="GPT models" data-secondary="examples and use cases"
                                        data-tertiary="Be My Eyes virtual volunteer" id="id372"></a> of GPT-4 makes it
                                    possible to process both text and images, so Be My Eyes began developing a new
                                    virtual volunteer based on GPT-4. This new virtual volunteer aims to reach the same
                                    level of assistance and understanding as a human volunteer. </p>
                                <p>“The implications for global accessibility are profound. In the not-so-distant
                                    future, the blind and low-vision community will utilize these tools not only for a
                                    host of visual interpretation needs but also to have a greater degree of
                                    independence in their lives,” says Michael Buckley, CEO of Be My Eyes.</p>
                                <p>At the time of this writing, the virtual volunteer is still in the beta version. To
                                    gain access to it, you must register to be put on a waiting list in the app, but
                                    initial feedback from beta testers is very positive.</p>
                            </div>
                        </section>
                        <section data-type="sect2" data-pdf-bookmark="Morgan Stanley">
                            <div class="sect2" id="morgan_stanley">
                                <h3>Morgan Stanley</h3>
                                <p><a href="https://www.morganstanley.com">Morgan Stanley</a> is a multinational<a
                                        contenteditable="false" data-type="indexterm"
                                        data-primary="Morgan Stanley GPT use case" id="id373"></a><a
                                        contenteditable="false" data-type="indexterm"
                                        data-primary="large language models (LLMs)"
                                        data-secondary="use cases and example products" data-tertiary="Morgan Stanley"
                                        id="id374"></a><a contenteditable="false" data-type="indexterm"
                                        data-primary="GPT models" data-secondary="examples and use cases"
                                        data-tertiary="Morgan Stanley intellectual capital" id="id375"></a> investment
                                    bank and financial services company in the United States. As a leader in wealth
                                    management, Morgan Stanley has a content library of hundreds of thousands of pages
                                    of knowledge and insight covering investment strategies, market research and
                                    commentary, and analyst opinions. This vast amount of information is spread across
                                    multiple internal sites and is mostly in PDF format. This means consultants must
                                    search a large number of documents to find answers to their questions. As you can
                                    imagine, this search can be long and tedious.</p>
                                <p>The company evaluated how it could leverage its intellectual capital with GPT’s
                                    integrated research capabilities. The resulting internally developed model will
                                    power a chatbot that performs a comprehensive search of wealth management content
                                    and efficiently unlocks Morgan Stanley’s accumulated knowledge. In this way, GPT-4
                                    has provided a way to analyze all this information in a format that is much easier
                                    to use. </p>
                            </div>
                        </section>
                        <section data-type="sect2" data-pdf-bookmark="Khan Academy">
                            <div class="sect2" id="khan_academy">
                                <h3>Khan Academy</h3>
                                <p><a href="https://www.khanacademy.org">Khan Academy</a> is a US-based nonprofit<a
                                        contenteditable="false" data-type="indexterm"
                                        data-primary="Khanmigo (Khan Academy) GPT use case" id="id376"></a><a
                                        contenteditable="false" data-type="indexterm"
                                        data-primary="large language models (LLMs)"
                                        data-secondary="use cases and example products" data-tertiary="Khan Academy"
                                        id="id377"></a><a contenteditable="false" data-type="indexterm"
                                        data-primary="Khan, Sal" id="id378"></a><a contenteditable="false"
                                        data-type="indexterm" data-primary="GPT models"
                                        data-secondary="examples and use cases" data-tertiary="Khanmigo by Khan Academy"
                                        id="id379"></a> educational organization founded in 2008 by Sal Khan. Its
                                    mission is to create a set of free online tools to help educate students worldwide.
                                    The organization offers thousands of math, science, and social studies lessons for
                                    students of all ages. In addition, the organization produces short lessons through
                                    videos and blogs, and recently it began offering Khanmigo. </p>
                                <p>Khanmigo is a new AI assistant powered by GPT-4. Khanmigo can do a lot of things for
                                    students, such as guiding and encouraging them, asking questions, and preparing them
                                    for tests. Khanmigo is designed to be a friendly chatbot that helps students with
                                    their classwork. It does not give students answers directly, but instead guides them
                                    in the learning process. Khanmigo can also support teachers by helping them make
                                    lesson plans, complete administrative tasks, and create lesson books, among other
                                    things. </p>
                                <p class="pagebreak-before less_space">“We think GPT-4 is opening up new frontiers in
                                    education. A lot of people have dreamed about this kind of technology for a long
                                    time. It’s transformative, and we plan to proceed responsibly with testing to
                                    explore if it can be used effectively for learning and teaching,” <a
                                        contenteditable="false" data-type="indexterm" data-primary="DiCerbo, Kristen"
                                        id="id380"></a>says Kristen DiCerbo, chief learning officer at Khan Academy.</p>
                                <p>At the time of this writing, access to Khanmigo’s pilot program is limited to
                                    selected people. To participate in the program, you must be placed on a <a
                                        href="https://oreil.ly/oP6KN">waiting list</a>. </p>
                            </div>
                        </section>
                        <section data-type="sect2" data-pdf-bookmark="Duolingo">
                            <div class="sect2" id="duolingo_ideEzo9b">
                                <h3>Duolingo</h3>
                                <p><a href="https://www.duolingo.com">Duolingo</a> is a US-based educational<a
                                        contenteditable="false" data-type="indexterm"
                                        data-primary="Duolingo GPT use case" id="id381"></a><a contenteditable="false"
                                        data-type="indexterm" data-primary="large language models (LLMs)"
                                        data-secondary="use cases and example products" data-tertiary="Duolingo"
                                        id="id382"></a><a contenteditable="false" data-type="indexterm"
                                        data-primary="GPT models" data-secondary="examples and use cases"
                                        data-tertiary="Duolingo" id="id383"></a> technology company, founded in 2011,
                                    that produces applications used by millions of people who want to learn a second
                                    language. Duolingo users need to understand the rules of grammar to learn the basics
                                    of a language. They need to have conversations, ideally with a native speaker, to
                                    understand those grammar rules and master the language. This is not possible for
                                    everyone.</p>
                                <p>Duolingo has added two new features to the product using OpenAI’s GPT-4: Role Play
                                    and Explain My Answer. These features are available in a new subscription level
                                    called Duolingo Max. With these features, Duolingo has bridged the gap between
                                    theoretical knowledge and the practical application of language. Thanks to LLMs,
                                    Duolingo allows learners to immerse themselves in real-world scenarios. </p>
                                <p>The Role Play feature simulates conversations with native speakers, allowing users to
                                    practice their language skills in a variety of settings. The Explain My Answer
                                    feature provides personalized feedback on grammar errors, facilitating a deeper
                                    understanding of the structure of the language. </p>
                                <p>“We wanted AI-powered features that were deeply integrated into the app and leveraged
                                    the gamified aspect of Duolingo that our learners love,” <a contenteditable="false"
                                        data-type="indexterm" data-primary="Bodge, Edwin" id="id384"></a>says Edwin
                                    Bodge, principal product manager at Duolingo.</p>
                                <p>The integration of GPT-4 into Duolingo Max not only enhances the overall learning
                                    experience but also paves the way for more effective language acquisition,
                                    especially for those without access to native speakers or immersive environments.
                                    This innovative approach should transform the way learners master a second language
                                    and contribute to better long-term learning outcomes.</p>
                            </div>
                        </section>
                        <section data-type="sect2" data-pdf-bookmark="Yabble">
                            <div class="sect2" id="yabble_idY4IWeg">
                                <h3>Yabble</h3>
                                <p><a href="https://www.yabble.com">Yabble</a> is a market research company<a
                                        contenteditable="false" data-type="indexterm" data-primary="Yabble GPT use case"
                                        id="id385"></a><a contenteditable="false" data-type="indexterm"
                                        data-primary="large language models (LLMs)"
                                        data-secondary="use cases and example products" data-tertiary="Yabble"
                                        id="id386"></a><a contenteditable="false" data-type="indexterm"
                                        data-primary="GPT models" data-secondary="examples and use cases"
                                        data-tertiary="Yabble" id="id387"></a> that uses AI to analyze consumer data in
                                    order to deliver actionable insights to businesses. Its platform transforms raw,
                                    unstructured data into visualizations, enabling businesses to make informed
                                    decisions based on customer needs.</p>
                                <p class="pagebreak-before less_space">The integration of advanced AI technologies such
                                    as GPT into Yabble’s platform has enhanced its consumer data processing
                                    capabilities. This enhancement allows for a more effective understanding of complex
                                    questions and answers, enabling businesses to gain deeper insights based on the
                                    data. As a result, organizations can make more informed decisions by identifying key
                                    areas for improvement based on customer feedback.</p>
                                <p>“We knew that if we wanted to expand our existing offers, we needed artificial
                                    intelligence to do a lot of the heavy lifting so that we could spend our time and
                                    creative energy elsewhere. OpenAI fit the bill perfectly,” <a
                                        contenteditable="false" data-type="indexterm" data-primary="Roe, Ben"
                                        id="id388"></a>says Ben Roe, Head of Product at Yabble.</p>
                            </div>
                        </section>
                        <section data-type="sect2" data-pdf-bookmark="Waymark ">
                            <div class="sect2" id="waymark_idGWFoS5">
                                <h3>Waymark </h3>
                                <p><a href="https://waymark.com">Waymark</a> provides a platform for<a
                                        contenteditable="false" data-type="indexterm"
                                        data-primary="Waymark GPT use case" id="id389"></a><a contenteditable="false"
                                        data-type="indexterm" data-primary="large language models (LLMs)"
                                        data-secondary="use cases and example products" data-tertiary="Waymark"
                                        id="id390"></a><a contenteditable="false" data-type="indexterm"
                                        data-primary="GPT models" data-secondary="examples and use cases"
                                        data-tertiary="Waymark" id="id391"></a> creating video ads. This platform uses
                                    AI to help businesses easily create high-quality videos without the need for
                                    technical skills or expensive equipment.</p>
                                <p>Waymark has integrated GPT into its platform, which has significantly improved the
                                    scripting process for platform users. This GPT-powered enhancement allows the
                                    platform to generate custom scripts for businesses in seconds. This allows users to
                                    focus more on their primary goals, as they spend less time editing scripts and more
                                    time creating video ads. The integration of GPT into Waymark’s platform therefore
                                    provides a more efficient and personalized video creation experience.</p>
                                <p>“I’ve tried every AI-powered product available over the last five years but found
                                    nothing that could effectively summarize a business’s online footprint, let alone
                                    write effective marketing copy, until GPT-3,” <a contenteditable="false"
                                        data-type="indexterm" data-primary="Labenz, Nathan" id="id392"></a>says Waymark
                                    founder, Nathan Labenz.</p>
                            </div>
                        </section>
                        <section data-type="sect2" data-pdf-bookmark="Inworld AI">
                            <div class="sect2" id="inworld_ai">
                                <h3>Inworld AI</h3>
                                <p><a href="https://www.inworld.ai">Inworld AI</a> provides a developer platform<a
                                        contenteditable="false" data-type="indexterm"
                                        data-primary="Inworld AI as GPT use case" id="id393"></a><a
                                        contenteditable="false" data-type="indexterm"
                                        data-primary="large language models (LLMs)"
                                        data-secondary="use cases and example products" data-tertiary="Inworld AI"
                                        id="id394"></a><a contenteditable="false" data-type="indexterm"
                                        data-primary="GPT models" data-secondary="examples and use cases"
                                        data-tertiary="Inworld AI" id="id395"></a> for creating AI characters with
                                    distinct personalities, multimodal expression, and contextual <span
                                        class="keep-together">awareness.</span> </p>
                                <p>One of the main use cases of the Inworld AI platform is video games. The integration
                                    of GPT as the basis for the character engine of Inworld AI enables efficient and
                                    rapid video game character development. By combining GPT with other ML models, the
                                    platform can generate unique personalities, emotions, memory, and behaviors for AI
                                    characters. This process allows game developers to focus on storytelling and other
                                    topics without having to invest significant time in creating language models from
                                    scratch. </p>
                                <p>“With GPT-3, we had more time and creative energy to invest in our proprietary
                                    technology that powers the next generation of non-player characters (NPCs),” <a
                                        contenteditable="false" data-type="indexterm" data-primary="Gibbs, Kylan"
                                        id="id396"></a>says Kylan Gibbs, chief product officer and cofounder of Inworld.
                                </p>
                            </div>
                        </section>
                    </div>
                </section>
                <section data-type="sect1"
                    data-pdf-bookmark="Beware of AI Hallucinations: Limitations and Considerations">
                    <div class="sect1" id="beware_of_ai_hallucinations_limitations_and_consi">
                        <h2>Beware of AI Hallucinations: Limitations and Considerations</h2>
                        <p>As you have seen, an LLM generates<a contenteditable="false" data-type="indexterm"
                                data-primary="large language models (LLMs)" data-secondary="AI hallucinations"
                                id="ch01-halu"></a><a contenteditable="false" data-type="indexterm"
                                data-primary="AI (artificial intelligence)" data-secondary="hallucinations"
                                id="ch01-halu2"></a><a contenteditable="false" data-type="indexterm"
                                data-primary="hallucinations in AI responses" id="ch01-halu3"></a><a
                                contenteditable="false" data-type="indexterm" data-primary="answering questions"
                                data-secondary="AI hallucinations" id="ch01-halu4"></a><a contenteditable="false"
                                data-type="indexterm" data-primary="mistakes in AI responses" id="ch01-halu5"></a><a
                                contenteditable="false" data-type="indexterm" data-primary="question answering"
                                data-secondary="AI hallucinations" id="ch01-halu6"></a> an answer by predicting the next
                            words (or tokens) one by one based on a given input prompt. In most situations, the model’s
                            output is relevant and entirely usable for your task, but it is essential to be careful when
                            you are using language models in your applications because they can give incoherent answers.
                            These answers are often referred to as <em>hallucinations</em>. AI hallucinations occur when
                            AI gives you a confident response that is false or that refers to imaginary facts. This can
                            be dangerous for users who rely on GPT. You need to double-check and critically examine the
                            model’s response.</p>
                        <p>Consider the following example. <a contenteditable="false" data-type="indexterm"
                                data-primary="calculator not installed by default" id="id397"></a><a
                                contenteditable="false" data-type="indexterm" data-primary="plug-ins of GPT-4"
                                data-secondary="calculator not installed by default" id="id398"></a><a
                                contenteditable="false" data-type="indexterm" data-primary="math calculations"
                                data-secondary="AI hallucinations" id="id399"></a><a contenteditable="false"
                                data-type="indexterm" data-primary="calculations" data-secondary="AI hallucinations"
                                id="id400"></a>We start by asking the model to do a simple calculation: 2 + 2. As
                            expected, it answers 4. So it is correct. Excellent! We then ask it to do a more complex
                            calculation: 3,695 × 123,548. Although the correct answer is 456,509,860, the model gives
                            with great confidence a wrong answer, as you can see in <a data-type="xref"
                                href="#fig_7_chatgpt_hallucinating_bad_math_chatgpt_april_22">Figure&nbsp;1-7</a>. And
                            when we ask it to check and recalculate, it still gives a wrong answer.</p>
                        <figure>
                            <div id="fig_7_chatgpt_hallucinating_bad_math_chatgpt_april_22" class="figure">
                                <img src="/api/v2/epubs/urn:orm:book:9781098152475/files/assets/dagc_0107.png" alt=""
                                    width="600" height="423">
                                <h6><span class="label">Figure 1-7. </span>ChatGPT hallucinating bad math (ChatGPT,
                                    April 22, 2023)</h6>
                            </div>
                        </figure>
                        <p class="pagebreak-before less_space">Although, as we will see, you can add new features to GPT
                            using a plug-in system, GPT does not include a calculator by default. To answer our question
                            of what is 2 + 2, GPT generates each token one at a time. It answers correctly because it
                            probably has often seen “2 + 2 equals 4” in the texts used for its training. It doesn’t
                            really do the calculation—it is just text completion. </p>
                        <div data-type="warning" epub:type="warning">
                            <h6>Warning</h6>
                            <p>It is unlikely that GPT has seen the numbers we chose for the multiplication problem,
                                3,695 × 123,548, many times in its training. This is why it makes a mistake. And as you
                                can see, even when it makes a mistake, it can be reasonably sure about its wrong output.
                                Be careful, mainly if you use the model in one of your applications. If GPT makes
                                mistakes, your application may get inconsistent results.</p>
                        </div>
                        <p>Notice that ChatGPT’s result is <em>close</em> to the correct answer and not completely
                            random. It is an interesting side effect of its algorithm: even though it has no
                            mathematical capabilities, it can give a close estimation with a language approach only.
                        </p>
                        <div data-type="note" epub:type="note">
                            <h6>Note</h6>
                            <p>OpenAI introduced the ability to use plug-ins with GPT-4.<a contenteditable="false"
                                    data-type="indexterm" data-primary="GPT-4 (OpenAI)" data-secondary="plug-ins added"
                                    id="id401"></a><a contenteditable="false" data-type="indexterm"
                                    data-primary="plug-ins of GPT-4" data-secondary="GPT-4 capability" id="id402"></a><a
                                    contenteditable="false" data-type="indexterm" data-primary="GPT models"
                                    data-secondary="plug-ins of GPT-4" id="id403"></a> As we will see in <a
                                    data-type="xref"
                                    href="ch05.html#advancing_llm_capabilities_with_the_langchain_fram">Chapter&nbsp;5</a>,
                                these tools allow you to add additional functionality to the LLM. <a
                                    contenteditable="false" data-type="indexterm"
                                    data-primary="calculator not installed by default"
                                    data-secondary="calculator plug-in available" id="id404"></a><a
                                    contenteditable="false" data-type="indexterm" data-primary="plug-ins of GPT-4"
                                    data-secondary="calculator not installed by default"
                                    data-tertiary="calculator plug-in available" id="id405"></a>One tool is a calculator
                                that helps GPT correctly answer these types of questions.</p>
                        </div>
                        <p>In the preceding example, ChatGPT made a mistake. <a contenteditable="false"
                                data-type="indexterm" data-primary="deceit by LLMs" id="id406"></a>But in some cases, it
                            can even be deliberately deceitful, such as shown in <a data-type="xref"
                                href="#fig_8_asking_chatgpt_to_count_zebras_on_a_wikipedia_pict">Figure&nbsp;1-8</a>.
                        </p>
                        <figure>
                            <div id="fig_8_asking_chatgpt_to_count_zebras_on_a_wikipedia_pict" class="figure">
                                <img src="/api/v2/epubs/urn:orm:book:9781098152475/files/assets/dagc_0108.png" alt=""
                                    width="600" height="211">
                                <h6><span class="label">Figure 1-8. </span>Asking ChatGPT to count zebras on a Wikipedia
                                    picture (ChatGPT, April 5, 2023)</h6>
                            </div>
                        </figure>
                        <p>ChatGPT begins by claiming that it cannot access the internet. However, if we insist,
                            something interesting happens (see <a data-type="xref"
                                href="#fig_9_chatgpt_claiming_it_accessed_the_wikipedia_link">Figure&nbsp;1-9</a>).</p>
                        <figure>
                            <div id="fig_9_chatgpt_claiming_it_accessed_the_wikipedia_link" class="figure">
                                <img src="/api/v2/epubs/urn:orm:book:9781098152475/files/assets/dagc_0109.png" alt=""
                                    width="600" height="181">
                                <h6><span class="label">Figure 1-9. </span>ChatGPT claiming it accessed the Wikipedia
                                    link</h6>
                            </div>
                        </figure>
                        <p>ChatGPT now implies that it <em>did</em> access the link. However, this is definitely not
                            possible at the moment. ChatGPT is blatantly leading the user to think that it has
                            capabilities it doesn’t have. By the way, as <a data-type="xref"
                                href="#fig_10_the_zebras_chatgpt_didn_t_really_count">Figure&nbsp;1-10</a> shows, there
                            are more than three zebras in the image.</p>
                        <figure>
                            <div id="fig_10_the_zebras_chatgpt_didn_t_really_count" class="figure">
                                <img src="/api/v2/epubs/urn:orm:book:9781098152475/files/assets/dagc_0110.png" alt=""
                                    width="500" height="750">
                                <h6><span class="label">Figure 1-10. </span>The zebras ChatGPT didn’t really count</h6>
                            </div>
                        </figure>
                        <div data-type="warning" epub:type="warning">
                            <h6>Warning</h6>
                            <p>ChatGPT and other GPT-4 models are, by design, not reliable: they can make mistakes, give
                                false information, or even mislead the user.</p>
                        </div>
                        <p>In summary, we highly recommend using pure GPT-based solutions for creative applications, not
                            question answering where the truth matters—such as for medical tools. For such use cases, as
                            you will see, plug-ins are probably an ideal solution.<a contenteditable="false"
                                data-type="indexterm" data-primary="" data-startref="ch01-halu" id="id407"></a><a
                                contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch01-halu2"
                                id="id408"></a><a contenteditable="false" data-type="indexterm" data-primary=""
                                data-startref="ch01-halu3" id="id409"></a><a contenteditable="false"
                                data-type="indexterm" data-primary="" data-startref="ch01-halu4" id="id410"></a><a
                                contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch01-halu5"
                                id="id411"></a><a contenteditable="false" data-type="indexterm" data-primary=""
                                data-startref="ch01-halu6" id="id412"></a></p>
                    </div>
                </section>
                <section data-type="sect1" data-pdf-bookmark="Optimizing GPT Models with Plug-ins and Fine-Tuning">
                    <div class="sect1" id="optimizing_gpt_models_with_plug_ins_and_fine_tunin">
                        <h2>Optimizing GPT Models with Plug-ins and Fine-Tuning</h2>
                        <p>In addition to its simple completion feature, more advanced techniques can be used to further
                            exploit the capabilities of the language models provided by OpenAI. This book looks at two
                            of these methods:</p>
                        <ul>
                            <li>
                                <p>Plug-ins</p>
                            </li>
                            <li>
                                <p>Fine-tuning</p>
                            </li>
                        </ul>
                        <p>GPT has some limitations, for example, with calculations. As you’ve seen, GPT can correctly
                            answer simple math problems like 2 + 2 but may struggle with more complex calculations like
                            3,695 × 123,548. Moreover, it does not have direct access to the internet, which means that
                            GPT models lack access to new information and are limited to the data they were trained on.
                            For GPT-4, the last knowledge update occurred in September 2021. <a contenteditable="false"
                                data-type="indexterm" data-primary="plug-ins of GPT-4" data-secondary="about"
                                id="id413"></a><a contenteditable="false" data-type="indexterm"
                                data-primary="APIs (application programming interfaces)" data-secondary="plug-ins"
                                data-tertiary="API interactions enabled by" id="id414"></a>The plug-in service provided
                            by OpenAI allows the model to be connected to applications that may be developed by third
                            parties. These plug-ins enable the models to interact with developer-defined APIs, and this
                            process can potentially greatly enhance the capabilities of the GPT models, as they
                            <em>can</em> access the outside world through a wide range of actions. </p>
                        <p>For developers, plug-ins potentially open up many new opportunities. Consider that in the
                            future, each company may want to have its own plug-in for LLMs. There could be collections
                            of plug-ins similar to what we find today in smartphone app stores. The number of
                            applications that could be added via plug-ins could be enormous. </p>
                        <p>On its website, OpenAI says that plug-ins can allow ChatGPT to do things such as the
                            following:</p>
                        <ul>
                            <li>
                                <p>Retrieve real-time information, such as sports scores, stock prices, the latest news,
                                    and so forth</p>
                            </li>
                            <li>
                                <p>Retrieve knowledge-based information, such as company docs, personal notes, and more
                                </p>
                            </li>
                            <li>
                                <p>Perform actions on behalf of the user, such as booking a flight, ordering food, and
                                    so on</p>
                            </li>
                            <li>
                                <p>Execute accurate math calculations</p>
                            </li>
                        </ul>
                        <p>These are just a few examples of use cases; it is up to you to find new ones.</p>
                        <p>This book also examines<a contenteditable="false" data-type="indexterm"
                                data-primary="fine-tuning a model" data-secondary="about" id="id415"></a> fine-tuning
                            techniques. As you will see, fine-tuning can improve the accuracy of an existing model for a
                            specific task. The fine-tuning process involves retraining an existing GPT model on a
                            particular set of new data. This new model is designed for a specific task, and this
                            additional training process allows the model to adjust its internal parameters to learn the
                            nuances of this given task. The resulting fine-tuned model should perform better on the task
                            for which it has been fine-tuned. For example, a model fine-tuned on financial textual data
                            should be able to better answer queries in that domain and generate more relevant content.
                        </p>
                    </div>
                </section>
                <section data-type="sect1" data-pdf-bookmark="Summary">
                    <div class="sect1" id="summary_idl4L7kc">
                        <h2>Summary</h2>
                        <p>LLMs have come a long way, starting with simple n-gram models and moving to RNNs, LSTMs, and
                            advanced transformer-based architectures. LLMs are computer programs that can process and
                            generate human-like language, with ML techniques to analyze vast amounts of text data. By
                            using self-attention and cross-attention mechanisms, transformers have greatly enhanced
                            language understanding. </p>
                        <p>This book explores how to use GPT-4 and ChatGPT, as they offer advanced capabilities for
                            understanding and generating context. Building applications with them goes beyond the scope
                            of traditional BERT or LSTM models to provide human-like interactions.</p>
                        <p>Since early 2023, ChatGPT and GPT-4 have demonstrated remarkable capabilities in NLP. As a
                            result, they have contributed to the rapid advancement of AI-enabled applications in various
                            industries. Different use cases already exist, ranging from applications such as Be My Eyes
                            to platforms such as Waymark, which are testaments to the potential of these models to
                            revolutionize how we interact with technology. </p>
                        <p>It is important to keep in mind the potential risks of using these LLMs. As a developer of
                            applications that will use the OpenAI API, you should be sure that users know the risk of
                            errors and can verify the AI-generated information. </p>
                        <p>The next chapter will give you the tools and information to use the OpenAI models available
                            as a service and help you be part of this incredible transformation we are living today.</p>
                    </div>
                </section>
            </div>
        </section>
    </div>
</div>

</html>