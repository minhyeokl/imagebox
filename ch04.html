<html>

<head>
    <link href="book.css" rel="stylesheet" />
</head>
<div id="bookContainer">
    <div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 4. Advanced GPT-4 and ChatGPT Techniques"><div class="chapter" id="advanced_gpt_4_and_chatgpt_techniques">
        <h1><span class="label">Chapter 4. </span>Advanced GPT-4 and ChatGPT Techniques</h1>
        <p>Now that you are familiar with the basics of LLMs and the OpenAI API, it’s time to take your skills to the next level. This chapter covers powerful strategies that will enable you to harness the true potential of ChatGPT and GPT-4. From prompt engineering, zero-shot learning, and few-shot learning to fine-tuning models for specific tasks, this chapter will give you all the knowledge you need to create any application you can imagine.</p>
        <section data-type="sect1" data-pdf-bookmark="Prompt Engineering "><div class="sect1" id="prompt_engineering">
          <h2>Prompt Engineering </h2>
          <p>Before we dive into prompt engineering,<a contenteditable="false" data-type="indexterm" data-primary="chat_completion()" id="id871"></a><a contenteditable="false" data-type="indexterm" data-primary="prompt engineering" data-secondary="chat_completion() defined" id="id872"></a><a contenteditable="false" data-type="indexterm" data-primary="ChatCompletion" data-secondary="chat_completion() calling" id="id873"></a><a contenteditable="false" data-type="indexterm" data-primary="openai.ChatCompletion.create()" data-secondary="chat_completion() calling" id="id874"></a> let’s briefly review the chat model’s <span class="keep-together"><code translate="no">completion</code></span> function, as this section will use it extensively. To make the code more compact, we define the function as follows:</p>
        
            <pre translate="no" data-type="programlisting" data-code-language="python"><code translate="no" class="k">def</code> <code translate="no" class="nf">chat_completion</code><code translate="no" class="p">(</code><code translate="no" class="n">prompt</code><code translate="no" class="p">,</code> <code translate="no" class="n">model</code><code translate="no" class="o">=</code><code translate="no" class="s2">"gpt-4"</code><code translate="no" class="p">,</code> <code translate="no" class="n">temperature</code><code translate="no" class="o">=</code><code translate="no" class="mi">0</code><code translate="no" class="p">):</code>
      <code translate="no" class="n">res</code> <code translate="no" class="o">=</code> <code translate="no" class="n">openai</code><code translate="no" class="o">.</code><code translate="no" class="n">ChatCompletion</code><code translate="no" class="o">.</code><code translate="no" class="n">create</code><code translate="no" class="p">(</code>
          <code translate="no" class="n">model</code><code translate="no" class="o">=</code><code translate="no" class="n">model</code><code translate="no" class="p">,</code>
          <code translate="no" class="n">messages</code><code translate="no" class="o">=</code><code translate="no" class="p">[{</code><code translate="no" class="s2">"role"</code><code translate="no" class="p">:</code> <code translate="no" class="s2">"user"</code><code translate="no" class="p">,</code> <code translate="no" class="s2">"content"</code><code translate="no" class="p">:</code> <code translate="no" class="n">prompt</code><code translate="no" class="p">}],</code>
          <code translate="no" class="n">temperature</code><code translate="no" class="o">=</code><code translate="no" class="n">temperature</code><code translate="no" class="p">,</code>
      <code translate="no" class="p">)</code>
      <code translate="no" class="nb">print</code><code translate="no" class="p">(</code><code translate="no" class="n">res</code><code translate="no" class="p">[</code><code translate="no" class="s2">"choices"</code><code translate="no" class="p">][</code><code translate="no" class="mi">0</code><code translate="no" class="p">][</code><code translate="no" class="s2">"message"</code><code translate="no" class="p">][</code><code translate="no" class="s2">"content"</code><code translate="no" class="p">])</code></pre>
          
          <p>This function receives a prompt and displays the completion result in the terminal. The model and the temperature are two optional features set by default, respectively, to GPT-4 and 0.</p>
          <p>To demonstrate prompt engineering,<a contenteditable="false" data-type="indexterm" data-primary="prompt engineering" data-secondary="about" id="id875"></a> we will return to the example text “As Descartes said, I think therefore”. If this input is passed to GPT-4, it is natural for the model to complete the sentence by iteratively adding the most likely tokens: </p>
          
            <pre translate="no" data-type="programlisting" data-code-language="python"><code translate="no" class="n">chat_completion</code><code translate="no" class="p">(</code><code translate="no" class="s2">"As Descartes said, I think therefore"</code><code translate="no" class="p">)</code></pre>
         
          <p>As a result, we get the following output message:</p>
          
            <pre translate="no" data-type="programlisting">I am. This famous philosophical statement, also known as "Cogito, ergo sum," 
  emphasizes the existence of the self through the act of thinking or doubting.
  Descartes used this statement as a foundational principle in his philosophy,
  arguing that one's own existence is the most certain and indubitable fact
  that can be known.</pre>
          
          <p><em>Prompt engineering</em> is an emergent discipline focused on developing best practices for building optimal inputs for LLMs in order to produce desirable outputs as programmatically as possible. As an AI engineer, you must know how to interact with AI to obtain exploitable results for your apps, how to ask the right questions, and how to write quality prompts; all topics we will cover in this section. </p>
          <p>It should be noted that prompt engineering<a contenteditable="false" data-type="indexterm" data-primary="prompt engineering" data-secondary="costs related to" id="id876"></a><a contenteditable="false" data-type="indexterm" data-primary="pricing OpenAI models" data-secondary="tokens used" data-tertiary="prompt engineering" id="id877"></a><a contenteditable="false" data-type="indexterm" data-primary="max_tokens input parameter for chat completion" data-secondary="managing for cost control" id="id878"></a><a contenteditable="false" data-type="indexterm" data-primary="max_tokens input parameter for text completion" data-secondary="managing for cost control" id="id879"></a><a contenteditable="false" data-type="indexterm" data-primary="tokenization in GPT models" data-secondary="tokens" data-tertiary="max_tokens input parameter" id="id880"></a><a contenteditable="false" data-type="indexterm" data-primary="pricing OpenAI models" data-secondary="tokens used" data-tertiary="max_tokens input parameter" id="id881"></a><a contenteditable="false" data-type="indexterm" data-primary="parameters" data-secondary="max_tokens input parameter" data-tertiary="managing for cost control" id="id882"></a> can affect the cost of using the OpenAI API. The amount of money you will pay to use the API is proportional to the number of tokens you send to and receive from OpenAI. As mentioned in <a data-type="xref" href="ch02.html#a_deep_dive_into_the_gpt_4_and_chatgpt_apis">Chapter&nbsp;2</a>, use of the <code translate="no">max_token</code> parameter is highly recommended to avoid unpleasant surprises on your bills.</p>
          <p>Also note that you should consider<a contenteditable="false" data-type="indexterm" data-primary="parameters" data-secondary="prompt engineering and" id="id883"></a><a contenteditable="false" data-type="indexterm" data-primary="openai method parameters and prompt engineering" id="id884"></a><a contenteditable="false" data-type="indexterm" data-primary="prompts" data-secondary="designing effective prompts" data-seealso="prompt engineering" id="ch04-des"></a> the different parameters you can use in the <code translate="no">openai</code> methods, as you can get significantly different results with the same prompt if you use parameters like <code translate="no">temperature</code>, <code translate="no">top_p</code>, and <code translate="no">max_token</code>. </p>
          <section data-type="sect2" data-pdf-bookmark="Designing Effective Prompts"><div class="sect2" id="designing_effective_prompts">
            <h3>Designing Effective Prompts</h3>
            <p>A lot of tasks can be performed via prompts.<a contenteditable="false" data-type="indexterm" data-primary="prompt engineering" data-secondary="designing effective prompts" data-tertiary="about" id="id885"></a> They include summarization, text classification, sentiment analysis, and question answering. In all these tasks, it is common to define three elements in the prompt: a role, a context, and a task, as depicted in <a data-type="xref" href="#fig_1_an_effective_prompt">Figure&nbsp;4-1</a>. </p>
            <figure><div id="fig_1_an_effective_prompt" class="figure">
              <img src="/api/v2/epubs/urn:orm:book:9781098152475/files/assets/dagc_0401.png" alt="" width="600" height="175">
              <h6><span class="label">Figure 4-1. </span>An effective prompt</h6>
            </div></figure>
            <p class="pagebreak-before less_space">All three elements are not always necessary, and their order can be changed, but if your prompt is well constructed and the elements are well defined, you should get good results. Note that even when these three elements are used, for complex tasks you may need to use more advanced techniques, such as zero-shot learning, few-shot learning, and fine-tuning. These advanced techniques will be discussed later in this chapter.</p>
            <section data-type="sect3" data-pdf-bookmark="The context"><div class="sect3" id="the_context">
              <h4><a id="_6nr68xxj69o8"></a>The context</h4>
              <p>The first essential element in a prompt<a contenteditable="false" data-type="indexterm" data-primary="prompt engineering" data-secondary="designing effective prompts" data-tertiary="context" id="id886"></a><a contenteditable="false" data-type="indexterm" data-primary="context in prompt design" id="id887"></a> is the context. When you write an input text for an LLM, you must detail the context as much as possible. To illustrate this, say you want to use GPT-4 to create an application that suggests the main course for lunch. In the following, we will compare the results we obtain with two different contexts. The first context will have little detail, and the second will have more detail. </p>
              <p>With a short input message such as:</p>
            
                <pre translate="no" data-type="programlisting" data-code-language="python"><code translate="no" class="n">chat_completion</code><code translate="no" class="p">(</code><code translate="no" class="s2">"Give me a suggestion for the main course for today's lunch."</code><code translate="no" class="p">)</code></pre>
              
              <p>we get the following output message:</p>
              
                <pre translate="no" data-type="programlisting">Grilled chicken with roasted vegetables and quinoa.</pre>
           
              <p>Now, with more details about the context in the input message:</p>
              
                <pre translate="no" data-type="programlisting" data-code-language="python"><code translate="no" class="n">prompt</code> <code translate="no" class="o">=</code> <code translate="no" class="s2">"""</code>
  <code translate="no" class="s2">Context: I do 2 hours of sport a day. I am vegetarian, and I don't like green </code>
  <code translate="no" class="s2">vegetables. I am conscientious about eating healthily. </code>
  <code translate="no" class="s2">Task: Give me a suggestion for a main course for today's lunch."""</code>
  <code translate="no" class="n">chat_completion</code><code translate="no" class="p">(</code><code translate="no" class="n">prompt</code><code translate="no" class="p">)</code></pre>
             
              <p>we get the following output message:</p>
              
                <pre translate="no" data-type="programlisting">For today's lunch, you can try a Chickpea and Sweet Potato Curry served with
  Quinoa. This dish is packed with protein, fiber, and essential nutrients,
  while also being vegetarian and not relying on green vegetables. The curry
  can be made with chickpeas, sweet potatoes, tomatoes, onions, garlic, and a
  blend of spices like turmeric, cumin, and coriander. Cook the quinoa
  separately and serve it alongside the curry for a delicious and healthy meal.</pre>
              
              <p>In the second example, the sentence is more detailed because the model has more context: it suggests a healthy vegetarian dish that is full of protein. </p>
              <p>The context guides GPT’s “thinking” to add value to its response. Building a prompt with a good context is an iterative process, often requiring trial-and-error steps. Identifying the essential points the context must contain is sometimes tricky. <a contenteditable="false" data-type="indexterm" data-primary="prompt engineering" data-secondary="designing effective prompts" data-tertiary="context questions for GPT to ask" id="id888"></a><a contenteditable="false" data-type="indexterm" data-primary="context in prompt design" data-secondary="questions for GPT to ask" id="id889"></a>To help determine possible improvements, you can ask GPT-4 for suggestions. In the following example, we change the prompt: instead of asking the model to answer our question, we instruct it to ask questions about the context in an effort to achieve a better completion.</p>
              <p>With the following input message:</p>
              
                <pre translate="no" data-type="programlisting" data-code-language="python"><code translate="no" class="n">prompt</code> <code translate="no" class="o">=</code> <code translate="no" class="s2">"""</code>
  <code translate="no" class="s2">Context: I do 2 hours of sport a day. I am vegetarian and I don't like green </code>
  <code translate="no" class="s2">vegetables. I am very careful to eat healthily.</code>
  <code translate="no" class="s2">Task: Give me a suggestion for a main course for today's lunch?</code>
  <code translate="no" class="s2">Do not perform the requested task! Instead, can you ask me questions about the </code>
  <code translate="no" class="s2">context so that when I answer, you can perform the requested task more</code>
  <code translate="no" class="s2">efficiently?</code>
  <code translate="no" class="s2">"""</code>
  <code translate="no" class="n">chat_completion</code><code translate="no" class="p">(</code><code translate="no" class="n">prompt</code><code translate="no" class="p">)</code></pre>
              
              <p>we get the following questions from the GPT-4 model:</p>
             
                <pre translate="no" data-type="programlisting">Sure, I have a few questions to help me suggest a suitable main course for you:
  1. What type of sport do you do, and do you have any specific nutritional
    requirements related to it?
  2. Are there any other vegetables or ingredients that you particularly enjoy
    or dislike?
  3. Do you prefer a hot or cold meal for lunch?
  4. Are you open to trying new recipes or do you prefer sticking to
    familiar dishes?</pre>
             
              <p>The questions asked by the model are often relevant. Based on these questions, you can update your context in your prompt. Therefore, we advise you to use this tip often when building your prompt.</p>
            </div></section>
            <section data-type="sect3" data-pdf-bookmark="The task"><div class="sect3" id="the_task_idi97kFa">
              <h4>The task</h4>
              <p>The task defines your use of<a contenteditable="false" data-type="indexterm" data-primary="prompt engineering" data-secondary="designing effective prompts" data-tertiary="task" id="ch04-task"></a><a contenteditable="false" data-type="indexterm" data-primary="task defined for prompt engineering" id="ch04-task2"></a> the GPT-4 model and should be well defined and specific. You should provide sufficient information for the task and use appropriate phrases in the prompt to guide the model to the desired outcome. </p>
              <p>In the preceding section, the task was for the model to suggest a main course for lunch. We can add details to that task to clarify what we want. Here, we ask the model to add a table with the ingredients and required quantities of each in grams:</p>
             
                <pre translate="no" data-type="programlisting" data-code-language="python"><code translate="no" class="n">prompt</code> <code translate="no" class="o">=</code> <code translate="no" class="s2">"""</code>
  <code translate="no" class="s2">Context: I do 2 hours of sport a day. I am vegetarian, and I don't like green </code>
  <code translate="no" class="s2">vegetables. I am conscientious about eating healthily.</code>
  <code translate="no" class="s2">Task: Give me a suggestion for a main course for today's lunch.</code>
  <code translate="no" class="s2">With this suggestion, I also want a table with two columns where each row </code>
  <code translate="no" class="s2">contains an ingredient from the main course.</code>
  <code translate="no" class="s2">The first column in the table is the name of the ingredient.</code>
  <code translate="no" class="s2">The second column of the table is the number of grams of that ingredient needed </code>
  <code translate="no" class="s2">for one person. Do not give the recipe for preparing the main course.</code>
  <code translate="no" class="s2">"""</code>
  <code translate="no" class="n">chat_completion</code><code translate="no" class="p">(</code><code translate="no" class="n">prompt</code><code translate="no" class="p">)</code></pre>
             
              <p class="pagebreak-before less_space">The model gives us the following output:</p>
             
                <pre translate="no" data-type="programlisting">Suggestion for today's lunch: Stuffed Bell Peppers with Quinoa and Black Beans
  | Ingredient          | Grams per person |
  |---------------------|------------------|
  | Bell peppers        | 200g             |
  | Quinoa              | 60g              |
  | Black beans         | 75g              |
  | Onion               | 50g              |
  | Garlic              | 5g               |
  | Olive oil           | 10g              |
  | Tomato sauce        | 50g              |
  | Cumin               | 1g               |
  | Chili powder        | 1g               |
  | Salt                | 1g               |
  | Pepper              | 1g               |
  | Shredded cheese     | 30g              |</pre>
              
              <p>On the <a href="https://platform.openai.com/examples">OpenAI API example page</a>, there is a list with 48 examples of tasks that GPT models can perform, always with the associated prompt and a demo. While these examples use the GPT-3 models and the completion endpoint, the principle would be the same for the chat endpoint, and the examples nicely illustrate how to give a task to OpenAI models. We won’t go through all of them here, but here are a few of them:</p>
              <dl>
                <dt>Grammar correction </dt>
                <dd>
                  <p>Corrects sentences to standard English. </p>
                
                  <p>Prompt: </p>
             
              <pre translate="no" data-type="programlisting">Correct this to standard English: She no went to the market.</pre>   </dd>
           
                <dt>Summarize for a second-grader</dt>
                <dd>
                  <p>Translates complex text into more straightforward concepts. </p>
                  <p>Prompt: </p>
               <pre translate="no" data-type="programlisting">Summarize this for a second-grade student: Jupiter is the fifth planet [...]</pre> </dd>
              
              
  
                <dt>TL;DR summarization</dt>
                <dd>
                  <p>TL;DR stands for “too long; didn’t read.” It has been observed that a text can be summarized by simply adding <code translate="no">T</code><code translate="no">l;dr</code> at the end. </p>
                
                  <p>Prompt: </p>
                  <pre translate="no" data-type="programlisting">A neutron star [...] atomic nuclei. Tl;dr</pre>
                </dd>
            
              
            
                <dt class="pagebreak-before less_space">Python to natural language </dt>
                <dd>
                  <p>Explain a piece of Python code in a language people can understand. </p>
            
                  <p>Prompt: </p>
                
             
              <pre translate="no" data-type="programlisting"># Python 3 
  def hello(x): 
  print('hello '+str(x)) 
  # Explanation of what the code does</pre></dd>
              
                <dt>Calculate time complexity </dt>
                <dd>
                  <p>Find the time complexity of a function. </p>
              
                  <p>Prompt:</p>
             
             
              <pre translate="no" data-type="programlisting"># Python 3 
  def hello(x, n):
       for i in range(n):
          print('hello '+str(x))
  # The time complexity of this function is </pre>   </dd>
              
                <dt>Python bug fixer </dt>
                <dd>
                  <p>Fixes code containing a bug. </p>
              
                  <p>Prompt: </p>
                
              
              <pre translate="no" data-type="programlisting">### Buggy Python
  def hello(x, n):
       for i in rang(n):
          print('hello '+str(x))
  ### Fixed Python</pre></dd>
            
                <dt>SQL request </dt>
                <dd>
                  <p>Simple SQL query building. </p>
              
                  <p>Prompt: </p>
               
              
              <pre translate="no" data-type="programlisting">Create a SQL request to find all users who live in California and have
  over 1000 credits.</pre> </dd>
              
                <dt>Analogy maker </dt>
                <dd>
                  <p>Can make an analogy between two words. </p>
             
                  <p>Prompt: </p>
              
             
              <pre translate="no" data-type="programlisting">Create an analogy for this phrase. Questions are arrows in that:</pre>  </dd>
           
                <dt class="pagebreak-before less_space">Summary notes </dt>
                <dd>
                  <p>Summarize notes from a meeting. </p>
              
                  <p>Prompt:</p>
               
              <pre translate="no" data-type="programlisting">Convert my shorthand into a first-hand account of the meeting:
  Tom: Profits up 50%
  Jane: New servers are online
  Kjel: Need more time to fix software</pre> </dd>
              </dl>
            </div></section>
            <section data-type="sect3" data-pdf-bookmark="The role"><div class="sect3" id="the_role_idbKSXAv">
              <h4>The role</h4>
              <p>A final way to influence the model<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch04-task" id="id890"></a><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch04-task2" id="id891"></a><a contenteditable="false" data-type="indexterm" data-primary="prompt engineering" data-secondary="designing effective prompts" data-tertiary="role" id="id892"></a><a contenteditable="false" data-type="indexterm" data-primary="role in prompt engineering" id="id893"></a> while writing the prompt is to give it a role. Of course, the role and the context can be used independently, but using both increases your control over the model’s output. In the following example, we put the model in the role of a sports nutrition expert:</p>
              
                <pre translate="no" data-type="programlisting" data-code-language="python"><code translate="no" class="n">prompt</code> <code translate="no" class="o">=</code> <code translate="no" class="s2">"""</code>
  <code translate="no" class="s2">Role: You are a nutritionist designing healthy diets for high-performance </code>
  <code translate="no" class="s2">athletes. You take into account the nutrition needed for a good recovery.</code>
  <code translate="no" class="s2">Context: I do 2 hours of sport a day. I am vegetarian, and I don't like green </code>
  <code translate="no" class="s2">vegetables. I am conscientious about eating healthily.</code>
  <code translate="no" class="s2">Task: Based on your expertise defined in your role, give me a suggestion for a </code>
  <code translate="no" class="s2">main course for today's lunch. With this suggestion, I also want a table with</code>
  <code translate="no" class="s2">two columns where each row in the table contains an ingredient from the main </code>
  <code translate="no" class="s2">course. The first column in the table is the name of the ingredient.</code>
  <code translate="no" class="s2">The second column of the table is the number of grams of that ingredient needed</code>
  <code translate="no" class="s2">for one person. Do not give the recipe for preparing the main course.</code>
  <code translate="no" class="s2">"""</code>
  <code translate="no" class="n">chat_completion</code><code translate="no" class="p">(</code><code translate="no" class="n">prompt</code><code translate="no" class="p">)</code></pre>
             
              <p>And we get the following result:</p>
             
                <pre translate="no" data-type="programlisting">Main Course Suggestion: Quinoa and Chickpea Stuffed Bell Peppers
  Ingredient Table:
  | Ingredient          | Grams per Person |
  |---------------------|------------------|
  | Quinoa              | 60               |
  | Chickpeas           | 100              |
  | Bell Peppers        | 200              |
  | Olive Oil           | 10               |
  | Onion               | 50               |
  | Garlic              | 5                |
  | Tomato              | 100              |
  | Feta Cheese         | 30               |
  | Kalamata Olives     | 20               |
  | Fresh Basil         | 5                |
  | Salt                | 2                |
  | Black Pepper        | 1                |</pre>
             
              <p>As you’ve seen, prompts can be used to condition the probabilistic distribution set of LLMs such as GPT models. They can be seen as a guide that directs the model to produce a particular type of result. While there is no definitive structure for prompt design, a useful framework to consider is the combination of context, role, and task.</p>
              <p>It’s important to understand that<a contenteditable="false" data-type="indexterm" data-primary="prompt engineering" data-secondary="designing effective prompts" data-tertiary="other approaches" id="id894"></a> this is just one approach, and prompts can be created without explicitly defining these elements. Some prompts may benefit from a different structure or require a more creative approach based on the specific needs of your application. Therefore, this context-role-task framework should not limit your thinking, but rather be a tool to help you effectively design your prompts when appropriate.<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch04-des" id="id895"></a></p>
            </div></section>
          </div></section>
          <section data-type="sect2" data-pdf-bookmark="Thinking Step by Step "><div class="sect2" id="thinking_step_by_step">
            <h3>Thinking Step by Step </h3>
            <p>As we know, GPT-4 is not good for computation. It cannot compute 369 × 1,235:<a contenteditable="false" data-type="indexterm" data-primary="prompt engineering" data-secondary="thinking step by step" id="ch04tsbs"></a><a contenteditable="false" data-type="indexterm" data-primary="calculations" data-secondary="thinking step by step" id="ch04tsbs2"></a><a contenteditable="false" data-type="indexterm" data-primary="math calculations" data-secondary="thinking step by step" id="ch04tsbs3"></a><a contenteditable="false" data-type="indexterm" data-primary="thinking step by step" id="ch04tsbs4"></a><a contenteditable="false" data-type="indexterm" data-primary="zero-shot-CoT strategy" id="ch04tsbs5"></a><a contenteditable="false" data-type="indexterm" data-primary="GPT models" data-secondary="“Let's think step by step” on prompt" data-secondary-sortas="Let's think" id="ch04tsbs6"></a><a contenteditable="false" data-type="indexterm" data-primary="prompt engineering" data-secondary="zero-shot-CoT strategy" id="ch04tsbs7"></a></p>
           
              <pre translate="no" data-type="programlisting" data-code-language="python"><code translate="no" class="n">prompt</code> <code translate="no" class="o">=</code> <code translate="no" class="s2">"How much is 369 * 1235?"</code>
  <code translate="no" class="n">chat_completion</code><code translate="no" class="p">(</code><code translate="no" class="n">prompt</code><code translate="no" class="p">)</code></pre>
         
            <p>We get the following answer: <code translate="no">454965</code></p>
            <p>The correct answer is 455,715. Does GPT-4 not solve complex mathematical problems? Remember that the model formulates this answer by predicting each token in the answer sequentially, starting from the left. This means that GPT-4 generates the leftmost digit first, then uses that as part of the context to generate the next digit, and so on, until the complete answer is formed. The challenge here is that each number is predicted independent of the final correct value. GPT-4 considers numbers like tokens; there is no mathematical logic. </p>
            <div data-type="note" epub:type="note"><h6>Note</h6>
              <p>In <a data-type="xref" href="ch05.html#advancing_llm_capabilities_with_the_langchain_fram">Chapter&nbsp;5</a>, we’ll explore how OpenAI has enriched GPT-4 with plug-ins. An example is a calculator plug-in for providing accurate mathematical solutions.</p>
            </div>
            <p>There is a trick to increasing the reasoning capacity of language models. For example, when asked to solve 369 × 1235, we can see that the model tries to answer directly in one shot. Consider that you probably won’t be able to solve this multiplication either without the help of a pencil and a sheet of paper to do the calculations. It is possible to encourage the model to make intermediate reasonings via the prompt. And like you with your pencil and paper, the model can solve more complex problems if you give it time to reason. </p>
            <p>Adding “Let’s think step by step” at the<a contenteditable="false" data-type="indexterm" data-primary="prompt engineering" data-secondary="thinking step by step" data-tertiary="“Let's think step by step” on prompt" data-tertiary-sortas="Let's think" id="id896"></a><a contenteditable="false" data-type="indexterm" data-primary="thinking step by step" data-secondary="“Let's think step by step” on prompt" data-secondary-sortas="Let's think" id="id897"></a><a contenteditable="false" data-type="indexterm" data-primary="math calculations" data-secondary="thinking step by step" data-tertiary="“Let's think step by step” on prompt" data-tertiary-sortas="Let's think" id="id898"></a><a contenteditable="false" data-type="indexterm" data-primary="calculations" data-secondary="thinking step by step" data-tertiary="“Let's think step by step” on prompt" data-tertiary-sortas="Let's think" id="id899"></a><a contenteditable="false" data-type="indexterm" data-primary="“Let's think step by step” on prompt" data-primary-sortas="Let's think" id="id900"></a><a contenteditable="false" data-type="indexterm" data-primary="zero-shot-CoT strategy" data-secondary="“Let's think step by step” on prompt" data-secondary-sortas="Let's think" id="id901"></a> end of the prompt has been empirically proven to enable the model to solve more complicated reasoning problems. <a contenteditable="false" data-type="indexterm" data-primary="thinking step by step" data-secondary="zero-shot-CoT strategy name" id="id902"></a><a contenteditable="false" data-type="indexterm" data-primary="“Large Language Models are Zero-Shot Reasoners” (Kojima et al.)" data-primary-sortas="Large Language Models are Zero-Shot" id="id903"></a><a contenteditable="false" data-type="indexterm" data-primary="Kojima, Takeshi" id="id904"></a>This technique, called the <em>zero-shot-CoT strategy</em>, was introduced in the scientific paper <a href="https://oreil.ly/2tHHy">“Large Language Models Are Zero-Shot Reasoners”</a> by Kojima et al., published in 2022. </p>
            <p><em>CoT</em> stands for <em>chain of thought</em>; it refers<a contenteditable="false" data-type="indexterm" data-primary="zero-shot-CoT strategy" data-secondary="CoT as chain of thought" id="id905"></a><a contenteditable="false" data-type="indexterm" data-primary="CoT as chain of thought" id="id906"></a><a contenteditable="false" data-type="indexterm" data-primary="chain of thought (CoT)" id="id907"></a> here to the technique of using prompts that encourage the model to imitate step-by-step reasoning. <a contenteditable="false" data-type="indexterm" data-primary="zero-shot-CoT strategy" data-secondary="zero-shot learning definition" id="id908"></a><a contenteditable="false" data-type="indexterm" data-primary="large language models (LLMs)" data-secondary="zero-shot learning definition" id="id909"></a>The term <em>zero-shot</em> means the model does not rely on task-specific examples to perform this reasoning; it is ready to handle new tasks based on its general training. <a contenteditable="false" data-type="indexterm" data-primary="few-shot learning" data-secondary="zero-shot learning versus" id="id910"></a>Unlike techniques such as few-shot learning (which we will examine shortly) that require the model to have seen examples of the task, zero-shot learning tries to generalize without needing task-specific examples.</p>
            <p>As we will see, with this sentence added to the prompt, the model begins to reason by breaking the problem into subproblems. As a result, it can take time to reason, allowing it to find solutions to problems it could not solve before in only one shot. Let’s see what happens if we change our input prompt:</p>
          
              <pre translate="no" data-type="programlisting" data-code-language="python"><code translate="no" class="n">prompt</code> <code translate="no" class="o">=</code> <code translate="no" class="s2">"How much is 369 * 1235 ? Let's think step by step."</code>
  <code translate="no" class="n">chat_completion</code><code translate="no" class="p">(</code><code translate="no" class="n">prompt</code><code translate="no" class="p">)</code></pre>
           
            <p>We get the following output:</p>
           
              <pre translate="no" data-type="programlisting">Step 1: Multiply 369 by 5 (the last digit of 1235).
  369 * 5 = 1845
  Step 2: Multiply 369 by 30 (the second last digit of 1235 is 3, so multiply by
     3 and add a 0 at the end).
  369 * 30 = 11070
  Step 3: Multiply 369 by 200 (the third digit of 1235 is 2, so multiply by 2 and
     add two 0s at the end).
  369 * 200 = 73800
  Step 4: Multiply 369 by 1000 (the first digit of 1235 is 1, so multiply by 1 and 
     add three 0s at the end).
  369 * 1000 = 369000
  Step 5: Add the results from steps 1 to 4.
  1845 + 11070 + 73800 + 369000 = 455715
  So, 369 * 1235 = 455,715.</pre>
            
            <p>There has been a remarkable shift in the model’s approach. It now breaks down the primary problem into smaller, more manageable steps rather than trying to tackle the problem head-on. </p>
            <div data-type="warning" epub:type="warning"><h6>Warning</h6>
              <p>Despite prompting the model to “think step by step,” it is still crucial that you carefully evaluate its responses, as GPT-4 is not infallible. For a more complex computation such as 3,695 × 123,548, even with this trick the LLM is not able to find the correct solution.</p>
            </div>
            <p>Of course, it’s hard to tell from one example whether this trick generally works or whether we just got lucky. On benchmarks with various math problems, empirical experiments have shown that this trick significantly increased the accuracy of GPT models. Although the trick works well for most math problems, it is not practical for all situations. The authors of “Large Language Models are Zero-Shot Reasoners” found it to be most beneficial for multistep arithmetic problems, problems involving symbolic reasoning, problems involving strategy, and other issues involving reasoning. It was not found to be useful for commonsense problems.<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch04tsbs" id="id911"></a><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch04tsbs2" id="id912"></a><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch04tsbs3" id="id913"></a><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch04tsbs4" id="id914"></a><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch04tsbs5" id="id915"></a><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch04tsbs6" id="id916"></a><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch04tsbs7" id="id917"></a></p>
          </div></section>
          <section data-type="sect2" data-pdf-bookmark="Implementing Few-Shot Learning "><div class="sect2" id="implementing_few_shot_learning">
            <h3>Implementing Few-Shot Learning </h3>
            <p><em>Few-shot learning</em>, introduced in <a href="https://oreil.ly/eSoRo">“Language Models Are Few-Shot Learners”</a> by Brown et al., <a contenteditable="false" data-type="indexterm" data-primary="few-shot learning" id="ch04-few"></a><a contenteditable="false" data-type="indexterm" data-primary="prompt engineering" data-secondary="few-shot learning implemented" id="ch04-few2"></a><a contenteditable="false" data-type="indexterm" data-primary="“Language Models are Few-Shot Learners” (Brown et al.)" data-primary-sortas="Language Models Are Few-Shot" id="id918"></a><a contenteditable="false" data-type="indexterm" data-primary="Brown, Tom B." id="id919"></a><a contenteditable="false" data-type="indexterm" data-primary="large language models (LLMs)" data-secondary="few-shot learning definition" id="id920"></a><a contenteditable="false" data-type="indexterm" data-primary="few-shot learning" data-secondary="about" id="id921"></a>refers to the ability of the LLM to generalize and produce valuable results with only a few examples in the prompt. With few-shot learning, you give a few examples of the task you want the model to perform, as illustrated in <a data-type="xref" href="#fig_2_a_prompt_containing_a_few_examples">Figure&nbsp;4-2</a>. These examples guide the model to process the desired output format.</p>
            <figure><div id="fig_2_a_prompt_containing_a_few_examples" class="figure">
              <img src="/api/v2/epubs/urn:orm:book:9781098152475/files/assets/dagc_0402.png" alt="" width="600" height="158">
              <h6><span class="label">Figure 4-2. </span>A prompt containing a few examples</h6>
            </div></figure>
            <p>In this example, we ask the LLM to convert specific words into emojis. It is difficult to imagine the instructions to put in a prompt to do this task. But with few-shot learning, it’s easy. Give it examples, and the model will automatically try to reproduce them: </p>
           
              <pre translate="no" data-type="programlisting" data-code-language="python"><code translate="no" class="n">prompt</code><code translate="no"> </code><code translate="no" class="o">=</code><code translate="no"> </code><code translate="no" class="s2">"""</code><code translate="no" class="s2">
  </code><code translate="no" class="s2">I go home --&gt; </code><img src="/api/v2/epubs/urn:orm:book:9781098152475/files/assets/blush.png" width="160" height="160"><code translate="no" class="s2"> go </code><img src="/api/v2/epubs/urn:orm:book:9781098152475/files/assets/house.png" width="160" height="160"><code translate="no" class="s2">
  </code><code translate="no" class="s2">my dog is sad --&gt; my </code><img src="/api/v2/epubs/urn:orm:book:9781098152475/files/assets/dog.png" width="160" height="160"><code translate="no" class="s2"> is </code><img src="/api/v2/epubs/urn:orm:book:9781098152475/files/assets/disappointed.png" width="160" height="160"><code translate="no" class="s2">
  </code><code translate="no" class="s2">I run fast --&gt; </code><img src="/api/v2/epubs/urn:orm:book:9781098152475/files/assets/blush.png" width="160" height="160"><code translate="no" class="s2"> run </code><img src="/api/v2/epubs/urn:orm:book:9781098152475/files/assets/zap.png" width="160" height="160"><code translate="no" class="s2">
  </code><code translate="no" class="s2">I love my wife --&gt; </code><img src="/api/v2/epubs/urn:orm:book:9781098152475/files/assets/blush.png" width="160" height="160"><code translate="no" class="s2"> </code><img src="/api/v2/epubs/urn:orm:book:9781098152475/files/assets/heart.png" width="160" height="160"><code translate="no" class="s2"> my wife</code><code translate="no" class="s2">
  </code><code translate="no" class="s2">the girl plays with the ball --&gt; the </code><img src="/api/v2/epubs/urn:orm:book:9781098152475/files/assets/girl.png" width="160" height="160"><code translate="no" class="s2"> </code><img src="/api/v2/epubs/urn:orm:book:9781098152475/files/assets/video_game.png" width="160" height="160"><code translate="no" class="s2"> with the </code><img src="/api/v2/epubs/urn:orm:book:9781098152475/files/assets/basketball.png" width="160" height="160"><code translate="no" class="s2">
  </code><code translate="no" class="s2">The boy writes a letter to a girl --&gt; </code><code translate="no" class="s2">
  </code><code translate="no" class="s2">"""</code><code translate="no">
  </code><code translate="no" class="n">chat_completion</code><code translate="no" class="p">(</code><code translate="no" class="n">prompt</code><code translate="no" class="p">)</code></pre>
            
            <p>From the preceding example, we get the following message as output:</p>
           
              <pre translate="no" data-type="programlisting">The <img src="/api/v2/epubs/urn:orm:book:9781098152475/files/assets/boy.png" width="160" height="160"> <img src="/api/v2/epubs/urn:orm:book:9781098152475/files/assets/writing-hand.png" width="120" height="120"> a <img src="/api/v2/epubs/urn:orm:book:9781098152475/files/assets/love_letter.png" width="160" height="160"> to a <img src="/api/v2/epubs/urn:orm:book:9781098152475/files/assets/girl.png" width="160" height="160"></pre>
           
            <p>The few-shot learning technique gives examples of inputs with the desired outputs. Then, in the last line, we provide the prompt for which we want a completion. This prompt is in the same form as the earlier examples. Naturally, the language model will perform a completion operation considering the pattern of the examples given.</p>
            <p>We can see that with only a few examples, the model can reproduce the instructions. By leveraging the extensive knowledge that LLMs have acquired in their training phase, they can quickly adapt and generate accurate answers based on only a few examples. </p>
            <div data-type="note" epub:type="note"><h6>Note</h6>
              <p>Few-shot learning is a <a contenteditable="false" data-type="indexterm" data-primary="large language models (LLMs)" data-secondary="few-shot learning definition" data-tertiary="powerful capability" id="id922"></a><a contenteditable="false" data-type="indexterm" data-primary="few-shot learning" data-secondary="about" data-tertiary="powerful aspect of LLMs" id="id923"></a>powerful aspect of LLMs because it allows them to be highly flexible and adaptable, requiring only a limited amount of additional information to perform various tasks.</p>
            </div>
            <p>When you provide examples in the prompt,<a contenteditable="false" data-type="indexterm" data-primary="prompt engineering" data-secondary="designing effective prompts" data-tertiary="context" id="id924"></a><a contenteditable="false" data-type="indexterm" data-primary="context in prompt design" data-secondary="clear and relevant for provided examples" id="id925"></a> it is essential to ensure that the context is clear and relevant. Clear examples improve the model’s ability to match the desired output format and execute the problem-solving process. Conversely, inadequate or ambiguous examples can lead to unexpected or incorrect results. Therefore, writing examples carefully and ensuring that they convey the correct information can significantly impact the model’s ability to perform the task accurately.</p>
            <p>Another approach to guiding LLMs is <em>one-shot learning</em>. <a contenteditable="false" data-type="indexterm" data-primary="one-shot learning" id="id926"></a><a contenteditable="false" data-type="indexterm" data-primary="prompt engineering" data-secondary="one-shot learning" id="id927"></a><a contenteditable="false" data-type="indexterm" data-primary="large language models (LLMs)" data-secondary="one-shot learning" id="id928"></a>As its name indicates, in this case you provide only one example to help the model execute the task. Although this approach provides less guidance than few-shot learning, it can be effective for more straightforward tasks or when the LLM already has substantial background knowledge about the topic. <a contenteditable="false" data-type="indexterm" data-primary="pricing OpenAI models" data-secondary="one-shot learning lowering costs" id="id929"></a><a contenteditable="false" data-type="indexterm" data-primary="prompt engineering" data-secondary="costs related to" data-tertiary="one-shot learning lower cost" id="id930"></a>The advantages of one-shot learning are simplicity, faster prompt generation, and lower computational cost and thus lower API costs. However, for complex tasks or situations that require a deeper understanding of the desired outcome, few-shot learning might be a more suitable approach to ensure accurate results. </p>
            <div data-type="tip"><h6>Tip</h6>
              <p>Prompt engineering has become a trending topic,<a contenteditable="false" data-type="indexterm" data-primary="prompt engineering" data-secondary="effective prompts collection link" id="id931"></a><a contenteditable="false" data-type="indexterm" data-primary="prompts" data-secondary="effective prompts collection link" id="id932"></a><a contenteditable="false" data-type="indexterm" data-primary="resources online" data-secondary="effective prompts collection" id="id933"></a> and you will find many online resources to delve deeper into the subject. As an example, this <a href="https://github.com/f/awesome-chatgpt-prompts">GitHub repository</a> contains a list of effective prompts that were contributed by more than 70 different users.</p>
            </div>
            <p>While this section explored various prompt engineering techniques that you can use individually, note that you can combine the techniques to obtain even better results. As a developer, it is your job to find the most effective prompt for your specific problem. Remember that prompt engineering is an iterative process of trial-and-error experimentation.<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch04-few" id="id934"></a><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch04-few2" id="id935"></a> </p>
          </div></section>
          <section data-type="sect2" data-pdf-bookmark="Improving Prompt Effectiveness "><div class="sect2" id="improving_prompt_effectiveness">
            <h3>Improving Prompt Effectiveness </h3>
            <p>We have seen several prompt engineering<a contenteditable="false" data-type="indexterm" data-primary="prompt engineering" data-secondary="improving prompt effectiveness" id="ch04-impef"></a> techniques that allow us to influence the behavior of the GPT models to get better results that meet our needs. We’ll end this section with a few more tips and tricks you can use in different situations when writing prompts for GPT models. </p>
            <section data-type="sect3" data-pdf-bookmark="Instruct the model to ask more questions"><div class="sect3" id="instruct_the_model_to_ask_more_questions">
              <h4>Instruct the model to ask more questions</h4>
              <p>Ending prompts by asking the model<a contenteditable="false" data-type="indexterm" data-primary="prompt engineering" data-secondary="designing effective prompts" data-tertiary="context questions for GPT to ask" id="id936"></a><a contenteditable="false" data-type="indexterm" data-primary="context in prompt design" data-secondary="questions for GPT to ask" data-tertiary="improving prompt effectiveness" id="id937"></a><a contenteditable="false" data-type="indexterm" data-primary="questions asked by GPT to clarify request" id="id938"></a><a contenteditable="false" data-type="indexterm" data-primary="answering questions" data-secondary="questions from GPT for clarification" id="id939"></a> if it understood the question and instructing the model to ask more questions is an effective technique if you are building a chatbot-based solution. You can add a text like this to the end of your prompts:</p>
             
                <pre translate="no" data-type="programlisting">Did you understand my request clearly? If you do not fully understand my request,
  ask me questions about the context so that when I answer, you can
  perform the requested task more efficiently.</pre>
              
            </div></section>
            <section data-type="sect3" data-pdf-bookmark="Format the output"><div class="sect3" id="format_the_output">
              <h4>Format the output</h4>
              <p>Sometimes you’ll want to use the LLM<a contenteditable="false" data-type="indexterm" data-primary="output result format" data-secondary="JSON output formatting" id="id940"></a><a contenteditable="false" data-type="indexterm" data-primary="JSON output formatting" id="id941"></a> output in a longer process: in such cases, the output format matters. For example, if you want a JSON output, the model tends to write in the output before and after the JSON block. If you add in the prompt <code translate="no">the output must be accepted by json.loads</code> then it tends to work better. This type of trick can be used in many situations.</p>
              <p>For example, with this script:</p>
             
                <pre translate="no" data-type="programlisting" data-code-language="python"><code translate="no" class="n">prompt</code> <code translate="no" class="o">=</code> <code translate="no" class="s2">"""</code>
  <code translate="no" class="s2">Give a JSON output with 5 names of animals. The output must be accepted </code>
  <code translate="no" class="s2">by json.loads.</code>
  <code translate="no" class="s2">"""</code>
  <code translate="no" class="n">chat_completion</code><code translate="no" class="p">(</code><code translate="no" class="n">prompt</code><code translate="no" class="p">,</code> <code translate="no" class="n">model</code><code translate="no" class="o">=</code><code translate="no" class="s1">'gpt-4'</code><code translate="no" class="p">)</code></pre>
              
              <p>we get the following JSON block of code:</p>
              
                <pre translate="no" data-type="programlisting">{
    "animals": [
      "lion",
      "tiger",
      "elephant",
      "giraffe",
      "zebra"
    ]
  }</pre>
             
            </div></section>
            <section data-type="sect3" data-pdf-bookmark="Repeat the instructions"><div class="sect3" id="repeat_the_instructions">
              <h4>Repeat the instructions</h4>
              <p>It has been found empirically that<a contenteditable="false" data-type="indexterm" data-primary="prompt engineering" data-secondary="designing effective prompts" data-tertiary="repeating instructions, differently each time" id="id942"></a><a contenteditable="false" data-type="indexterm" data-primary="repeating instructions, differently each time" id="id943"></a> repeating instructions gives good results, especially when the prompt is long. The idea is to add to the prompt the same instruction several times, but formulated differently each time. </p>
              <p>This can also be done with negative prompts.</p>
            </div></section>
            <section data-type="sect3" data-pdf-bookmark="Use negative prompts"><div class="sect3" id="use_negative_prompts">
              <h4>Use negative prompts</h4>
              <p>Negative prompts in the context<a contenteditable="false" data-type="indexterm" data-primary="prompts" data-secondary="negative prompts with text generation" id="id944"></a><a contenteditable="false" data-type="indexterm" data-primary="negative prompts with text generation" id="id945"></a><a contenteditable="false" data-type="indexterm" data-primary="text generation" data-secondary="negative prompts with" id="id946"></a> of text generation are a way to guide the model by specifying what you don’t want to see in the output. They act as constraints or guidelines to filter out certain types of responses. This technique is particularly useful when the task is complicated: models tend to follow instructions more precisely when the tasks are repeated several times in different ways. </p>
              <p>Continuing with the previous example, we can insist on the output format with negative prompting by adding <code translate="no">Do not add anything before or after the json </code><code translate="no">text.</code>.</p>
              <p>In <a data-type="xref" href="ch03.html#building_apps_with_gpt_4_and_chatgpt">Chapter&nbsp;3</a>, we used negative prompting in the third project:</p>
             
                <pre translate="no" data-type="programlisting">Extract the keywords from the following question: {user_question}. Do not answer
  anything else, only the keywords.</pre>
             
              <p>Without this addition to the prompt, the model tended to not follow the instructions.</p>
            </div></section>
            <section data-type="sect3" data-pdf-bookmark="Add length constraints"><div class="sect3" id="add_length_constraints">
              <h4><a id="_5k0cl1wyforu"></a>Add length constraints</h4>
              <p>A length constraint is often<a contenteditable="false" data-type="indexterm" data-primary="prompts" data-secondary="length of answer specified" id="id947"></a><a contenteditable="false" data-type="indexterm" data-primary="question answering" data-secondary="length of answer specified in prompt" id="id948"></a><a contenteditable="false" data-type="indexterm" data-primary="answering questions" data-secondary="length of answer specified in prompt" id="id949"></a> a good idea: if you expect only a single-word answer or 10 sentences, add it to your prompt. This is what we did in <a data-type="xref" href="ch03.html#building_apps_with_gpt_4_and_chatgpt">Chapter&nbsp;3</a> in the first project: we specified <code translate="no">LENGTH: 100 words</code> to generate an adequate news article. In the fourth project, our prompt also had a length instruction: <code translate="no">If you can answer the question: ANSWER, if you need more information: MORE, if you can not answer: OTHER. Only answer one </code><code translate="no">word.</code>. Without that last sentence, the model would tend to formulate sentences rather than follow the instructions.<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch04-impef" id="id950"></a></p>
            </div></section>
          </div></section>
        </div></section>
        <section data-type="sect1" data-pdf-bookmark="Fine-Tuning"><div class="sect1" id="fine_tuning">
          <h2>Fine-Tuning</h2>
          <p>OpenAI provides many ready-to-use GPT models. Although these models excel at a broad array of tasks, fine-tuning them for specific tasks or contexts can further enhance their performance.</p>
          <section data-type="sect2" data-pdf-bookmark="Getting Started"><div class="sect2" id="getting_started">
            <h3>Getting Started</h3>
            <p>Let’s imagine that you want to<a contenteditable="false" data-type="indexterm" data-primary="fine-tuning a model" data-secondary="getting started" id="id951"></a><a contenteditable="false" data-type="indexterm" data-primary="email response generator example of fine-tuning" id="id952"></a><a contenteditable="false" data-type="indexterm" data-primary="GPT models" data-secondary="fine-tuning an existing model" data-seealso="fine-tuning a model" id="id953"></a> create an email response generator for your company. As your company works in a specific industry with a particular vocabulary, you want the generated email responses to retain your current writing style. There are two strategies for doing this: either you can use the prompt engineering techniques introduced earlier to force the model to output the text you want, or you can fine-tune an existing model. This section explores the second technique. </p>
            <p>For this example, you must collect a large number of emails containing data about your particular business domain, inquiries from customers, and responses to those inquiries. You can then use this data to fine-tune an existing model to learn your company’s specific language patterns and vocabulary. <a contenteditable="false" data-type="indexterm" data-primary="fine-tuning a model" data-secondary="about" id="id954"></a><a contenteditable="false" data-type="indexterm" data-primary="large language models (LLMs)" data-secondary="existing model fine-tuned" data-seealso="fine-tuning a model" id="id955"></a>The fine-tuned model is essentially a new model built from one of the original models provided by OpenAI, in which the internal weights of the model are adjusted to fit your specific problem so that the new model increases its accuracy on tasks similar to the examples it saw in the dataset provided for the fine-tuning. By fine-tuning an existing LLM, it is possible to create a highly customized and specialized email response generator tailored explicitly to the language patterns and words used in your particular business.</p>
            <p><a data-type="xref" href="#fig_3_the_fine_tuning_process">Figure&nbsp;4-3</a> illustrates the fine-tuning process in which a dataset from a specific domain is used to update the internal weights of an existing GPT model. The objective is for the new fine-tuned model to make better predictions in the particular domain than the original GPT model. <a contenteditable="false" data-type="indexterm" data-primary="fine-tuning a model" data-secondary="getting started" data-tertiary="new model on OpenAI servers" id="id956"></a><a contenteditable="false" data-type="indexterm" data-primary="APIs (application programming interfaces)" data-secondary="OpenAI API" data-tertiary="fine-tuning an existing model" id="id957"></a><a contenteditable="false" data-type="indexterm" data-primary="OpenAI" data-secondary="API" data-tertiary="fine-tuning an existing model" id="id958"></a>It should be emphasized that this is a <em>new model</em>. This new model is on the OpenAI servers: as before, you must use the OpenAI APIs to use it, as it cannot be accessed locally.</p>
            <figure><div id="fig_3_the_fine_tuning_process" class="figure">
              <img src="/api/v2/epubs/urn:orm:book:9781098152475/files/assets/dagc_0403.png" alt="" width="600" height="532">
              <h6><span class="label">Figure 4-3. </span>The fine-tuning process</h6>
            </div></figure>
            <div data-type="note" epub:type="note"><h6>Note</h6>
              <p>Even after you have fine-tuned an LLM with your own specific data, the new model remains on OpenAI’s servers. You’ll interact with it through OpenAI’s APIs, not locally.</p>
            </div>
            <section data-type="sect3" data-pdf-bookmark="Adapting GPT base models for domain-specific needs"><div class="sect3" id="adapting_gpt_base_models_for_domain_specific_needs">
              <h4>Adapting GPT base models for domain-specific needs</h4>
              <p>Currently, fine-tuning is only available<a contenteditable="false" data-type="indexterm" data-primary="fine-tuning a model" data-secondary="getting started" data-tertiary="models that can be fine-tuned" id="id959"></a><a contenteditable="false" data-type="indexterm" data-primary="GPT models" data-secondary="fine-tuning an existing model" data-tertiary="models that can be fine-tuned" id="id960"></a><a contenteditable="false" data-type="indexterm" data-primary="davinci available in API" data-secondary="fine-tuning available" id="id961"></a><a contenteditable="false" data-type="indexterm" data-primary="curie available in API" data-secondary="fine-tuning available" id="id962"></a><a contenteditable="false" data-type="indexterm" data-primary="babbage available in API" data-secondary="fine-tuning available" id="id963"></a><a contenteditable="false" data-type="indexterm" data-primary="ada available in API" data-secondary="fine-tuning available" id="id964"></a><a contenteditable="false" data-type="indexterm" data-primary="fine-tuning a model" data-secondary="getting started" data-tertiary="adapting a base model" id="id965"></a><a contenteditable="false" data-type="indexterm" data-primary="pricing OpenAI models" data-secondary="fine-tuning existing models" id="id966"></a><a contenteditable="false" data-type="indexterm" data-primary="fine-tuning a model" data-secondary="pricing" id="id967"></a> for the <code translate="no">davinci</code>, <code translate="no">curie</code>, <code translate="no">babbage</code>, and <code translate="no">ada</code> base models. Each of these offers a trade-off between accuracy and required resources. As a developer, you can select the most appropriate model for your application: while the smaller models, such as <code translate="no">ada</code> and <code translate="no">babbage</code>, may be faster and more cost-effective for simple tasks or applications with limited resources, the larger models, <code translate="no">curie</code> and <code translate="no">davinci</code>, offer more advanced language processing and generation capabilities, making them ideal for more complex tasks in which higher accuracy is critical.</p>
              <p class="pagebreak-before less_space">These are the original models that are not part of the InstructGPT family of models. For example, they did not benefit from a reinforcement learning phase with a human in the loop. By fine-tuning these base models—for example, adjusting their internal weights based on a custom dataset—you can tailor them to specific tasks or domains. Although they do not have the processing and reasoning capabilities of the InstructGPT family, they do provide a strong foundation for building specialized applications by leveraging their pretrained language processing and generation <span class="keep-together">capabilities.</span></p>
              <div data-type="note" epub:type="note"><h6>Note</h6>
                <p>For fine-tuning, you must use the base models; it is not possible to use the instructed models. </p>
              </div>
            </div></section>
            <section data-type="sect3" data-pdf-bookmark="Fine-tuning versus few-shot learning"><div class="sect3" id="fine_tuning_vs_few_shot_learning">
              <h4>Fine-tuning versus few-shot learning</h4>
              <p>Fine-tuning is a process of <em>retraining</em> an<a contenteditable="false" data-type="indexterm" data-primary="fine-tuning a model" data-secondary="few-shot learning versus" id="id968"></a><a contenteditable="false" data-type="indexterm" data-primary="few-shot learning" data-secondary="fine-tuning versus" id="id969"></a><a contenteditable="false" data-type="indexterm" data-primary="large language models (LLMs)" data-secondary="few-shot learning definition" data-tertiary="fine-tuning an existing model versus" id="id970"></a><a contenteditable="false" data-type="indexterm" data-primary="GPT models" data-secondary="fine-tuning an existing model" data-tertiary="few-shot learning versus" id="id971"></a> existing model on a set of data from a specific task to improve its performance and make its answers more accurate. In fine-tuning, you update the internal parameters of the model. As we saw before, few-shot learning provides the model with a limited number of good examples through its input prompt, which guides the model to produce desired results based on these few examples. With few-shot learning, the internal parameters of the model are not modified. </p>
              <p>Both fine-tuning and few-shot learning can serve to enhance GPT models. Fine-tuning produces a highly specialized model that can provide more accurate and contextually relevant results for a given task. This makes it an ideal choice for cases in which a large amount of data is available. This customization ensures that the generated content is more closely aligned with the target domain’s specific language patterns, vocabulary, and tone. </p>
              <p>Few-shot learning is a more flexible and data-efficient approach because it does not require retraining the model. This technique is beneficial when limited examples are available or rapid adaptation to different tasks is needed. Few-shot learning allows developers to quickly prototype and experiment with various tasks, making it a versatile and practical option for many use cases. Another essential criterion for choosing between the two methods is that using and training a model that uses fine-tuning is more expensive.</p>
              <p>Fine-tuning methods often require<a contenteditable="false" data-type="indexterm" data-primary="fine-tuning a model" data-secondary="about" data-tertiary="data required in vast amounts" id="id972"></a> vast amounts of data. The lack of available examples often limits the use of this type of technique. To give you an idea of the amount of data needed for fine-tuning, you can assume that for relatively simple tasks or when only minor adjustments are required, you may achieve good fine-tuning results with a few hundred examples of input prompts with their corresponding desired completion. This approach works when the pretrained GPT model already performs reasonably well on the task but needs slight refinements to better align with the target domain. However, for more complex tasks or in situations where your app needs more customization, your model may need to use many thousands of examples for the training. This can, for example, correspond to the use case we proposed earlier, with the automatic response to an email that respects your writing style. You can also do fine-tuning for very specialized tasks for which your model may need hundreds of thousands or even millions of examples. This fine-tuning scale can lead to significant performance improvements and better model adaptation to the specific domain. </p>
              <div data-type="note" epub:type="note"><h6>Note</h6>
                <p><em>Transfer learning</em> applies<a contenteditable="false" data-type="indexterm" data-primary="transfer learning" id="id973"></a><a contenteditable="false" data-type="indexterm" data-primary="transfer learning" data-secondary="fine-tuning and" id="id974"></a><a contenteditable="false" data-type="indexterm" data-primary="fine-tuning a model" data-secondary="transfer learning and" id="id975"></a> knowledge learned from one domain to a different but related environment. Therefore, you may sometimes hear the term <em>transfer learning</em> in relation to fine-tuning. </p>
              </div>
            </div></section>
          </div></section>
          <section data-type="sect2" data-pdf-bookmark="Fine-Tuning with the OpenAI API"><div class="sect2" id="fine_tuning_with_the_openai_api">
            <h3>Fine-Tuning with the OpenAI API</h3>
            <p>This section guides you through<a contenteditable="false" data-type="indexterm" data-primary="fine-tuning a model" data-secondary="OpenAI API for" data-tertiary="preparing the data" id="id976"></a><a contenteditable="false" data-type="indexterm" data-primary="OpenAI" data-secondary="API" data-tertiary="fine-tuning an existing model" id="ch04-ftem"></a><a contenteditable="false" data-type="indexterm" data-primary="APIs (application programming interfaces)" data-secondary="OpenAI API" data-tertiary="fine-tuning an existing model" id="ch04-ftem2"></a> the process of tuning an LLM using the OpenAI API. We will explain how to prepare your data, upload datasets, and create a fine-tuned model using the API.</p>
            <section data-type="sect3" data-pdf-bookmark="Preparing your data "><div class="sect3" id="preparing_your_data">
              <h4>Preparing your data </h4>
              <p>To update an LLM model, it is necessary to provide a dataset with examples. <a contenteditable="false" data-type="indexterm" data-primary="datasets" data-secondary="JSONL file for fine-tuning" id="id977"></a><a contenteditable="false" data-type="indexterm" data-primary="JSONL dataset for fine-tuning" id="id978"></a>The dataset should be in a JSONL file in which each row corresponds to a pair of prompts and completions: </p>
              
                <pre translate="no" data-type="programlisting">{"prompt": "&lt;prompt text&gt;", "completion": "&lt;completion text&gt;"}
  {"prompt": "&lt;prompt text&gt;", "completion": "&lt;completion text&gt;"}
  {"prompt": "&lt;prompt text&gt;", "completion": "&lt;completion text&gt;"}
  …</pre>
            
              <p>A JSONL file is a text file, with each line representing a single JSON object. You can use it to store large amounts of data efficiently. <a contenteditable="false" data-type="indexterm" data-primary="fine-tuning a model" data-secondary="OpenAI API for" data-tertiary="tool for preparing the data" id="id979"></a><a contenteditable="false" data-type="indexterm" data-primary="datasets" data-secondary="JSONL file for fine-tuning" data-tertiary="tool for" id="id980"></a><a contenteditable="false" data-type="indexterm" data-primary="JSONL dataset for fine-tuning" data-secondary="tool for" id="id981"></a>OpenAI provides a tool that helps you generate this training file. This tool can take various file formats as input (CSV, TSV, XLSX, JSON, or JSONL), requiring only that they contain a prompt and completion column/key, and that they output a training JSONL file ready to be sent for the fine-tuning process. This tool also validates and gives suggestions to improve the quality of your data.</p>
              <p>Run this tool in your terminal using the following line of code: <a contenteditable="false" data-type="indexterm" data-primary="openai tools fine_tunes.prepare_data" id="id982"></a></p>
             
                <pre translate="no" data-type="programlisting">$ openai tools fine_tunes.prepare_data -f &lt;LOCAL_FILE&gt;</pre>
            
              <p>The application will make a series of suggestions to improve the result of the final file; you can accept them or not. You can also specify the option <code translate="no">-q</code>, which auto-accepts all suggestions.</p>
              <div data-type="note" epub:type="note"><h6>Note</h6>
                <p>This <code translate="no">openai</code> tool was installed and available in your terminal when you executed <code translate="no">pip install openai</code>.<a contenteditable="false" data-type="indexterm" data-primary="pip" data-secondary="installing openai" id="id983"></a></p>
              </div>
              <p>If you have enough data, the tool will ask whether dividing the data into training and validation sets is necessary. This is a recommended practice. The algorithm will use the training data to modify the model’s parameters during fine-tuning. The validation set can measure the model’s performance on a dataset that has not been used to update the parameters. </p>
              <p>Fine-tuning an LLM benefits from using high-quality examples, ideally reviewed by experts. When fine-tuning with preexisting datasets, ensure that the data is screened for offensive or inaccurate content, or examine random samples if the dataset is too large to review all entries manually.</p>
            </div></section>
            <section data-type="sect3" data-pdf-bookmark="Making your data available "><div class="sect3" id="making_your_data_available">
              <h4>Making your data available </h4>
              <p>Once your dataset with the training<a contenteditable="false" data-type="indexterm" data-primary="fine-tuning a model" data-secondary="OpenAI API for" data-tertiary="data uploaded to OpenAI servers" id="id984"></a><a contenteditable="false" data-type="indexterm" data-primary="datasets" data-secondary="JSONL file for fine-tuning" data-tertiary="uploading to OpenAI servers" id="id985"></a><a contenteditable="false" data-type="indexterm" data-primary="JSONL dataset for fine-tuning" data-secondary="uploading to OpenAI servers" id="id986"></a> examples is prepared, you need to upload it to the OpenAI servers. The OpenAI API provides different functions to manipulate files. Here are the most important ones: </p>
              <p>Uploading a file:<a contenteditable="false" data-type="indexterm" data-primary="openai.File.create()" id="id987"></a> </p>
              <pre translate="no" data-type="programlisting" data-code-language="python"><code translate="no" class="n">openai</code><code translate="no" class="o">.</code><code translate="no" class="n">File</code><code translate="no" class="o">.</code><code translate="no" class="n">create</code><code translate="no" class="p">(</code>
      <code translate="no" class="n">file</code><code translate="no" class="o">=</code><code translate="no" class="nb">open</code><code translate="no" class="p">(</code><code translate="no" class="s2">"out_openai_completion_prepared.jsonl"</code><code translate="no" class="p">,</code> <code translate="no" class="s2">"rb"</code><code translate="no" class="p">),</code>
      <code translate="no" class="n">purpose</code><code translate="no" class="o">=</code><code translate="no" class="s1">'fine-tune'</code>
  <code translate="no" class="p">)</code></pre>
              <ul class="simplelist">
                <li>
                  <p>Two parameters are mandatory: <code translate="no">file</code> and <code translate="no">purpose</code>. Set <code translate="no">purpose</code> to <code translate="no">fine-tune</code>. This validates the downloaded file format for fine-tuning. The output of this function is a dictionary in which you can retrieve the <code translate="no">file_id</code> in the <code translate="no">id</code> field. Currently, the total file size can be up to 1 GB. For more, you need to contact OpenAI.</p>
                </li>
              </ul>
              <p>Deleting a file:<a contenteditable="false" data-type="indexterm" data-primary="openai.File.delete()" id="id988"></a> </p>
              <pre translate="no" data-type="programlisting" data-code-language="python"><code translate="no" class="n">openai</code><code translate="no" class="o">.</code><code translate="no" class="n">File</code><code translate="no" class="o">.</code><code translate="no" class="n">delete</code><code translate="no" class="p">(</code><code translate="no" class="s2">"file-z5mGg(...)"</code><code translate="no" class="p">)</code></pre>
              <ul>
                <li>
                  <p>One parameter is mandatory: <code translate="no">file_id</code>.</p>
                </li>
              </ul>
              <p class="pagebreak-before less_space">Listing all uploaded files:<a contenteditable="false" data-type="indexterm" data-primary="openai.File.list()" id="id989"></a> </p>
              <pre translate="no" data-type="programlisting" data-code-language="python"><code translate="no" class="n">openai</code><code translate="no" class="o">.</code><code translate="no" class="n">File</code><code translate="no" class="o">.</code><code translate="no" class="n">list</code><code translate="no" class="p">()</code></pre>
              <ul>
                <li>
                  <p>It can be helpful to retrieve the ID of a file, for example, when you start the fine-tuning process.</p>
                </li>
              </ul>
            </div></section>
            <section data-type="sect3" data-pdf-bookmark="Creating a fine-tuned model"><div class="sect3" id="creating_a_fine_tuned_model">
              <h4>Creating a fine-tuned model</h4>
              <p>Fine-tuning an uploaded file<a contenteditable="false" data-type="indexterm" data-primary="fine-tuning a model" data-secondary="OpenAI API for" data-tertiary="creating a fine-tuned model" id="id990"></a><a contenteditable="false" data-type="indexterm" data-primary="openai.FineTune.create()" id="id991"></a> is a straightforward process. The endpoint <code translate="no">openai.FineTune.create()</code> creates a job on the OpenAI servers to refine a specified model from a given dataset. The response of this function contains the details of the queued job, including the status of the job, the <code translate="no">fine_tune_id</code>, and the name of the model at the end of the process.</p>
              <p>The main input parameters are described in <a data-type="xref" href="#table-4-1">Table&nbsp;4-1</a>.</p>
              <table class="lines" id="table-4-1">
                <caption><span class="label">Table 4-1. </span>Parameters for <code translate="no">openai.FineTune.create()</code></caption>
                <thead><tr>
                  <th>Field name</th>
                  <th>Type</th>
                  <th>Description</th>
                </tr></thead>
               <tbody> 
                <tr>
                  <td>
                    <code translate="no"><span class="keep-together">training_file</span> </code>
                  </td>
                  <td>String</td>
                  <td>This is the only mandatory parameter containing the <code translate="no">file_id</code> of the uploaded file. Your dataset must be formatted as a JSONL file. Each training example is a JSON object with the keys <code translate="no">prompt</code> and <code translate="no">completion</code>. </td>
                </tr>
                <tr>
                  <td>
                    <code translate="no">model </code>
                  </td>
                  <td>String</td>
                  <td>This specifies the base model used for fine-tuning. You can select <code translate="no">ada</code>, <code translate="no">babbage</code>, <code translate="no">curie</code>, <code translate="no">davinci</code>, or a previously tuned model. The default base model is <code translate="no">curie</code>. </td>
                </tr>
                <tr>
                  <td>
                    <code translate="no"><span class="keep-together">validation_file</span> </code>
                  </td>
                  <td>String</td>
                  <td>This contains the <code translate="no">file_id</code> of the uploaded file with the validation data. If you provide this file, the data will be used to generate validation metrics periodically during fine-tuning.</td>
                </tr>
                <tr>
                  <td>
                    <code translate="no">suffix </code>
                  </td>
                  <td>String</td>
                  <td>This is a string of up to 40 characters that is added to your custom model name.</td>
                </tr></tbody>
              </table>
            </div></section>
            <section data-type="sect3" data-pdf-bookmark="Listing fine-tuning jobs"><div class="sect3" id="listing_fine_tuning_jobs">
              <h4>Listing fine-tuning jobs</h4>
              <p>It is possible to obtain a list of all the fine-tuning jobs on the OpenAI servers via the following function:<a contenteditable="false" data-type="indexterm" data-primary="openai.FineTune.list()" id="id992"></a><a contenteditable="false" data-type="indexterm" data-primary="fine-tuning a model" data-secondary="OpenAI API for" data-tertiary="listing fine-tuning jobs" id="id993"></a></p>
            
                <pre translate="no" data-type="programlisting" data-code-language="python"><code translate="no" class="n">openai</code><code translate="no" class="o">.</code><code translate="no" class="n">FineTune</code><code translate="no" class="o">.</code><code translate="no" class="n">list</code><code translate="no" class="p">()</code></pre>
              
              <p>The result is a dictionary that contains information on all the refined models. </p>
            </div></section>
            <section data-type="sect3" data-pdf-bookmark="Canceling a fine-tuning job"><div class="sect3" id="canceling_a_fine_tuning_job">
              <h4>Canceling a fine-tuning job</h4>
              <p>It is possible to immediately interrupt a job running on OpenAI servers via the following function:<a contenteditable="false" data-type="indexterm" data-primary="openai.FineTune.cancel()" id="id994"></a><a contenteditable="false" data-type="indexterm" data-primary="fine-tuning a model" data-secondary="OpenAI API for" data-tertiary="canceling a fine-tuning job" id="id995"></a></p>
             
                <pre translate="no" data-type="programlisting" data-code-language="python"><code translate="no" class="n">openai</code><code translate="no" class="o">.</code><code translate="no" class="n">FineTune</code><code translate="no" class="o">.</code><code translate="no" class="n">cancel</code><code translate="no" class="p">()</code></pre>
              
              <p class="pagebreak-before less_space">This function has only one mandatory parameter: <code translate="no">fine_tune_id</code>. The <code translate="no">fine_tune_id</code> parameter is a string that starts with <code translate="no">ft-</code>; for example, <code translate="no">ft-Re12otqdRaJ(...)</code>. It is obtained after the creation of your job with the function <code translate="no">openai.FineTune.​cre⁠ate()</code>. If you have lost your <code translate="no">fine_tune_id</code>, you can retrieve it with <code translate="no">openai.FineTune.list()</code>.<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch04-ftem" id="id996"></a><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch04-ftem2" id="id997"></a></p>
            </div></section>
          </div></section>
          <section data-type="sect2" data-pdf-bookmark="Fine-Tuning Applications "><div class="sect2" id="fine_tuning_applications">
            <h3>Fine-Tuning Applications </h3>
            <p>Fine-tuning offers a powerful way to enhance<a contenteditable="false" data-type="indexterm" data-primary="fine-tuning a model" data-secondary="across applications" data-tertiary="about" id="id998"></a><a contenteditable="false" data-type="indexterm" data-primary="large language models (LLMs)" data-secondary="fine-tuning across applications" data-tertiary="about" id="id999"></a> the performance of models across various applications. This section looks at several use cases in which fine-tuning has been effectively deployed. Take inspiration from these examples! Perhaps you have the same kind of issue in your use cases. <a contenteditable="false" data-type="indexterm" data-primary="pricing OpenAI models" data-secondary="fine-tuning existing models" data-tertiary="other prompt engineering versus" id="id1000"></a><a contenteditable="false" data-type="indexterm" data-primary="fine-tuning a model" data-secondary="pricing" id="id1001"></a>Once again, remember that fine-tuning is more expensive than other techniques based on prompt engineering, and therefore, it will not be necessary for most of your situations. But when it is, this technique can significantly improve your results.</p>
            <section data-type="sect3" data-pdf-bookmark="Legal document analysis"><div class="sect3" id="legal_document_analysis">
              <h4>Legal document analysis</h4>
              <p>In this use case, an LLM is used to process<a contenteditable="false" data-type="indexterm" data-primary="fine-tuning a model" data-secondary="across applications" data-tertiary="legal document analysis" id="id1002"></a><a contenteditable="false" data-type="indexterm" data-primary="legal document analysis" id="id1003"></a><a contenteditable="false" data-type="indexterm" data-primary="large language models (LLMs)" data-secondary="fine-tuning across applications" data-tertiary="legal document analysis" id="id1004"></a> legal texts and extract valuable information. These documents are often written with specific jargon, which makes it difficult for nonspecialists to understand these types of texts. <a contenteditable="false" data-type="indexterm" data-primary="legal document analysis" data-secondary="GPT-4 90th percentile score on Uniform Bar Exam" id="id1005"></a>We already saw in <a data-type="xref" href="ch01.html#gpt_4_and_chatgpt_essentials">Chapter&nbsp;1</a> that when tested on the Uniform Bar Exam, GPT-4 had a score in the 90th percentile. In this case, fine-tuning could specialize the model for a specific domain and/or allow it to assist nonspecialists in the legal process. By fine-tuning an LLM on a legal corpus of a particular topic or for a specific type of end user, the model can process the intricacies of legal language better and become more adept at performing tasks related to that particular type of end user.</p>
              <p>Fine-tuning an LLM with a large amount of data to analyze legal documents can potentially significantly improve the model’s performance in these tasks by allowing it to better process the nuances of legal language that are often beyond the capabilities of prompt engineering techniques.</p>
            </div></section>
            <section data-type="sect3" data-pdf-bookmark="Automated code review"><div class="sect3" id="automated_code_review">
              <h4>Automated code review</h4>
              <p>In this use case, fine-tuning can help<a contenteditable="false" data-type="indexterm" data-primary="fine-tuning a model" data-secondary="across applications" data-tertiary="automated code review" id="id1006"></a><a contenteditable="false" data-type="indexterm" data-primary="automated code review" id="id1007"></a><a contenteditable="false" data-type="indexterm" data-primary="large language models (LLMs)" data-secondary="fine-tuning across applications" data-tertiary="automated code review" id="id1008"></a> the model analyze and suggest improvements for developer-written code. This requires training the model on a dataset comprising code snippets and comments, enabling the model to process language-specific syntax, semantics, and best practices. <a contenteditable="false" data-type="indexterm" data-primary="GitHub" data-secondary="Copilot powered by Codex model" data-tertiary="automated code review" id="id1009"></a><a contenteditable="false" data-type="indexterm" data-primary="Copilot (GitHub) powered by Codex model" data-secondary="automated code review" id="id1010"></a>Note that this use case is similar to what GitHub does with its Copilot tool, which is designed to assist developers in writing code by providing suggestions of code and entire functions in real time. You can use fine-tuning to train the model on a specific task such as code review, and build a project for your own code with specific standards or dedicated to a particular programming framework.</p>
              <p>As an example of an input file<a contenteditable="false" data-type="indexterm" data-primary="JSONL dataset for fine-tuning" data-secondary="automated code review" id="id1011"></a> for this use case, your JSONL file could contain pairs of code and their corresponding review comments. This would help the model learn how to provide accurate and relevant feedback on code quality. One line of the JSONL file could be something like this:</p>
             
                <pre translate="no" data-type="programlisting">{"prompt": "Review the following Python code: 
      'def sum(a, b): return a + b\nresult = sum(5, '5')'",
  "completion": "Type error: The 'sum' function is adding an integer and a string. 
      Consider converting the string to an integer using int() before passing it
      to the function."}</pre>
             
            </div></section>
            <section data-type="sect3" data-pdf-bookmark="Financial document summarization"><div class="sect3" id="financial_document_summarization">
              <h4>Financial document summarization</h4>
              <p>In financial document synthesis, a fine-tuned<a contenteditable="false" data-type="indexterm" data-primary="fine-tuning a model" data-secondary="across applications" data-tertiary="financial document summarization" id="id1012"></a><a contenteditable="false" data-type="indexterm" data-primary="financial document summarization" id="id1013"></a><a contenteditable="false" data-type="indexterm" data-primary="large language models (LLMs)" data-secondary="fine-tuning across applications" data-tertiary="financial document summarization" id="id1014"></a> LLM can generate concise and accurate summaries of financial documents, such as earnings reports, financial statements, and analyst reports. By fine-tuning a language model on a dataset related explicitly to financial records, the resulting model can become more accurate in understanding the terminology and context of these documents. For example, the model could take a detailed report about an interest rate increase by a central bank and condense it into a brief yet informative summary.</p>
              <p>To adjust an existing model to get better summaries of financial documents, you need to already have a large number of sample summaries. <a contenteditable="false" data-type="indexterm" data-primary="JSONL dataset for fine-tuning" data-secondary="financial document summarization" id="id1015"></a>Then you have to provide the model with a JSONL file containing pairs of financial documents and their ideal summaries. For example:</p>
            
                <pre translate="no" data-type="programlisting">{"prompt": "The central bank announced a 0.25% increase in the benchmark 
      interest rate, citing concerns about inflationary pressures. This move is
      expected to affect borrowing costs for consumers and businesses.",
  "completion": "Bank increases rate 0.25% for inflation worry. May affect people,
      business borrowing costs."}</pre>
             
            </div></section>
            <section data-type="sect3" data-pdf-bookmark="Technical document translation"><div class="sect3" id="technical_document_translation">
              <h4>Technical document translation</h4>
              <p>Using fine-tuning to translate technical<a contenteditable="false" data-type="indexterm" data-primary="fine-tuning a model" data-secondary="across applications" data-tertiary="technical document translation" id="id1016"></a><a contenteditable="false" data-type="indexterm" data-primary="technical document translation" id="id1017"></a><a contenteditable="false" data-type="indexterm" data-primary="large language models (LLMs)" data-secondary="fine-tuning across applications" data-tertiary="technical document translation" id="id1018"></a> documents can significantly improve the performance of a base model compared to what you can obtain with few-shot learning. The main reason is that technical documents often contain specialized vocabulary and complex sentence structures that few-shot learning cannot handle effectively. The base models are GPT-3 models that have not been adjusted, like the InstructGPT-3 models, with reinforcement learning with human feedback techniques. To be used, they need to be fine-tuned. To adapt an existing base model, you have to prepare a JSONL file containing your training data. For the use case of technical document translation, the contents of this file would include translations of technical texts into the target language.</p>
            </div></section>
            <section data-type="sect3" data-pdf-bookmark="News article generation for niche topics"><div class="sect3" id="news_article_generation_for_niche_topics">
              <h4>News article generation for niche topics</h4>
              <p>In this use case, a fine-tuned model<a contenteditable="false" data-type="indexterm" data-primary="fine-tuning a model" data-secondary="across applications" data-tertiary="news article generation" id="id1019"></a><a contenteditable="false" data-type="indexterm" data-primary="news article generator project" data-secondary="fine-tuning application" id="id1020"></a><a contenteditable="false" data-type="indexterm" data-primary="JSONL dataset for fine-tuning" data-secondary="news article generation" id="id1021"></a><a contenteditable="false" data-type="indexterm" data-primary="large language models (LLMs)" data-secondary="fine-tuning across applications" data-tertiary="news article generation" id="id1022"></a> could generate high-quality, engaging, and contextually relevant news articles for highly specialized topics that the base model probably would not have seen enough of to be accurate. As with all other use cases, you need to create a training dataset to specialize your model to write articles. For that, you need to have at your disposal many articles written on that specific niche topic. This data will be used to create the JSONL file containing prompt-completion pairs. Here is a small example:</p>
              
                <pre translate="no" data-type="programlisting">{"prompt": "Write an introductory article about a new environmentally friendly
      cryptocurrency: 'EcoCoin: The Green Cryptocurrency Taking
      the Market by Storm'",
  "completion": "As concerns over the environmental impact of cryptocurrency
      mining (...) mining process and commitment to sustainability."}</pre>
           
            </div></section>
          </div></section>
          <section data-type="sect2" data-pdf-bookmark="Generating and Fine-Tuning Synthetic Data for an Email Marketing Campaign "><div class="sect2" id="generating_and_fine_tuning_synthetic_data_for_an_e">
            <h3>Generating and Fine-Tuning Synthetic Data for an Email <span class="keep-together">Marketing Campaign</span> </h3>
            <p>In this example, we will make a<a contenteditable="false" data-type="indexterm" data-primary="email marketing text generation tool" data-secondary="about" id="id1023"></a><a contenteditable="false" data-type="indexterm" data-primary="text generation" data-secondary="email marketing tool" data-tertiary="about" id="id1024"></a><a contenteditable="false" data-type="indexterm" data-primary="fine-tuning a model" data-secondary="email marketing text generation tool" data-tertiary="about" id="id1025"></a> text generation tool for an email marketing agency that utilizes targeted content to create personalized email campaigns for businesses. The emails are designed to engage audiences and promote products or services. </p>
            <p>Let’s assume that our agency has a client in the payment processing industry who has asked to help them run a direct email marketing campaign to offer stores a new payment service for ecommerce. The email marketing agency decides to use fine-tuning techniques for this project. Our email marketing agency will need a large amount of data to do this fine-tuning. </p>
            <p>In our case, we will need to generate the data synthetically for demonstration purposes, as you will see in the next subsection. Usually, the best results are obtained with data from human experts, but in some cases, synthetic data generation can be a helpful solution.</p>
            <section data-type="sect3" data-pdf-bookmark="Creating a synthetic dataset"><div class="sect3" id="creating_a_synthetic_dataset">
              <h4>Creating a synthetic dataset</h4>
              <p>In the following example, we create<a contenteditable="false" data-type="indexterm" data-primary="synthetic data" data-secondary="creating a dataset" id="ch04-emsd"></a><a contenteditable="false" data-type="indexterm" data-primary="datasets" data-secondary="synthetic data" data-tertiary="creating" id="ch04-emsd2"></a><a contenteditable="false" data-type="indexterm" data-primary="email marketing text generation tool" data-secondary="synthetic data created" id="ch04-emsd3"></a><a contenteditable="false" data-type="indexterm" data-primary="text generation" data-secondary="email marketing tool" data-tertiary="synthetic data created" id="ch04-emsd4"></a><a contenteditable="false" data-type="indexterm" data-primary="fine-tuning a model" data-secondary="email marketing text generation tool" data-tertiary="synthetic data created" id="ch04-emsd5"></a><a contenteditable="false" data-type="indexterm" data-primary="GPT-3.5 (OpenAI)" data-secondary="synthetic dataset from Turbo" id="ch04-emsd6"></a><a contenteditable="false" data-type="indexterm" data-primary="chat_completion()" data-secondary="synthetic dataset creation" id="ch04-emsd7"></a> artificial data from GPT-3.5 Turbo. To do this, we will specify in a prompt that we want promotional sentences to sell the ecommerce service to a specific merchant. The merchant is characterized by a sector of activity, the city where the store is located, and the size of the store. We get promotional sentences by sending the prompts to GPT-3.5 Turbo via the function <code translate="no">chat_completion</code>, defined earlier.</p>
              <p>We start our script by defining three lists that correspond respectively to the type of shop, the cities where the stores are located, and the size of the stores:</p>
             
                <pre translate="no" data-type="programlisting" data-code-language="python" class="pagebreak-before less_space"><code translate="no" class="n">l_sector</code> <code translate="no" class="o">=</code> <code translate="no" class="p">[</code><code translate="no" class="s1">'Grocery Stores'</code><code translate="no" class="p">,</code> <code translate="no" class="s1">'Restaurants'</code><code translate="no" class="p">,</code> <code translate="no" class="s1">'Fast Food Restaurants'</code><code translate="no" class="p">,</code>
                <code translate="no" class="s1">'Pharmacies'</code><code translate="no" class="p">,</code> <code translate="no" class="s1">'Service Stations (Fuel)'</code><code translate="no" class="p">,</code> <code translate="no" class="s1">'Electronics Stores'</code><code translate="no" class="p">]</code>
  <code translate="no" class="n">l_city</code> <code translate="no" class="o">=</code> <code translate="no" class="p">[</code><code translate="no" class="s1">'Brussels'</code><code translate="no" class="p">,</code> <code translate="no" class="s1">'Paris'</code><code translate="no" class="p">,</code> <code translate="no" class="s1">'Berlin'</code><code translate="no" class="p">]</code>
  <code translate="no" class="n">l_size</code> <code translate="no" class="o">=</code> <code translate="no" class="p">[</code><code translate="no" class="s1">'small'</code><code translate="no" class="p">,</code> <code translate="no" class="s1">'medium'</code><code translate="no" class="p">,</code> <code translate="no" class="s1">'large'</code><code translate="no" class="p">]</code> </pre>
              
              <p>Then we define the first prompt in a string. In this prompt, the role, context, and task are well defined, as they were constructed using the prompt engineering techniques described earlier in this chapter. In this string, the three values between the braces are replaced with the corresponding values later in the code. This first prompt is used to generate the synthetic data:</p>
              
                <pre translate="no" data-type="programlisting" data-code-language="python"><code translate="no" class="n">f_prompt</code> <code translate="no" class="o">=</code> <code translate="no" class="s2">""" </code>
  <code translate="no" class="s2">Role: You are an expert content writer with extensive direct marketing </code>
  <code translate="no" class="s2">experience. You have strong writing skills, creativity, adaptability to </code>
  <code translate="no" class="s2">different tones and styles, and a deep understanding of audience needs and</code>
  <code translate="no" class="s2">preferences for effective direct campaigns.</code>
  <code translate="no" class="s2">Context: You have to write a short message in no more than 2 sentences for a</code>
  <code translate="no" class="s2">direct marketing campaign to sell a new e-commerce payment service to stores. </code>
  <code translate="no" class="s2">The target stores have the following three characteristics:</code>
  <code translate="no" class="s2">- The sector of activity: </code><code translate="no" class="si">{sector}</code><code translate="no" class="s2"></code>
  <code translate="no" class="s2">- The city where the stores are located: </code><code translate="no" class="si">{city}</code><code translate="no" class="s2"> </code>
  <code translate="no" class="s2">- The size of the stores: </code><code translate="no" class="si">{size}</code><code translate="no" class="s2"></code>
  <code translate="no" class="s2">Task: Write a short message for the direct marketing campaign. Use the skills</code>
  <code translate="no" class="s2">defined in your role to write this message! It is important that the message</code>
  <code translate="no" class="s2">you create takes into account the product you are selling and the</code>
  <code translate="no" class="s2">characteristics of the store you are writing to.</code>
  <code translate="no" class="s2">"""</code></pre>
            
              <p>The following prompt contains only the values of the three variables, separated by commas. It is not used to create the synthetic data; only for fine-tuning:</p>
             
                <pre translate="no" data-type="programlisting" data-code-language="python"><code translate="no" class="n">f_sub_prompt</code> <code translate="no" class="o">=</code> <code translate="no" class="s2">"</code><code translate="no" class="si">{sector}</code><code translate="no" class="s2">, </code><code translate="no" class="si">{city}</code><code translate="no" class="s2">, </code><code translate="no" class="si">{size}</code><code translate="no" class="s2">"</code></pre>
             
              <p>Then comes the main part of the code, which iterates over the three value lists we defined earlier. We can see that the code of the block in the loop is straightforward. We replace the values in the braces of the two prompts with the appropriate values. The variable <code translate="no">prompt</code> is used with the function <code translate="no">chat_completion</code> to generate an advertisement saved in <code translate="no">response_txt</code>. The <code translate="no">sub_prompt</code> and <code translate="no">response_txt</code> variables are then added to the <em>out_openai_completion.csv</em> file, our training set for fine-tuning:</p>
             
                <pre translate="no" data-type="programlisting" data-code-language="python"><code translate="no" class="n">df</code> <code translate="no" class="o">=</code> <code translate="no" class="n">pd</code><code translate="no" class="o">.</code><code translate="no" class="n">DataFrame</code><code translate="no" class="p">()</code>
  <code translate="no" class="k">for</code> <code translate="no" class="n">sector</code> <code translate="no" class="ow">in</code> <code translate="no" class="n">l_sector</code><code translate="no" class="p">:</code>
      <code translate="no" class="k">for</code> <code translate="no" class="n">city</code> <code translate="no" class="ow">in</code> <code translate="no" class="n">l_city</code><code translate="no" class="p">:</code>
          <code translate="no" class="k">for</code> <code translate="no" class="n">size</code> <code translate="no" class="ow">in</code> <code translate="no" class="n">l_size</code><code translate="no" class="p">:</code>
              <code translate="no" class="k">for</code> <code translate="no" class="n">i</code> <code translate="no" class="ow">in</code> <code translate="no" class="nb">range</code><code translate="no" class="p">(</code><code translate="no" class="mi">3</code><code translate="no" class="p">):</code>  <code translate="no" class="c1">## 3 times each</code>
                  <code translate="no" class="n">prompt</code> <code translate="no" class="o">=</code> <code translate="no" class="n">f_prompt</code><code translate="no" class="o">.</code><code translate="no" class="n">format</code><code translate="no" class="p">(</code><code translate="no" class="n">sector</code><code translate="no" class="o">=</code><code translate="no" class="n">sector</code><code translate="no" class="p">,</code> <code translate="no" class="n">city</code><code translate="no" class="o">=</code><code translate="no" class="n">city</code><code translate="no" class="p">,</code> <code translate="no" class="n">size</code><code translate="no" class="o">=</code><code translate="no" class="n">size</code><code translate="no" class="p">)</code>
                  <code translate="no" class="n">sub_prompt</code> <code translate="no" class="o">=</code> <code translate="no" class="n">f_sub_prompt</code><code translate="no" class="o">.</code><code translate="no" class="n">format</code><code translate="no" class="p">(</code>
                      <code translate="no" class="n">sector</code><code translate="no" class="o">=</code><code translate="no" class="n">sector</code><code translate="no" class="p">,</code> <code translate="no" class="n">city</code><code translate="no" class="o">=</code><code translate="no" class="n">city</code><code translate="no" class="p">,</code> <code translate="no" class="n">size</code><code translate="no" class="o">=</code><code translate="no" class="n">size</code>
                  <code translate="no" class="p">)</code>
                  <code translate="no" class="n">response_txt</code> <code translate="no" class="o">=</code> <code translate="no" class="n">chat_completion</code><code translate="no" class="p">(</code>
                      <code translate="no" class="n">prompt</code><code translate="no" class="p">,</code> <code translate="no" class="n">model</code><code translate="no" class="o">=</code><code translate="no" class="s2">"gpt-3.5-turbo"</code><code translate="no" class="p">,</code> <code translate="no" class="n">temperature</code><code translate="no" class="o">=</code><code translate="no" class="mi">1</code>
                  <code translate="no" class="p">)</code>
                  <code translate="no" class="n">new_row</code> <code translate="no" class="o">=</code> <code translate="no" class="p">{</code><code translate="no" class="s2">"prompt"</code><code translate="no" class="p">:</code> <code translate="no" class="n">sub_prompt</code><code translate="no" class="p">,</code> <code translate="no" class="s2">"completion"</code><code translate="no" class="p">:</code> <code translate="no" class="n">response_txt</code><code translate="no" class="p">}</code>
                  <code translate="no" class="n">new_row</code> <code translate="no" class="o">=</code> <code translate="no" class="n">pd</code><code translate="no" class="o">.</code><code translate="no" class="n">DataFrame</code><code translate="no" class="p">([</code><code translate="no" class="n">new_row</code><code translate="no" class="p">])</code>
                  <code translate="no" class="n">df</code> <code translate="no" class="o">=</code> <code translate="no" class="n">pd</code><code translate="no" class="o">.</code><code translate="no" class="n">concat</code><code translate="no" class="p">([</code><code translate="no" class="n">df</code><code translate="no" class="p">,</code> <code translate="no" class="n">new_row</code><code translate="no" class="p">],</code> <code translate="no" class="n">axis</code><code translate="no" class="o">=</code><code translate="no" class="mi">0</code><code translate="no" class="p">,</code> <code translate="no" class="n">ignore_index</code><code translate="no" class="o">=</code><code translate="no" class="kc">True</code><code translate="no" class="p">)</code>
  <code translate="no" class="n">df</code><code translate="no" class="o">.</code><code translate="no" class="n">to_csv</code><code translate="no" class="p">(</code><code translate="no" class="s2">"out_openai_completion.csv"</code><code translate="no" class="p">,</code>  <code translate="no" class="n">index</code><code translate="no" class="o">=</code><code translate="no" class="kc">False</code><code translate="no" class="p">)</code></pre>
              
              <p>Note that for each combination of characteristics, we produce three examples. To maximize the model’s creativity, we set the temperature to <code translate="no">1</code>. At the end of this script, we have a Pandas table stored in the file <em>out_openai_completion.csv</em>. It contains 162 observations, with two columns containing the prompt and the corresponding completion. Here are the first two lines of this file:</p>
              
                <pre translate="no" data-type="programlisting">"Grocery Stores, Brussels, small",Introducing our new e-commerce payment service - 
  the perfect solution for small Brussels-based grocery stores to easily and 
  securely process online transactions. "Grocery Stores, Brussels, small",
  Looking for a hassle-free payment solution for your small grocery store in
  Brussels? Our new e-commerce payment service is here to simplify your
  transactions and increase your revenue. Try it now!</pre>
             
              <p>We can now call the tool to generate the training file from <em>out_openai_completion.csv</em> as follows:</p>
            
                <pre translate="no" data-type="programlisting">$ openai tools fine_tunes.prepare_data -f out_openai_completion.csv</pre>
             
              <p>As you can see in the following lines of code, this tool makes suggestions for improving our prompt-completion pairs. At the end of this text, it even gives instructions on how to continue the fine-tuning process and advice on using the model to make predictions once the fine-tuning process is complete:</p>
           
                <pre translate="no" data-type="programlisting">Analyzing...
  - Based on your file extension, your file is formatted as a CSV file
  - Your file contains 162 prompt-completion pairs
  - Your data does not contain a common separator at the end of your prompts. 
  Having a separator string appended to the end of the prompt makes it clearer
  to the fine-tuned model where the completion should begin. See
  https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset
  for more detail and examples. If you intend to do open-ended generation, 
  then you should leave the prompts empty
  - Your data does not contain a common ending at the end of your completions. 
  Having a common ending string appended to the end of the completion makes it
  clearer to the fine-tuned model where the completion should end. See
  https://oreil.ly/MOff7 for more detail and examples.
  - The completion should start with a whitespace character (` `). This tends to
  produce better results due to the tokenization we use. See 
  https://oreil.ly/MOff7 for more details
  Based on the analysis we will perform the following actions:
  - [Necessary] Your format `CSV` will be converted to `JSONL`
  - [Recommended] Add a suffix separator ` -&gt;` to all prompts [Y/n]: Y
  - [Recommended] Add a suffix ending `\n` to all completions [Y/n]: Y
  - [Recommended] Add a whitespace character to the beginning of the completion
  [Y/n]: Y
  Your data will be written to a new JSONL file. Proceed [Y/n]: Y
  Wrote modified file to `out_openai_completion_prepared.jsonl`
  Feel free to take a look!
  Now use that file when fine-tuning:
  &gt; openai api fine_tunes.create -t "out_openai_completion_prepared.jsonl"
  After you’ve fine-tuned a model, remember that your prompt has to end with the 
  indicator string ` -&gt;` for the model to start generating completions, rather
  than continuing with the prompt. Make sure to include `stop=["\n"]` so that the
  generated texts ends at the expected place.
  Once your model starts training, it'll approximately take 4.67 minutes to train
  a `curie` model, and less for `ada` and `babbage`. Queue will approximately
  take half an hour per job ahead of you.</pre>
              
              <p>At the end of this process, a new file called <em>out_openai_completion_prepared.jsonl</em> is available and ready to be sent to the OpenAI servers to run the fine-tuning process. </p>
              <p>Note that, as explained in the message of the function, the prompt has been modified by adding the string <code translate="no">-&gt;</code> at the end, and a suffix ending with <code translate="no">\n</code> has been added to all completions.<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch04-emsd" id="id1026"></a><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch04-emsd2" id="id1027"></a><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch04-emsd3" id="id1028"></a><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch04-emsd4" id="id1029"></a><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch04-emsd5" id="id1030"></a><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch04-emsd6" id="id1031"></a></p>
            </div></section>
            <section data-type="sect3" data-pdf-bookmark="Fine-tuning a model with the synthetic dataset"><div class="sect3" id="fine_tuning_a_model_with_the_synthetic_dataset">
              <h4>Fine-tuning a model with the synthetic dataset</h4>
              <p>The following code uploads the file<a contenteditable="false" data-type="indexterm" data-primary="datasets" data-secondary="synthetic data" data-tertiary="fine-tuning a model with" id="id1032"></a><a contenteditable="false" data-type="indexterm" data-primary="synthetic data" data-secondary="fine-tuning a model with" id="id1033"></a><a contenteditable="false" data-type="indexterm" data-primary="email marketing text generation tool" data-secondary="synthetic data used to fine-tune" id="id1034"></a><a contenteditable="false" data-type="indexterm" data-primary="text generation" data-secondary="email marketing tool" data-tertiary="synthetic data used to fine-tune" id="id1035"></a><a contenteditable="false" data-type="indexterm" data-primary="fine-tuning a model" data-secondary="email marketing text generation tool" data-tertiary="synthetic data used to fine-tune" id="id1036"></a> and does the fine-tuning. In this example, we will use <code translate="no">davinci</code> as the base model, and the name of the resulting model will have <code translate="no">direct_marketing</code> as a suffix:<a contenteditable="false" data-type="indexterm" data-primary="openai.FineTune.create()" data-secondary="email marketing synthetic dataset" id="id1037"></a></p>
             
                <pre translate="no" data-type="programlisting" data-code-language="python"><code translate="no" class="n">ft_file</code> <code translate="no" class="o">=</code> <code translate="no" class="n">openai</code><code translate="no" class="o">.</code><code translate="no" class="n">File</code><code translate="no" class="o">.</code><code translate="no" class="n">create</code><code translate="no" class="p">(</code>
      <code translate="no" class="n">file</code><code translate="no" class="o">=</code><code translate="no" class="nb">open</code><code translate="no" class="p">(</code><code translate="no" class="s2">"out_openai_completion_prepared.jsonl"</code><code translate="no" class="p">,</code> <code translate="no" class="s2">"rb"</code><code translate="no" class="p">),</code> <code translate="no" class="n">purpose</code><code translate="no" class="o">=</code><code translate="no" class="s2">"fine-tune"</code>
  <code translate="no" class="p">)</code>
  <code translate="no" class="n">openai</code><code translate="no" class="o">.</code><code translate="no" class="n">FineTune</code><code translate="no" class="o">.</code><code translate="no" class="n">create</code><code translate="no" class="p">(</code>
      <code translate="no" class="n">training_file</code><code translate="no" class="o">=</code><code translate="no" class="n">ft_file</code><code translate="no" class="p">[</code><code translate="no" class="s2">"id"</code><code translate="no" class="p">],</code> <code translate="no" class="n">model</code><code translate="no" class="o">=</code><code translate="no" class="s2">"davinci"</code><code translate="no" class="p">,</code> <code translate="no" class="n">suffix</code><code translate="no" class="o">=</code><code translate="no" class="s2">"direct_marketing"</code>
  <code translate="no" class="p">)</code></pre>
            
              <p>This will start the update process of the <code translate="no">davinci</code> model with our data. This fine-tuning process can take some time, but when it is finished, you will have a new model adapted for your task. The time needed for this fine-tuning is mainly a function of the number of examples available in your dataset, the number of tokens in your examples, and the base model you have chosen. To give you an idea of the time needed for fine-tuning, in our example it took less than five minutes. However, we have seen some cases in which fine-tuning took more than 30 minutes:<a contenteditable="false" data-type="indexterm" data-primary="openai api fine_tunes.create" id="id1038"></a><a contenteditable="false" data-type="indexterm" data-primary="openai api fine_tunes.create" data-secondary="Ctrl-C interrupting stream but not fine-tuning" id="id1039"></a><a contenteditable="false" data-type="indexterm" data-primary="Ctrl-C interrupting stream but not fine-tuning" id="id1040"></a></p>
             
                <pre translate="no" data-type="programlisting">$ openai api fine_tunes.create -t out_openai_completion_prepared.jsonl \ 
                  -m davinci --suffix "direct_marketing"</pre>
                  <pre translate="no" data-type="programlisting">Upload progress: 100%|| 40.8k/40.8k [00:00&lt;00:00, 65.5Mit/s]
  Uploaded file from out_openai_completion_prepared.jsonl: file-z5mGg(...)
  Created fine-tune: ft-mMsm(...)
  Streaming events until fine-tuning is complete...
  (Ctrl-C will interrupt the stream, but not cancel the fine-tune)
  [] Created fine-tune: ft-mMsm(...)
  [] Fine-tune costs $0.84
  [] Fine-tune enqueued. Queue number: 0
  [] Fine-tune started
  [] Completed epoch 1/4
  [] Completed epoch 2/4
  [] Completed epoch 3/4
  [] Completed epoch 4/4</pre>
             
              <div data-type="warning" epub:type="warning"><h6>Warning</h6>
                <p>As the message in the terminal explains, you will break the connection to the OpenAI servers by typing Ctrl+C in the command line, but this will not interrupt the fine-tuning process. </p>
              </div>
              <p>To reconnect to the server and<a contenteditable="false" data-type="indexterm" data-primary="openai api fine_tunes.follow to resume stream" id="id1041"></a> get back the status of a running fine-tuning job, you can use the following command, <code translate="no">fine_tunes.follow</code>, where <code translate="no">fine_tune_id</code> is the ID of the fine-tuning job: </p>
              
                <pre translate="no" data-type="programlisting">$ openai api fine_tunes.follow -i <em><strong>fine_tune_id</strong></em></pre>
              
              <p>This ID is given when you create the job. In our earlier example, our <code translate="no">fine_tune_id</code> is <code translate="no">ft-mMsm(...)</code>. If you lose your <code translate="no">fine_tune_id</code>, it is possible to display all models via:<a contenteditable="false" data-type="indexterm" data-primary="openai api fine_tunes.list for fine_tune_id" id="id1042"></a><a contenteditable="false" data-type="indexterm" data-primary="fine_tune_id via openai api fine_tunes.list" id="id1043"></a></p>
              
                <pre translate="no" data-type="programlisting">$ openai api fine_tunes.list</pre>
              
              <p>To immediately cancel a fine-tune job, use this:<a contenteditable="false" data-type="indexterm" data-primary="openai api fine_tunes.cancel" id="id1044"></a></p>
             
                <pre translate="no" data-type="programlisting">$ openai api fine_tunes.cancel -i <em><strong>fine_tune_id</strong></em></pre>
             
              <p>And to delete a fine-tune job, use this:<a contenteditable="false" data-type="indexterm" data-primary="openai api fine_tunes.delete" id="id1045"></a></p>
             
                <pre translate="no" data-type="programlisting">$ openai api fine_tunes.delete -i <em><strong>fine_tune_id</strong></em></pre>
             
            </div></section>
            <section data-type="sect3" data-pdf-bookmark="Using the fine-tuned model for text completion"><div class="sect3" id="using_the_fine_tuned_model_for_text_completion">
              <h4>Using the fine-tuned model for text completion</h4>
              <p>Once your new model is built,<a contenteditable="false" data-type="indexterm" data-primary="email marketing text generation tool" data-secondary="testing model in Playground" id="id1046"></a><a contenteditable="false" data-type="indexterm" data-primary="text generation" data-secondary="email marketing tool" data-tertiary="testing model in Playground" id="id1047"></a><a contenteditable="false" data-type="indexterm" data-primary="fine-tuning a model" data-secondary="email marketing text generation tool" data-tertiary="testing model in Playground" id="id1048"></a><a contenteditable="false" data-type="indexterm" data-primary="OpenAI Playground" data-secondary="fine-tuned model tested" id="id1049"></a><a contenteditable="false" data-type="indexterm" data-primary="testing fine-tuned model in Playground" data-seealso="OpenAI Playground" id="id1050"></a><a contenteditable="false" data-type="indexterm" data-primary="OpenAI Playground" data-secondary="interface" data-tertiary="accessing models" id="id1051"></a> it can be accessed in different ways to make new completions. The easiest way to test it is probably via the Playground. To access your models in this tool, you can search for them in the drop-down menu on the righthand side of the Playground interface (see <a data-type="xref" href="#fig_4_using_the_fine_tuned_model_in_the_playground">Figure&nbsp;4-4</a>). All your fine-tuned models are at the bottom of this list. Once you select your model, you can use it to make predictions.</p>
              <figure><div id="fig_4_using_the_fine_tuned_model_in_the_playground" class="figure">
                <img src="/api/v2/epubs/urn:orm:book:9781098152475/files/assets/dagc_0404.png" alt="" width="600" height="318">
                <h6><span class="label">Figure 4-4. </span>Using the fine-tuned model in the Playground</h6>
              </div></figure>
              <p>We used the fine-tuned LLM in the following example with the input prompt <code translate="no">Hotel, New York, small -&gt;</code>. Without further instructions, the model automatically generated an advertisement to sell an ecommerce payment service for a small hotel in New York.</p>
              <p>We already obtained excellent results with a small dataset comprising only 162 examples. For a fine-tuning task, it is generally recommended to have several hundred instances, ideally several thousand. In addition, our training set was generated synthetically when ideally it should have been written by a human expert in marketing.</p>
              <p>To use it with the OpenAI API,<a contenteditable="false" data-type="indexterm" data-primary="openai.Completion.create()" data-secondary="testing fine-tuned model in Playground" id="id1052"></a> we proceed as before with <code translate="no">openai.Completion.​cre⁠ate()</code>, except that we need to use the name of our new model as an input parameter. Don’t forget to end all your prompts with <code translate="no">-&gt;</code> and to set <code translate="no">\n</code> as stop words:</p>
          
                <pre translate="no" data-type="programlisting" data-code-language="python"><code translate="no" class="n">openai</code><code translate="no" class="o">.</code><code translate="no" class="n">Completion</code><code translate="no" class="o">.</code><code translate="no" class="n">create</code><code translate="no" class="p">(</code>
    <code translate="no" class="n">model</code><code translate="no" class="o">=</code><code translate="no" class="s2">"davinci:ft-book:direct-marketing-2023-05-01-15-20-35"</code><code translate="no" class="p">,</code>
    <code translate="no" class="n">prompt</code><code translate="no" class="o">=</code><code translate="no" class="s2">"Hotel, New York, small -&gt;"</code><code translate="no" class="p">,</code>
    <code translate="no" class="n">max_tokens</code><code translate="no" class="o">=</code><code translate="no" class="mi">100</code><code translate="no" class="p">,</code>
    <code translate="no" class="n">temperature</code><code translate="no" class="o">=</code><code translate="no" class="mi">0</code><code translate="no" class="p">,</code>
    <code translate="no" class="n">stop</code><code translate="no" class="o">=</code><code translate="no" class="s2">"</code><code translate="no" class="se">\n</code><code translate="no" class="s2">"</code>
  <code translate="no" class="p">)</code></pre>
             
              <p>We obtain the following answer:</p>
          
                <pre translate="no" data-type="programlisting">&lt;OpenAIObject text_completion id=cmpl-7BTkrdo(...) at 0x7f2(4ca5c220&gt; JSON: {
    "choices": [
      {
        "finish_reason": "stop",
        "index": 0,
        "logprobs": null,
        "text": " \"Upgrade your hotel's payment system with our new e-commerce \ 
  service, designed for small businesses.
      }
    ],
    "created": 1682970309,
    "id": "cmpl-7BTkrdo(...)",
    "model": "davinci:ft-book:direct-marketing-2023-05-01-15-20-35",
    "object": "text_completion",
    "usage": {
      "completion_tokens": 37,
      "prompt_tokens": 8,
      "total_tokens": 45
    }
  }</pre>
             
              <p>As we have shown, fine-tuning can enable Python developers to tailor LLMs to their unique business needs, especially in dynamic domains such as our email marketing example. It’s a powerful approach to customizing the language models you need for your applications. Ultimately, this can easily help you serve your customers better and drive business growth. </p>
            </div></section>
          </div></section>
          <section data-type="sect2" data-pdf-bookmark="Cost of Fine-Tuning"><div class="sect2" id="cost_of_fine_tuning">
            <h3>Cost of Fine-Tuning</h3>
            <p>The use of fine-tuned models is costly.<a contenteditable="false" data-type="indexterm" data-primary="fine-tuning a model" data-secondary="pricing" data-tertiary="costly" id="id1053"></a><a contenteditable="false" data-type="indexterm" data-primary="pricing OpenAI models" data-secondary="fine-tuning existing models" data-tertiary="costly" id="id1054"></a> First you have to pay for the training, and once the model is ready, each prediction will cost you a little more than if you had used the base models provided by OpenAI.</p>
            <p>Pricing is subject to change, but<a contenteditable="false" data-type="indexterm" data-primary="ada available in API" data-secondary="fine-tuning available" data-tertiary="cost" id="id1055"></a><a contenteditable="false" data-type="indexterm" data-primary="babbage available in API" data-secondary="fine-tuning available" data-tertiary="cost" id="id1056"></a><a contenteditable="false" data-type="indexterm" data-primary="curie available in API" data-secondary="fine-tuning available" data-tertiary="cost" id="id1057"></a><a contenteditable="false" data-type="indexterm" data-primary="davinci available in API" data-secondary="fine-tuning available" data-tertiary="cost" id="id1058"></a> at the time of this writing, it looks like <a data-type="xref" href="#table-4-2">Table&nbsp;4-2</a>.</p>
            <table id="table-4-2">
              <caption><span class="label">Table 4-2. </span>Pricing for fine-tuning models at the time of this book’s writing</caption>
              <thead><tr>
                <th>Model</th>
                <th>Training</th>
                <th>Usage</th>
              </tr></thead>
             <tbody> 
              <tr>
                <td><code translate="no">ada</code></td>
                <td>$0.0004 per 1,000 tokens</td>
                <td>$0.0016 per 1,000 tokens</td>
              </tr>
              <tr>
                <td><code translate="no">babbage</code></td>
                <td>$0.0006 per 1,000 tokens</td>
                <td>$0.0024 per 1,000 tokens</td>
              </tr>
              <tr>
                <td><code translate="no">curie</code></td>
                <td>$0.0030 per 1,000 tokens</td>
                <td>$0.0120 per 1,000 tokens</td>
              </tr>
              <tr>
                <td><code translate="no">davinci</code></td>
                <td>$0.0300 per 1,000 tokens</td>
                <td>$0.1200 per 1,000 tokens</td>
              </tr></tbody>
            </table>
            <p>As a point of comparison, the price<a contenteditable="false" data-type="indexterm" data-primary="pricing OpenAI models" data-secondary="least expensive GPT 3.5 Turbo" id="id1059"></a><a contenteditable="false" data-type="indexterm" data-primary="GPT-3.5 (OpenAI)" data-secondary="pricing of Turbo" id="id1060"></a> of the <code translate="no">gpt-3.5-turbo</code> model is $0.002 per 1,000 tokens. As already mentioned, <code translate="no">gpt-3.5-turbo</code> has the best cost-performance ratio.</p>
            <p>To get the latest prices,<a contenteditable="false" data-type="indexterm" data-primary="pricing OpenAI models" data-secondary="about" data-tertiary="pricing page link" id="id1061"></a><a contenteditable="false" data-type="indexterm" data-primary="GPT models" data-secondary="pricing" data-tertiary="pricing page link" id="id1062"></a><a contenteditable="false" data-type="indexterm" data-primary="resources online" data-secondary="OpenAI" data-tertiary="pricing page link" id="id1063"></a><a contenteditable="false" data-type="indexterm" data-primary="OpenAI" data-secondary="prices of models" data-tertiary="pricing page link" id="id1064"></a> visit the <a href="https://openai.com/pricing">OpenAI pricing page</a>.</p>
          </div></section>
        </div></section>
        <section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="summary_id1dN8bY">
          <h2>Summary</h2>
          <p>This chapter discussed advanced techniques to unlock the full potential of GPT-4 and ChatGPT and provided key actionable takeaways to improve the development of applications using LLMs. </p>
          <p>Developers can benefit from understanding prompt engineering, zero-shot learning, few-shot learning, and fine-tuning to create more effective and targeted applications. We explored how to create effective prompts by considering the context, task, and role, which enable more precise interactions with the models. With step-by-step reasoning, developers can encourage the model to reason more effectively and handle complex tasks. In addition, we discussed the flexibility and adaptability that few-shot learning offers, highlighting its data-efficient nature and ability to adapt to different tasks quickly.</p>
          <p><a data-type="xref" href="#table-4-3">Table&nbsp;4-3</a> provides a quick summary of all these techniques, when to use them, and how they compare.<a contenteditable="false" data-type="indexterm" data-primary="large language models (LLMs)" data-secondary="zero-shot learning definition" id="id1065"></a><a contenteditable="false" data-type="indexterm" data-primary="zero-shot-CoT strategy" data-secondary="zero-shot learning definition" id="id1066"></a><a contenteditable="false" data-type="indexterm" data-primary="few-shot learning" data-secondary="definition" id="id1067"></a><a contenteditable="false" data-type="indexterm" data-primary="large language models (LLMs)" data-secondary="few-shot learning definition" id="id1068"></a><a contenteditable="false" data-type="indexterm" data-primary="prompt engineering" data-secondary="improving prompt effectiveness" id="id1069"></a><a contenteditable="false" data-type="indexterm" data-primary="fine-tuning a model" data-secondary="definition" id="id1070"></a></p>
          <table class="lines" id="table-4-3">
            <caption><span class="label">Table 4-3. </span>A comparison of different techniques</caption>
            <thead><tr>
              <th></th>
              <th>Zero-shot learning</th>
              <th>Few-shot learning</th>
              <th>Prompt engineering tricks </th>
              <th>Fine-tuning</th>
            </tr></thead><tbody>
            <tr>
              <td>Definition</td>
              <td>Predicting unseen tasks without prior examples</td>
              <td>Prompt includes examples of inputs and desired output</td>
              <td>Detailed prompt that can include context, role, and tasks, or tricks such as “think step by step” </td>
              <td>Model is further trained on a smaller, specific dataset; prompts used are simple</td>
            </tr>
            <tr>
              <td>Use case</td>
              <td>Simple tasks</td>
              <td>Well-defined but complex tasks, usually with specific output format</td>
              <td>Creative, complex tasks</td>
              <td>Highly complex tasks</td>
            </tr>
            <tr>
              <td>Data</td>
              <td>Requires no additional example data</td>
              <td>Requires a few examples</td>
              <td>Quantity of data depends on the prompt engineering technique </td>
              <td>Requires a large training dataset</td>
            </tr>
            <tr>
              <td>Pricing</td>
              <td>Usage: pricing per token (input + output)</td>
              <td>Usage: pricing per token (input + output); can lead to long prompts</td>
              <td>Usage: pricing per token (input + output), can lead to long prompts</td>
              <td>Training: <br>Usage: pricing per token (input + output) is about 80 times more expensive for fine-tuned <code translate="no">davinci</code> compared to GPT-3.5 Turbo. This means that fine-tuning is financially preferable if other techniques lead to a prompt 80 times as long.</td>
            </tr>
            <tr>
              <td>Conclusion</td>
              <td>Use by default</td>
              <td>If zero-shot learning does not work because the output needs to be particular, use few-shot learning.</td>
              <td>If zero-shot learning does not work because the task is too complex, try prompt engineering.</td>
              <td>If you have a very specific and large dataset and the other solutions do not give good enough results, this should be used as a last resort.</td>
            </tr></tbody>
          </table>
          <p class="pagebreak-before less_space">To ensure success in building LLM applications, developers should experiment with other techniques and evaluate the model’s responses for accuracy and relevance. In addition, developers should be aware of LLM’s computational limitations and adjust their prompts accordingly to achieve better results. By integrating these advanced techniques and continually refining their approach, developers can create powerful and innovative applications that unlock the true potential of GPT-4 and ChatGPT.</p>
          <p>In the next chapter, you will discover two additional ways to integrate LLM capabilities into your applications: plug-ins and the LangChain framework. These tools enable developers to create innovative applications, access up-to-date information, and simplify the development of applications that integrate LLMs. We will also provide insight into the future of LLMs and their impact on app development.</p>
        </div></section>
      </div></section></div>
</div>

</html>